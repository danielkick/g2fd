{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports ----\n",
    "import re\n",
    "import numpy as np # for np.nan\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "# import os   # for write_log, delete_logs\n",
    "import glob # for delete_all_logs\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import json # for saving a dict to txt with json.dumps\n",
    "\n",
    "import pickle\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings ----\n",
    "\n",
    "# remakeDaymet = False\n",
    "# remakeDaymet = True\n",
    "# findBestWeatherModels = False#True \n",
    "# findBestWeatherModels = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36367b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = './data/logs/' # '../data/external/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71f9ac",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a525c",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_SiteYear(df):\n",
    "#     df['SiteYear'] = df['ExperimentCode'].astype(str) + '_' + df['Year'].astype(str)\n",
    "#     return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata, 'Metadata', colGroupings\n",
    "def check_col_types(data, grouping, colGroupings\n",
    "):\n",
    "#     data = metadata\n",
    "#     grouping = 'Metadata'\n",
    "#     colGroupings = pd.read_csv('../data/external/UpdateColGroupings.csv')\n",
    "\n",
    "    considerCols = colGroupings.loc[colGroupings[grouping].notna(), (grouping, grouping+'Type')]\n",
    "    considerCols= considerCols.reset_index().drop(columns = 'index')\n",
    "    changeMessages = []\n",
    "    issueMessages  = []\n",
    "\n",
    "    for i in range(considerCols.shape[0]):\n",
    "        considerCol = considerCols.loc[i, grouping]\n",
    "        considerColType = considerCols.loc[i, grouping+'Type']\n",
    "        # print(i, considerCol, considerColType)\n",
    "        \n",
    "        if considerCol in list(data):\n",
    "            # handle nans, datetime\n",
    "            if not pd.isna(considerColType):\n",
    "                try:\n",
    "                    if considerColType == 'datetime':\n",
    "                        data[considerCol]= pd.to_datetime(data[considerCol])\n",
    "                    else:\n",
    "                        data[considerCol]= data[considerCol].astype(considerColType)\n",
    "                    changeMessage = (considerCol+' -> type '+considerColType+'')\n",
    "                    changeMessages = changeMessages + [changeMessage]\n",
    "\n",
    "                except (ValueError, TypeError):\n",
    "                    data[considerCol]= data[considerCol].astype('string')\n",
    "                    issueMessage = ('Returned as string. '+considerCol+' could not be converted to '+considerColType)\n",
    "                    issueMessages = issueMessages + [issueMessage]\n",
    "\n",
    "            else: # i.e. if type == na\n",
    "                data[considerCol]= data[considerCol].astype('string')\n",
    "                issueMessage = (considerCol+' has no type specified (na). Returned as string.')\n",
    "                issueMessages = issueMessages + [issueMessage]\n",
    "\n",
    "    return(data, changeMessages, issueMessages)\n",
    "\n",
    "\n",
    "# 'metadata2018', colGroupings\n",
    "def check_col_types_all_groupings(\n",
    "    dfNameStr, \n",
    "    colGroupings\n",
    "):\n",
    "    reTypedColumns = {\n",
    "        'Metadata': [], \n",
    "        'Soil': [],\n",
    "        'Phenotype': [],\n",
    "        'Genotype': [],\n",
    "        'Management': [],\n",
    "        'Weather': []\n",
    "    }\n",
    "\n",
    "    for grouping in reTypedColumns.keys(): #['Metadata', 'Soil', 'Phenotype', 'Genotype', 'Management', 'Weather']:\n",
    "    #     print(grouping)\n",
    "\n",
    "        outputList = check_col_types(data = eval(dfNameStr), grouping = grouping, colGroupings = colGroupings)\n",
    "    #     updatedDf = outputList[0]\n",
    "\n",
    "        reTypedColumns[grouping] = outputList[0]\n",
    "\n",
    "        if outputList[1] != []:\n",
    "            write_log(pathString= '../data/external/'+'log_'+dfNameStr+'_changes.txt', \n",
    "                      message = (grouping+' ========').encode())\n",
    "            for change in outputList[1]:\n",
    "                write_log(pathString= '../data/external/'+'log_'+dfNameStr+'_changes.txt', \n",
    "                          message = change.encode())\n",
    "\n",
    "        if outputList[2] != []:\n",
    "            write_log(pathString= '../data/external/'+'log_'+dfNameStr+'_issues.txt', \n",
    "                      message = (grouping+' ========').encode())\n",
    "            for issue in outputList[2]:\n",
    "                write_log(pathString= '../data/external/'+'log_'+dfNameStr+'_issues.txt', \n",
    "                          message = issue.encode())\n",
    "\n",
    "    return(reTypedColumns)\n",
    "\n",
    "\n",
    "def check_col_types_all_groupings_across_tables(\n",
    "    metadataDfNameStr, # = 'metadata2018',\n",
    "    soilDfNameStr, # = 'soil2018',\n",
    "    weatherDfNameStr, # = 'weather2018',\n",
    "    managementDfNameStr, # = 'agro2018',\n",
    "    phenotypeDfNameStr, # = 'pheno2018',\n",
    "    genotypeDfNameStr, # = '',\n",
    "    colGroupings # = colGroupings\n",
    "):\n",
    "    metadataDict = []\n",
    "    soilDict = []\n",
    "    phenoDict = []\n",
    "    genoDict = []\n",
    "    agroDict = []\n",
    "    weatherDict = []\n",
    "\n",
    "    if metadataDfNameStr != '':\n",
    "        metadataDict = check_col_types_all_groupings(dfNameStr = metadataDfNameStr, \n",
    "                                                     colGroupings = colGroupings)\n",
    "    if soilDfNameStr != '':\n",
    "        soilDict = check_col_types_all_groupings(dfNameStr = soilDfNameStr, \n",
    "                                                 colGroupings = colGroupings)\n",
    "    if phenotypeDfNameStr != '':\n",
    "        phenoDict = check_col_types_all_groupings(dfNameStr = phenotypeDfNameStr, \n",
    "                                                  colGroupings = colGroupings)\n",
    "    if genotypeDfNameStr != '':\n",
    "        genoDict = check_col_types_all_groupings(dfNameStr = genotypeDfNameStr, \n",
    "                                                 colGroupings = colGroupings)\n",
    "    if managementDfNameStr != '':\n",
    "        agroDict = check_col_types_all_groupings(dfNameStr = managementDfNameStr , \n",
    "                                                 colGroupings = colGroupings)\n",
    "    if weatherDfNameStr != '':\n",
    "        weatherDict = check_col_types_all_groupings(dfNameStr = weatherDfNameStr, \n",
    "                                                    colGroupings = colGroupings)\n",
    "\n",
    "    outputList = [entry for entry in [metadataDict,\n",
    "                                      soilDict,\n",
    "                                      phenoDict,\n",
    "                                      genoDict, \n",
    "                                      agroDict, \n",
    "                                      weatherDict] if entry != {}]\n",
    "\n",
    "    return(outputList)\n",
    "\n",
    "def combine_dfDicts(\n",
    "    key, # = 'Metadata', \n",
    "    dfDictList, # = [], #[metadataDict, soilDict, phenoDict, agroDict, weatherDict]\n",
    "    logfileName, # = 'combineMetadata',\n",
    "    colGroupings #= colGroupings\n",
    "):\n",
    "\n",
    "    desiredCols = list(colGroupings[key].dropna())\n",
    "    accumulator = pd.DataFrame()\n",
    "    \n",
    "    for dfDict in dfDictList:\n",
    "        if dfDict != []:\n",
    "            if accumulator.shape == (0,0):\n",
    "                accumulator = dfDict[key].loc[:, [col for col in list(dfDict[key]) if col in desiredCols]].drop_duplicates()\n",
    "            else:\n",
    "                temp = dfDict[key].loc[:, [col for col in list(dfDict[key]) if col in desiredCols]].drop_duplicates()\n",
    "                \n",
    "                # Only proceed if if there are more columns shared than just identifiers.\n",
    "                if [entry for entry in list(temp) if entry not in list(colGroupings['Keys'].dropna())] != []:\n",
    "\n",
    "                    # find mismatched columns\n",
    "                    downgradeToStr = [col for col in list(temp) if col in list(accumulator)]    \n",
    "#                     downgradeToStr = [col for col in downgradeToStr if type(temp[col][0]) != type(accumulator[col][0])]\n",
    "                    # This is not done as \n",
    "                    # downgradeToStr = [col for col in downgradeToStr if temp[col].dtype != accumulator[col].dtype]\n",
    "                    # because doing so causes TypeError: Cannot interpret 'StringDtype' as a data type\n",
    "                    # see also https://stackoverflow.com/questions/65079545/compare-dataframe-columns-typeerror-cannot-interpret-stringdtype-as-a-data-t\n",
    "\n",
    "                    for col in downgradeToStr:\n",
    "                        accumulator[col] = accumulator[col].astype(str)\n",
    "                        temp[col] = temp[col].astype(str)\n",
    "\n",
    "                        logMessage = \"Set to string:\"+col\n",
    "                        write_log(pathString= '../data/external/'+'log_'+logfileName+'_changes.txt', \n",
    "                                  message = logMessage.encode())\n",
    "\n",
    "                    # merge the now compatable dfs.\n",
    "                    accumulator= accumulator.merge(temp, how = 'outer').drop_duplicates()\n",
    "    return(accumulator)\n",
    "\n",
    "\n",
    "\n",
    "# combine_dfDicts(key, dfDictList, logfileName, colGroupings) -> accumulator\n",
    "#                   ^        ^\n",
    "#      e.g. 'metadata'       |_______________________________\n",
    "#                                                           \\  \n",
    "# check_col_types_all_groupings_across_tables(              |\n",
    "# metadataDfNameStr, ... weatherDfNameStr, colGroupings) -> [metadataDict ... weatherDict] if entry != {}]\n",
    "#                                 |                          ^           \n",
    "#                                 v                          |    \n",
    "# check_col_types_all_groupings(dfNameStr, colGroupings) -> {'Metadata': [], 'Soil': [], 'Phenotype': [], 'Genotype': [], 'Management': [], 'Weather': []}\n",
    "#                                 |                                      ^\n",
    "#                                 v                                      |\n",
    "#               check_col_types(data, grouping, colGroupings) -> return(data, changeMessages, issueMessages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6eb79",
   "metadata": {},
   "source": [
    "## Remove output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e8647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf5e4a15",
   "metadata": {},
   "source": [
    "## Clean up logs & intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_logs(pathString = '../data/external/'):\n",
    "    logs = glob.glob(pathString+'log*.txt') \n",
    "    for log in logs:\n",
    "        os.remove(log)\n",
    "\n",
    "def write_log(pathString= '../data/external/logfile.txt', message = 'hello'):\n",
    "    # fileName = pathString.split(sep = '/')[-1]\n",
    "    # mk log file if it doesn't exist\n",
    "#     print(os.path.isfile(pathString))\n",
    "    if os.path.isfile(pathString) == False:\n",
    "        with open(pathString, 'wb') as textFile: \n",
    "            # wb allows for unicode \n",
    "            # only needed because there's a non-standard character in one of the column names\n",
    "            # https://www.kite.com/python/answers/how-to-write-unicode-text-to-a-text-file-in-python\n",
    "            textFile.write(''.encode(\"utf8\"))\n",
    "    else:    \n",
    "        with open(pathString, \"ab\") as textFile:\n",
    "            textFile.write(message)\n",
    "            textFile.write('\\n'.encode(\"utf8\"))\n",
    "\n",
    "delete_logs(pathString = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc51393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm database files\n",
    "# for filePath in glob.glob('../data/interim/*.db'):\n",
    "#     os.remove(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6719d75",
   "metadata": {},
   "source": [
    "## Data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a038ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull in info/data objects for converting names\n",
    "\n",
    "# def get_json_dict(path):\n",
    "#     with open(path) as f:\n",
    "#         dat = json.load(f)\n",
    "#     return(dat)\n",
    "\n",
    "# # gets the first item of the dict, returns the assoicated list\n",
    "# def get_json_list(path):\n",
    "#     dat = get_json_dict(path)\n",
    "#     # get only the 0th value\n",
    "#     dat = dat[list(dat.keys())[0]]\n",
    "#     return(dat)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return(data)\n",
    "\n",
    "# this is an unideal workaround that comes from some data having characters within the string (\",\",\"�\",\"Ê\",\"\\xa0\") that prevent using csv or json easily\n",
    "# Putting the conversions into a python script isn't as nice as a table but it's still plaintext so it can be version controlled easily.\n",
    "expectedExperimentCodes = load_pickle(path = \"./data/manual/expectedExperimentCodes.pickle\")\n",
    "replaceExperimentCodes  = load_pickle(path = \"./data/manual/replaceExperimentCodes.pickle\")\n",
    "expectedManagementCols  = load_pickle(path = \"./data/manual/expectedManagementCols.pickle\")\n",
    "newApplicationNames     = load_pickle(path = \"./data/manual/newApplicationNames.pickle\")\n",
    "expectedApplicationUnits= load_pickle(path = \"./data/manual/expectedApplicationUnits.pickle\")\n",
    "replaceApplicationUnits = load_pickle(path = \"./data/manual/replaceApplicationUnits.pickle\")\n",
    "newProductNames         = load_pickle(path = \"./data/manual/newProductNames.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c54cf9",
   "metadata": {},
   "source": [
    "## Class for each year.\n",
    "\n",
    "Goal: allow for reuse of methods on like data (e.g. metadata) while enabling year specific methods or data types. \n",
    "To accomplish this, each data type gets a class subclassed from the `data_obj` class.\n",
    "These are bundled into a generic `g2f_year` class which can be subclassed to apply to different years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generic data object template ====\n",
    "class data_obj:\n",
    "    def __init__(self, path = None, data = None):\n",
    "        self.path = path\n",
    "        self.data = data \n",
    "        \n",
    "    def load_csv(self):\n",
    "        self.data = pd.read_csv(self.path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b21b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data type specific classes ====\n",
    "# metadata\n",
    "class meta_obj(data_obj):\n",
    "    pass\n",
    "\n",
    "# phenotype\n",
    "class phno_obj(data_obj):\n",
    "    pass\n",
    "\n",
    "# genotype\n",
    "class geno_obj(data_obj):\n",
    "    pass\n",
    "\n",
    "# weather\n",
    "class wthr_obj(data_obj):\n",
    "    pass\n",
    "\n",
    "# soil\n",
    "class soil_obj(data_obj):\n",
    "    pass\n",
    "\n",
    "# managment\n",
    "class mgmt_obj(data_obj):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0829d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combined yearly data release ====\n",
    "class g2f_year:\n",
    "    def __init__(self, \n",
    "                 meta_path = None, \n",
    "                 phno_path = None,  \n",
    "                 geno_path = None,  \n",
    "                 wthr_path = None, \n",
    "                 soil_path = None,\n",
    "                 mgmt_path = None,\n",
    "                 year = None\n",
    "                ):        \n",
    "        self.meta = meta_obj(meta_path)\n",
    "        self.phno = phno_obj(phno_path)\n",
    "        self.geno = geno_obj(geno_path)\n",
    "        self.wthr = wthr_obj(wthr_path)\n",
    "        self.soil = soil_obj(soil_path)\n",
    "        self.mgmt = mgmt_obj(mgmt_path)\n",
    "        self.year = year\n",
    "\n",
    "    # load all the csvs that are provided. Ignore genotypic data for now.\n",
    "    def ready_csvs(self):\n",
    "        for entry in ['meta', 'phno', #'geno', \n",
    "                                     'wthr', 'soil', 'mgmt']:\n",
    "            if getattr(self,  entry).path != None:\n",
    "                getattr(self, entry).load_csv()\n",
    "                \n",
    "    # Helper function mimicing `ready_csv`'s check for initialized \n",
    "    def _get_active_attr(self):\n",
    "        active_attr = [entry for entry in ['meta', 'phno', #'geno', \n",
    "                                          'wthr', 'soil', 'mgmt'\n",
    "                                         ] if type(getattr(self,  entry).data) != type(None)]\n",
    "        return(active_attr)\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    # want to standardize column names all at once _then_ we'll worry about bigger disagreements between years \n",
    "    # e.g. make `Location` and `LOCATION` equivalent then worry about equivalence with `ExperimentLocation`\n",
    "    # This will give us a capitalized snake case while allowing special names (e.g. elements) to stay capitalized\n",
    "    def _transform_name(self, input):\n",
    "        def _transform_string(input):\n",
    "            if type(input) != str:\n",
    "                print(\"input must be a string\")\n",
    "                return None\n",
    "            else:\n",
    "                transformed_string = input\n",
    "                # if there are multiple spaces in a replace them with 1\n",
    "                transformed_string = re.sub(' {2,}', ' ', transformed_string)\n",
    "                # remove space after % as in '% Silt'\n",
    "                transformed_string = re.sub('% ', '%', transformed_string)\n",
    "                transformed_string = re.sub(' %', '%', transformed_string)\n",
    "\n",
    "                # Split to Words\n",
    "                transformed_string = transformed_string.replace(' ', '_')\n",
    "                transformed_string = transformed_string.split('_')\n",
    "                # decapitalize words that are capitalized but a few words (e.g. elements)\n",
    "                # should be capitalized will be uncapitalized. Correct for this.\n",
    "                def _decapitalize(string): return (string[0].lower()+string[1:])\n",
    "                \"\"\"\n",
    "                This list holds all of the okayed strings that we want to stay uppercase\n",
    "                \"\"\"\n",
    "                ok_cap = [\"ID\",\"NWS\", \"LOI%\", \"N\", \"N/A\", \"K\", \"S\", \"H\", \"K\", \"Ca\", \"Mg\", \"Na\", \"P-III\", \"P\"]\n",
    "                transformed_string = [e if e in ok_cap else _decapitalize(e)  for e in transformed_string]\n",
    "                transformed_string = '_'.join(transformed_string)\n",
    "                # Now recapitalize the first letter\n",
    "                transformed_string = transformed_string[0].capitalize()+transformed_string[1:]\n",
    "\n",
    "                return transformed_string\n",
    "\n",
    "        if type(input) == list:\n",
    "            return [_transform_string(e) for e in input]\n",
    "        else:\n",
    "            return _transform_string(input)\n",
    "    \n",
    "    # walk over all active dataframes and apply simple renaming transformations\n",
    "    def transform_all_names(self):\n",
    "        for entry in self._get_active_attr():\n",
    "                cols_list = getattr(self, entry).data.columns\n",
    "                cols_list = list(cols_list) \n",
    "                getattr(self, entry).data = getattr(self, entry).data.rename(\n",
    "                    columns=dict(zip(cols_list,\n",
    "                                     self._transform_name(input = cols_list))))\n",
    "    \n",
    "    # This function acts to check if there are names in the csv that are not what we expect\n",
    "    # Year specific variations will be coerced to one of these when possible before standardizing them\n",
    "    def transform_all_names_check(self):\n",
    "        expected_cols_dict = {\n",
    "        'meta': [\n",
    "            'Experiment_code', \n",
    "            'Treatment', \n",
    "            'City', \n",
    "            'Farm', \n",
    "            'Field', \n",
    "            'Trial_ID_(Assigned_by_collaborator_for_internal_reference)', \n",
    "            'Soil_taxonomic_ID_and_horizon_description,_if_known', \n",
    "            'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)', \n",
    "            'Weather_station_latitude_(in_decimal_numbers_nOT_dMS)', \n",
    "            'Weather_station_longitude_(in_decimal_numbers_nOT_dMS)', \n",
    "            'Date_weather_station_placed', \n",
    "            'Date_weather_station_removed', \n",
    "            'In-field_weather_station_serial_number', \n",
    "            'In-field_weather_station_latitude_(in_decimal)', \n",
    "            'In-field_weather_station_longitude_(in_decimal)', \n",
    "            'Previous_crop', \n",
    "            'Pre-plant_tillage_method(s)', \n",
    "            'In-season_tillage_method(s)', \n",
    "            'Plot_length_(center-alley_to_center-alley_in_feet)', \n",
    "            'Alley_length_(in_inches)', \n",
    "            'Row_spacing_(in_inches)', \n",
    "            'Type_of_planter_(fluted_cone;_belt_cone;_air_planter)', \n",
    "            'Number_kernels_planted_per_plot_(>200_seed/pack_for_cone_planters)', \n",
    "            'System_determining_moisture', \n",
    "            'Pounds_needed_soil_moisture', \n",
    "            'Latitude_of_field_corner_#1_(lower_left)', \n",
    "            'Longitude_of_field_corner_#1_(lower_left)', \n",
    "            'Latitude_of_field_corner_#2_(lower_right)', \n",
    "            'Longitude_of_field_corner_#2_(lower_right)', \n",
    "            'Latitude_of_field_corner_#3_(upper_right)', \n",
    "            'Longitude_of_field_corner_#3_(upper_right)', \n",
    "            'Latitude_of_field_corner_#4_(upper_left)', \n",
    "            'Longitude_of_field_corner_#4_(upper_left)', \n",
    "            'Cardinal_heading_pass_1', \n",
    "            'Local_check_#1_pedigree', \n",
    "            'Local_check_#1_source', \n",
    "            'Local_check_#2_pedigree', \n",
    "            'Local_check_#2_source', \n",
    "            'Local_check_#3_pedigree', \n",
    "            'Local_check_#3_source', \n",
    "            'Local_check_#4_pedigree', \n",
    "            'Local_check_#4_source', \n",
    "            'Local_check_#5_pedigree', \n",
    "            'Local_check_#5_source', \n",
    "            'Issue/comment_#1', \n",
    "            'Issue/comment_#2', \n",
    "            'Issue/comment_#3', \n",
    "            'Issue/comment_#4', \n",
    "            'Issue/comment_#5', \n",
    "            'Issue/comment_#6', \n",
    "            'Issue/comment_#7', \n",
    "            'Issue/comment_#8', \n",
    "            'Issue/comment_#9', \n",
    "            'Issue/comment_#10',\n",
    "            #2018\n",
    "            'Station_ID',\n",
    "            #2015\n",
    "            'LBS_for_test', \n",
    "            'Date_Collected', \n",
    "            'Preplant_herb', \n",
    "            'Postplant_herb', \n",
    "            'Total_N', \n",
    "            'Total_P', \n",
    "            'Total_K', \n",
    "            'Fert_dates_1', \n",
    "            'Fert_dates_2', \n",
    "            'Fert_dates_3', \n",
    "            'Fert_dates_4', \n",
    "            'Fert_dates_5', \n",
    "            'Fert_dates_6', \n",
    "            'Fert_dates_7', \n",
    "            'Fert_dates_8', \n",
    "            'Type_of_fert',\n",
    "            # 2014\n",
    "            'Location_short', \n",
    "            'Type', \n",
    "            'Location', \n",
    "            'Insecticide', \n",
    "            'Pre-plant_herbicides', \n",
    "            'Post-plant_herbicides', \n",
    "            'Tillage_method', \n",
    "            'Soil_test_type', \n",
    "            'Soil_texture', \n",
    "            'Soil_pH', \n",
    "            'Total_nitrogen', \n",
    "            'Total_phosphorus', \n",
    "            'Total_potassium', \n",
    "            'Nutrient_application_schedule', \n",
    "            'Irrigated?', \n",
    "            'Weather_station_includes_irrigation', \n",
    "            'Fertigation_schedule', \n",
    "            'Irrigation_schedule', \n",
    "            'Local_check', \n",
    "            'Date_plot_harvested_[MM/DD/YY]', \n",
    "            'Date_plot_planted_[MM/DD/YY]', \n",
    "            'Inbred_reps', \n",
    "            'Inbred_plots', \n",
    "            'Collaborators', \n",
    "            'State'\n",
    "        ], \n",
    "        'phno': [\n",
    "            'Year', \n",
    "            'Field-Location', \n",
    "            'State', \n",
    "            'City', \n",
    "            'Plot_length_(center-center_in_feet)', \n",
    "            'Plot_area_(ft2)', \n",
    "            'Alley_length_(in_inches)', \n",
    "            'Row_spacing_(in_inches)', \n",
    "            'Rows_per_plot', \n",
    "            '#_seed_per_plot', \n",
    "            'Experiment', \n",
    "            'Source', \n",
    "            'Pedigree', \n",
    "            'Family', \n",
    "            'Tester', \n",
    "            'Replicate', \n",
    "            'Block', \n",
    "            'Plot', \n",
    "            'Plot_ID', \n",
    "            'Range', \n",
    "            'Pass', \n",
    "            'Date_plot_planted_[MM/DD/YY]', \n",
    "            'Date_plot_harvested_[MM/DD/YY]', \n",
    "            'Anthesis_[MM/DD/YY]', \n",
    "            'Silking_[MM/DD/YY]', \n",
    "            'Anthesis_[days]', \n",
    "            'Silking_[days]', \n",
    "            'Plant_height_[cm]', \n",
    "            'Ear_height_[cm]', \n",
    "            'Stand_count_[#_of_plants]', \n",
    "            'Root_lodging_[#_of_plants]', \n",
    "            'Stalk_lodging_[#_of_plants]', \n",
    "            'Grain_moisture_[%]', \n",
    "            'Test_weight_[lbs]', \n",
    "            'Plot_weight_[lbs]', \n",
    "            'Grain_yield_(bu/A)', \n",
    "            \"Plot_discarded_[enter_'yes'_or_blank]\", \n",
    "            'Comments', \n",
    "            'Filler', \n",
    "            'Snap_[#_of_plants]', \n",
    "            'Kernels/Packet', \n",
    "            \"Filler_[enter_'filler'_or_blank]\", \n",
    "            'Possible_subs', \n",
    "            'Confirmed_subs', \n",
    "            'Single_plant_biomass_in_july(g)', \n",
    "            'Single_plant_biomass_in_august(g)', \n",
    "            'RootPullingForce(kgf)_july', \n",
    "            'RootPullingForce(kgf)_august',\n",
    "            #2018\n",
    "            'RecId', \n",
    "            'Local_check_(Yes,_no)', \n",
    "            'Plot_length_field', \n",
    "            'Plot_area', \n",
    "            'Rows/Plot', \n",
    "            'Packet/Plot', \n",
    "            '#_seed', \n",
    "            'Stand_[%]', \n",
    "            'Root_lodging_[plants]', \n",
    "            'Stalk_lodging_[plants]', \n",
    "            'Plot_discarded_[enter_\"Yes\"_or_\"blank\"]', \n",
    "            'Filler_[enter_\"filler\"_or_\"blank\"]', \n",
    "            '[add_additional_measurements_here]',\n",
    "            #2016\n",
    "            'RecId', \n",
    "            'Plot_area', \n",
    "            'Rows/Plot', \n",
    "            'Packet/Plot', \n",
    "            'Kernels/Packet', \n",
    "            '#_seed', \n",
    "            'Plot_length_(center-alley_to_center-alley_in_feet)', \n",
    "            'Additional_measurements',\n",
    "            #2015\n",
    "            'Alley_length_(in_feet)'\n",
    "        ], \n",
    "        'geno': [], \n",
    "        'wthr': [\n",
    "            'Field_location', \n",
    "            'Station_ID', \n",
    "            'NWS_network', \n",
    "            'NWS_station', \n",
    "            'Date_key', \n",
    "            'Month', \n",
    "            'Day', \n",
    "            'Year', \n",
    "            'Time', \n",
    "            'Temperature_[C]', \n",
    "            'Dew_point_[C]', \n",
    "            'Relative_humidity_[%]', \n",
    "            'Solar_radiation_[W/m2]', \n",
    "            'Rainfall_[mm]', \n",
    "            'Wind_speed_[m/s]', \n",
    "            'Wind_direction_[degrees]', \n",
    "            'Wind_gust_[m/s]', \n",
    "            'Soil_temperature_[C]', \n",
    "            'Soil_moisture_[%VWC]', \n",
    "            'Soil_eC_[mS/cm]', \n",
    "            'UV_light_[uM/m2s]', \n",
    "            'PAR_[uM/m2s]',\n",
    "            # 2020\n",
    "            'CO2_[ppm]', \n",
    "            # 2018\n",
    "            'Photoperiod_[hours]',\n",
    "            'Column_altered', \n",
    "            'Altered_column_names', \n",
    "            'Cleaning_method', \n",
    "            'Comment',\n",
    "            # 2015\n",
    "            'Record_number', 'Location(s)', 'DOY', 'Datetime_[UTC]',\n",
    "            # 2014\n",
    "            'DOY'\n",
    "        ], \n",
    "        'soil': [\n",
    "            'Grower', \n",
    "            'Location', \n",
    "            'Date_received', \n",
    "            'Date_reported', \n",
    "            'E_depth', \n",
    "            '1:1_soil_pH', \n",
    "            'WDRF_buffer_pH', \n",
    "            '1:1_S_salts_mmho/cm', \n",
    "            'Texture_no', \n",
    "            'Organic_matter_LOI%', \n",
    "            'Nitrate-N_ppm_N', \n",
    "            'Lbs_N/A', \n",
    "            'Potassium_ppm_K', \n",
    "            'Sulfate-S_ppm_S', \n",
    "            'Calcium_ppm_Ca', \n",
    "            'Magnesium_ppm_Mg', \n",
    "            'Sodium_ppm_Na', \n",
    "            'CEC/Sum_of_cations_me/100g', \n",
    "            '%H_sat', \n",
    "            '%K_sat', \n",
    "            '%Ca_sat', \n",
    "            '%Mg_sat', \n",
    "            '%Na_sat', \n",
    "            'Mehlich_P-III_ppm_P', \n",
    "            '%Sand', \n",
    "            '%Silt', \n",
    "            '%Clay', \n",
    "            'Texture', \n",
    "            'Comments',\n",
    "            #2016\n",
    "            'Zinc_ppm_Zn', 'Iron_ppm_Fe', 'Manganese_ppm_Mn', 'Copper_ppm_Cu', 'Boron_ppm_B',\n",
    "            # 2015\n",
    "            'Lab_id', \n",
    "            'Lab_sample_id', \n",
    "            'Date_Collected', \n",
    "            'Cooperator', \n",
    "            'Plow_depth', \n",
    "            'PH', \n",
    "            'BpH', \n",
    "            'OM', \n",
    "            'P', \n",
    "            'K'\n",
    "        ], \n",
    "        'mgmt': [\n",
    "            'Location', \n",
    "            'Application_or_treatment', \n",
    "            'Product_or_nutrient_applied', \n",
    "            'Date_of_application', \n",
    "            'Quantity_per_acre', \n",
    "            'Application_unit',\n",
    "            # 2015\n",
    "            'Irrigation/Fertigation_(yes/no)', \n",
    "            'Weather_station_documents_irrigation?_(yes/no)', \n",
    "            'Nutrients_applied', \n",
    "            'Notes'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        expected_cols_lol = [expected_cols_dict[e] for e in list(expected_cols_dict.keys())]\n",
    "        def _flatten_lol(lol): return [item for listWithin in lol for item in listWithin]\n",
    "        expected_cols_all = _flatten_lol(expected_cols_lol)\n",
    "        \n",
    "        for entry in ['meta', 'phno', 'geno', 'wthr', 'soil', 'mgmt']:\n",
    "            if type(getattr(self,  entry).data) == type(None):\n",
    "                print(entry+\" is not initialized and None type.\")\n",
    "            else:\n",
    "                expected_cols = expected_cols_dict[entry]\n",
    "                found_cols = list(getattr(self,  entry).data.columns)\n",
    "                \n",
    "                unexpected_cols = [e for e in found_cols if e not in expected_cols]\n",
    "                unexpected_cols_all = [e for e in unexpected_cols if e not in expected_cols_all]\n",
    "                \n",
    "                if unexpected_cols != []:\n",
    "                    print(\"Unexpected in \"+entry)\n",
    "                    print(unexpected_cols)\n",
    "                if unexpected_cols_all != []:\n",
    "                    print(\"Unexpected in any table\")\n",
    "                    print(unexpected_cols_all)\n",
    "                    \n",
    "    \n",
    "    # This is a function I expect to be overwritten with year specific subclasses.\n",
    "    # Year specific naming conventions will likely require case by case handling   \n",
    "    def standardize_all_transformed_names(self, entry):\n",
    "        pass\n",
    "    \n",
    "#     def standardize_all_cols(self):\n",
    "#         for entry in ['meta', 'phno', 'geno', 'wthr', 'soil', 'mgmt']:\n",
    "#             if type(getattr(self,  entry).data) == type(None):\n",
    "#                 print(entry+\" is not initialized and None type.\")\n",
    "#             else:\n",
    "#                 self.rename_id_cols(entry = entry)\n",
    "#                 print(entry+\" ids renamed.\")  \n",
    "    \n",
    "    \n",
    "    \n",
    "    def add_year_col_to_all(self):\n",
    "        for entry in ['meta', 'phno', 'geno', 'wthr', 'soil', 'mgmt']:\n",
    "            if type(getattr(self,  entry).data) == type(None):\n",
    "                print(entry+\" is not initialized and None type.\")\n",
    "            else:\n",
    "                getattr(self,  entry).data.loc[:, \"Year\"] = self.year\n",
    "        \n",
    "        \n",
    "    # This is a function I expect to be overwritten with year specific subclasses.\n",
    "    # Year specific naming conventions will likely require case by case handling\n",
    "    def rename_id_cols(self, entry):\n",
    "        if type(getattr(self,  entry).data) == type(None):\n",
    "            print(entry+\" is not initialized and None type.\")\n",
    "        else:\n",
    "            rename_id_dict = {}\n",
    "            \n",
    "            if entry == \"meta\":\n",
    "                rename_id_dict.update({\n",
    "                    'Experiment_code': 'Location'\n",
    "                })\n",
    "            if entry == \"phno\":\n",
    "                rename_id_dict.update({\n",
    "                    'Field-Location':'Location',\n",
    "                    'Replicate':'Replication'\n",
    "                })\n",
    "            if entry == \"wthr\":\n",
    "                rename_id_dict.update({\n",
    "                    'Field_location':'Location' ,\n",
    "                    'Date_key':'Date_Time'\n",
    "                })\n",
    "            if entry == \"soil\":\n",
    "                pass\n",
    "#                 rename_id_dict = rename_id_dict.update({\n",
    "                     \n",
    "#                 })\n",
    "            if entry == \"mgmt\":\n",
    "                rename_id_dict.update({\n",
    "                    #'Location': '', \n",
    "                    'Application_or_treatment': 'Application', \n",
    "                    'Product_or_nutrient_applied': 'Product',  \n",
    "                    'Date_of_application': 'Date', \n",
    "                    'Quantity_per_acre': 'Quantity/acre', \n",
    "                    'Application_unit': 'Unit'\n",
    "                })\n",
    "            \n",
    "            getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_id_dict)\n",
    "            \n",
    "    def rename_all_id_cols(self):\n",
    "        for entry in ['meta', 'phno', 'geno', 'wthr', 'soil', 'mgmt']:\n",
    "            if type(getattr(self,  entry).data) == type(None):\n",
    "                print(entry+\" is not initialized and None type.\")\n",
    "            else:\n",
    "                self.rename_id_cols(entry = entry)\n",
    "                print(entry+\" ids renamed.\")        \n",
    "    \n",
    "    def check_for_id_cols(self):\n",
    "        expected_ids_dict = {\n",
    "            'meta': ['Year', 'Location'                           ], \n",
    "            'phno': ['Year', 'Location', 'Pedigree', 'Replication'],\n",
    "            'geno': [], \n",
    "            'wthr': ['Year', 'Location', 'Date_Time'   ],\n",
    "            'soil': ['Year', 'Location'                           ],\n",
    "            'mgmt': ['Year', 'Location', 'Date',           'Product']\n",
    "        }\n",
    "        expected_cols_lol = [expected_ids_dict[e] for e in list(expected_ids_dict.keys())]\n",
    "        def _flatten_lol(lol): return [item for listWithin in lol for item in listWithin]\n",
    "        expected_cols_all = _flatten_lol(expected_cols_lol)\n",
    "        \n",
    "        for entry in ['meta', 'phno', 'geno', 'wthr', 'soil', 'mgmt']:\n",
    "            if type(getattr(self,  entry).data) == type(None):\n",
    "                print(entry+\" is not initialized and None type.\")\n",
    "            else:\n",
    "                expected_but_not_found = [e for e in expected_ids_dict[entry] if e not in getattr(self,  entry).data.columns]\n",
    "                if expected_but_not_found == []:\n",
    "                    pass\n",
    "                    #print('No missing id columns')\n",
    "                else:\n",
    "                    print(entry)\n",
    "                    print('Missing id columns: \"'+'\", \"'.join(expected_but_not_found)+'\"')  \n",
    "        \n",
    "                \n",
    "                \n",
    "    def _hide_comments(self): \n",
    "        #     ## Update Column Names =====\n",
    "        #     # used in rename_columns()\n",
    "        #     def _update_col_names(self, df, nameDict):\n",
    "        #         changes = []\n",
    "\n",
    "        #         keysNotInDf = [key for key in nameDict.keys() if key not in list(df)]\n",
    "        #         for key in keysNotInDf:\n",
    "        #             for colName in [colName for colName in list(df) if colName in nameDict[key]]:\n",
    "        #                 df= df.rename(columns={colName: key})\n",
    "        #                 changes = changes + [(colName+' -> '+ key)]\n",
    "\n",
    "        #         issues = [colName for colName in list(df) if colName not in nameDict.keys()]\n",
    "\n",
    "        #         return([df, changes, issues])\n",
    "\n",
    "        #     # used in rename_all_columns()\n",
    "        #     def _rename_columns(\n",
    "        #         self,\n",
    "        #         dataIn,\n",
    "        #         colNamePath='./data/external/UpdateColNames.csv',\n",
    "        #         logfileName=''):\n",
    "\n",
    "        #         # Make col name update dict ===================================================\n",
    "        #         temp = pd.read_csv(colNamePath, low_memory=False) # <--- set because Rainfall is coded with numerics and string based na\n",
    "        #         temp = temp.loc[:, (['Standardized']+[entry for entry in list(temp) if entry.startswith('Alias')])]\n",
    "\n",
    "        #         # Turn slice of df into one to many dict\n",
    "        #         renamingDict = {}\n",
    "        #         for i in temp.index:\n",
    "        #             tempSlice = list(temp.loc[i, :].dropna())\n",
    "        #             renamingDict[tempSlice[0]] = tempSlice[1:]\n",
    "\n",
    "        #         # Update Column Names =========================================================\n",
    "        #         outputList = self._update_col_names(\n",
    "        #             df=dataIn,\n",
    "        #             nameDict= renamingDict)\n",
    "\n",
    "        #         updatedDf = outputList[0]\n",
    "        #         dfChanges = outputList[1]\n",
    "        #         dfIssues = outputList[2]\n",
    "\n",
    "        #         # Write logs ==================================================================\n",
    "        #         # TODO bring write_log into class\n",
    "        #         for change in dfChanges:\n",
    "        #             write_log(pathString='./data/logs/log_'+logfileName+'_changes.txt',\n",
    "        #                       message=str(change).encode(\"utf8\"))\n",
    "\n",
    "        #         for issue in dfIssues:\n",
    "        #             write_log(pathString='./data/logs/log_'+logfileName+'_issues.txt',\n",
    "        #                       message=('Not defined: \"'+str(issue)+'\"').encode(\"utf8\"))\n",
    "        #         return(updatedDf)\n",
    "\n",
    "        #     def rename_all_columns(self,\n",
    "        #                            colNamePath='./data/external/UpdateColNames.csv',\n",
    "        #                            logfileName=''\n",
    "        #                           ):\n",
    "        #         for entry in self._get_active_attr():\n",
    "        #             getattr(self, entry).data = self._rename_columns(dataIn=getattr(self, entry).data,\n",
    "        #                                                              colNamePath=colNamePath,\n",
    "        #                                                              logfileName=logfileName)\n",
    "        #     ## Regroup Columns ====\n",
    "        #     def sort_cols_in_dfs(self,\n",
    "        #                      colGroupingsPath = './data/external/UpdateColGroupings.csv' \n",
    "        #                     ):\n",
    "        #         colGroupings = pd.read_csv(colGroupingsPath)\n",
    "        #         inputData = [getattr(self, entry).data for entry in self._get_active_attr()]\n",
    "\n",
    "        #         keyCols= list((colGroupings.Keys).dropna())\n",
    "        #         groupings= [col for col in list(colGroupings) if ((col.endswith('Type') == False) & (col != 'Keys'))]\n",
    "\n",
    "        #         groupingAccumulators = {} #groupings.copy()\n",
    "\n",
    "        #         # i = 0\n",
    "        #         # j = 0\n",
    "\n",
    "        #         for j in range(len(groupings)):\n",
    "        #             #keepTheseCols = keyCols+list(colGroupings[groupings[j]].dropna())\n",
    "        #             keepTheseCols = list(colGroupings[groupings[j]].dropna())\n",
    "        #             accumulator = pd.DataFrame()\n",
    "        #             for i in range(len(inputData)):\n",
    "        #                 data = inputData[i]\n",
    "        #                 data = data.loc[:, [entry for entry in list(data) if entry in keepTheseCols]].drop_duplicates()\n",
    "\n",
    "        #                 # this will hopefully prevent dropping data that is text in datetime or numeric cols.\n",
    "        #                 # And then we don't have to worry about merging by like types here!\n",
    "        #                 data = data.astype(str)\n",
    "\n",
    "        #                 if ((accumulator.shape[0] == 0) | (accumulator.shape[1] == 0) ):\n",
    "        #                     accumulator = data\n",
    "        #                 elif ((data.shape[0] > 0) & (data.shape[1] > 0)) :\n",
    "        #     #                 print(list(accumulator))\n",
    "        #     #                 print(list(data))\n",
    "        #     #                 print('\\n')\n",
    "        #                     accumulator = accumulator.merge(data, how = 'outer')\n",
    "\n",
    "        #                 currentGrouping = groupings[j] # string about to be replace\n",
    "        #                 #     groupingAccumulators[j] = {currentGrouping : accumulator}\n",
    "        #                 #     groupingAccumulators = groupingAccumulators + {currentGrouping : accumulator}\n",
    "\n",
    "        #                 groupingAccumulators.update({currentGrouping : accumulator})\n",
    "\n",
    "        #         return(groupingAccumulators) \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d660ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def prlst(lst): \n",
    "    \"This is just a helper function to ease formating lists of strings with each entryon a different line.\"\n",
    "    print('[')\n",
    "    for e in lst:\n",
    "        if e != lst[-1]:\n",
    "            print(\"'\"+e+\"', \")\n",
    "        else:\n",
    "            print(\"'\"+e+\"'\")\n",
    "    print(']')\n",
    "    \n",
    "def prlst2dct(lst): \n",
    "    \"This is just a helper function to ease formating lists of strings with each entryon a different line.\"\n",
    "    print('{')\n",
    "    for e in lst:\n",
    "        if e != lst[-1]:\n",
    "            print(\"'\"+e+\"': 'XXXXXXX', \")\n",
    "        else:\n",
    "            print(\"'\"+e+\"': 'XXXXXXX'\")\n",
    "    print('}')\n",
    "    \n",
    "    \n",
    "prlst([])\n",
    "# prlst2dct([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120db7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year specific subclasses ----\n",
    "class g2f_2021(g2f_year):\n",
    "    # no year specific renames because we're bringing everything up to 2021 as a standard\n",
    "    pass\n",
    "\n",
    "class g2f_2020(g2f_year):\n",
    "    pass\n",
    "\n",
    "class g2f_2019(g2f_year):\n",
    "    def standardize_all_transformed_names(self):\n",
    "        for entry in ['meta', 'phno', #'geno', \n",
    "                      'wthr', 'soil', 'mgmt']:\n",
    "            if type(getattr(self,  entry).data) == type(None):\n",
    "                print(entry+\" is not initialized and None type.\")\n",
    "            else:\n",
    "                rename_cols_dict = {}\n",
    "                drop_cols_list = []\n",
    "\n",
    "                if entry == \"meta\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "                if entry == \"phno\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Number_kernels_planted_per_plot_(>200_seed/pack_for_cone_planters)': 'Kernels/Packet', #2021.meta\n",
    "                        \"Filler_[enter_'filler'_or_blank]\": 'Filler'\n",
    "                    })\n",
    "                if entry == \"wthr\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "                if entry == \"soil\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "                if entry == \"mgmt\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "\n",
    "                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n",
    "\n",
    "                if drop_cols_list != []:\n",
    "                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\n",
    "                \n",
    "class g2f_2018(g2f_year):\n",
    "    def standardize_all_transformed_names(self):\n",
    "        for entry in ['meta', 'phno', #'geno', \n",
    "                      'wthr', 'soil', 'mgmt']:\n",
    "            if type(getattr(self,  entry).data) == type(None):\n",
    "                print(entry+\" is not initialized and None type.\")\n",
    "            else:\n",
    "                rename_cols_dict = {}\n",
    "                drop_cols_list = []\n",
    "\n",
    "                if entry == \"meta\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Weather_station_serial_number_(Last_four_digits,_e.g.\\xa0m2700s#####)': 'Station_ID'\n",
    "                    })\n",
    "                if entry == \"phno\":\n",
    "                    rename_cols_dict.update({\n",
    "                        # 'RecId': '', \n",
    "                        'Tester/Group': 'Tester', \n",
    "                        # 'Local_check_(Yes,_no)': '', \n",
    "                        # 'Plot_length_field': '', \n",
    "                        'Alley_length': 'Alley_length_(in_inches)', \n",
    "                        'Row_spacing': 'Row_spacing_(in_inches)', \n",
    "                        # 'Plot_area': '', \n",
    "                        # 'Rows/Plot': '', \n",
    "                        # 'Packet/Plot': '', \n",
    "                        # 'Kernels/Packet': '', \n",
    "                        # '#_seed': '', \n",
    "                        'Date_planted': 'Date_plot_planted_[MM/DD/YY]', \n",
    "                        'Date_harvested': 'Date_plot_harvested_[MM/DD/YY]', \n",
    "                        'Anthesis_[date]': 'Anthesis_[MM/DD/YY]', \n",
    "                        'Silking_[date]': 'Silking_[MM/DD/YY]', \n",
    "                        'Pollen_dAP_[days]': 'Anthesis_[days]', \n",
    "                        'Silk_dAP_[days]': 'Silking_[days]', \n",
    "                        'Stand_count_[plants]': 'Stand_count_[#_of_plants]', \n",
    "                        # 'Stand_[%]': '', \n",
    "                        # 'Root_lodging_[plants]': '', \n",
    "                        # 'Stalk_lodging_[plants]': '', \n",
    "                        'Test_weight_[lbs/bu]': 'Plot_weight_[lbs]', \n",
    "                        'Grain_yield_[bu/A]': 'Grain_yield_(bu/A)'#, \n",
    "                        # 'Plot_discarded_[enter_\"Yes\"_or_\"blank\"]': '', \n",
    "                        # 'Filler_[enter_\"filler\"_or_\"blank\"]': '', \n",
    "                        # '[add_additional_measurements_here]': '',\n",
    "                    })\n",
    "                if entry == \"wthr\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'UVL_(uM/m^2s)': 'UV_light_[uM/m2s]', \n",
    "                        'Photoperiod_[_hours]': 'Photoperiod_[hours]'\n",
    "                    })\n",
    "\n",
    "                    drop_cols_list = ['Ï»¿Record_number']\n",
    "                if entry == \"soil\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Field_ID': 'Location',\n",
    "                        'Date_recieved': 'Date_received'\n",
    "                    })\n",
    "                if entry == \"mgmt\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "\n",
    "                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n",
    "\n",
    "                if drop_cols_list != []:\n",
    "                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\n",
    "            \n",
    "class g2f_2017(g2f_year):\n",
    "    def standardize_all_transformed_names(self):\n",
    "        for entry in ['meta', 'phno', #'geno', \n",
    "                      'wthr', 'soil', 'mgmt']:\n",
    "            if type(getattr(self,  entry).data) == type(None):\n",
    "                print(entry+\" is not initialized and None type.\")\n",
    "            else:\n",
    "                rename_cols_dict = {}\n",
    "                drop_cols_list = []\n",
    "\n",
    "                if entry == \"meta\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Weather_station_serial_number_(Last_four_digits,_e.g.\\xa0m2700s#####)': 'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)'\n",
    "                    })\n",
    "                if entry == \"phno\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'LOCAL_cHECK_(Yes,_no[Blank])': 'Local_check_(Yes,_no)', \n",
    "                        'Alley_length': 'Alley_length_(in_inches)', \n",
    "                        'Row_spacing': 'Row_spacing_(in_inches)', \n",
    "                        'Date_planted': 'Date_plot_planted_[MM/DD/YY]', \n",
    "                        'Date_harvested': 'Date_plot_harvested_[MM/DD/YY]', \n",
    "                        'Anthesis_[date]': 'Anthesis_[MM/DD/YY]', \n",
    "                        'Silking_[date]': 'Silking_[MM/DD/YY]', \n",
    "                        'Pollen_dAP_[days]': 'Anthesis_[days]', \n",
    "                        'Silk_dAP_[days]': 'Silking_[days]', \n",
    "                        'Stand_count_[plants]': 'Stand_count_[#_of_plants]', \n",
    "                        'Test_weight_[lbs/bu]': 'Test_weight_[lbs]', \n",
    "                        'Grain_yield_[bu/A]': 'Grain_yield_(bu/A)', \n",
    "                        'Plot_discarded_[enter_\"yes\"_or_\"blank\"]': \"Plot_discarded_[enter_'yes'_or_blank]\"\n",
    "                    })\n",
    "                    drop_cols_list = ['Ï»¿Year']\n",
    "                if entry == \"wthr\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "                if entry == \"soil\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "                if entry == \"mgmt\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "\n",
    "                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n",
    "\n",
    "                if drop_cols_list != []:\n",
    "                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\n",
    "                    \n",
    "class g2f_2016(g2f_year):\n",
    "    def standardize_all_transformed_names(self):\n",
    "        for entry in ['meta', 'phno', #'geno', \n",
    "                      'wthr', 'soil', 'mgmt']:\n",
    "            if type(getattr(self,  entry).data) == type(None):\n",
    "                print(entry+\" is not initialized and None type.\")\n",
    "            else:\n",
    "                rename_cols_dict = {}\n",
    "                drop_cols_list = []\n",
    "\n",
    "                if entry == \"meta\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Weather_station_serial_number_(Last_four_digits,_e.g.\\xa0m2700s#####)': 'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)'\n",
    "                    })\n",
    "                if entry == \"phno\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'LOCAL_cHECK_(Yes,_no[Blank])': 'Local_check_(Yes,_no)', \n",
    "                        'Plot_length_field': 'Plot_length_(center-alley_to_center-alley_in_feet)', \n",
    "                        'Alley_length': 'Alley_length_(in_inches)', \n",
    "                        'Row_spacing': 'Row_spacing_(in_inches)', \n",
    "                        'Date_planted': 'Date_plot_planted_[MM/DD/YY]', \n",
    "                        'Date_harvested': 'Date_plot_harvested_[MM/DD/YY]', \n",
    "                        'Anthesis_[date]': 'Anthesis_[MM/DD/YY]', \n",
    "                        'Silking_[date]': 'Silking_[MM/DD/YY]', \n",
    "                        'Pollen_dAP_[days]': 'Anthesis_[days]', \n",
    "                        'Silk_dAP_[days]': 'Silking_[days]', \n",
    "                        'Stand_count_[plants]': 'Stand_count_[#_of_plants]', \n",
    "                        'Root_lodging_[plants]': 'Root_lodging_[#_of_plants]', \n",
    "                        'Stalk_lodging_[plants]': 'Stalk_lodging_[#_of_plants]', \n",
    "                        'Test_weight_[lbs/bu]': 'Test_weight_[lbs]', \n",
    "                        'Grain_yield_[bu/A]': 'Grain_yield_(bu/A)', \n",
    "                        'Plot_discarded_[enter_\"yes\"_or_\"blank\"]': \"Plot_discarded_[enter_'yes'_or_blank]\", \n",
    "                        'Filler_[enter_\"filler\"_or_\"blank\"]': 'Filler', \n",
    "                        '[add_additional_measurements_here]': 'Additional_measurements'\n",
    "                    })\n",
    "                    drop_cols_list = ['Ï»¿Year']\n",
    "                if entry == \"wthr\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Time[Local]': 'Time', \n",
    "                        'Rainfall[mm]': 'Rainfall_[mm]', \n",
    "                        'Photoperiod[hours]': 'Photoperiod_[hours]'\n",
    "                    })\n",
    "                if entry == \"soil\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Zinc_ppm_zn': 'Zinc_ppm_Zn', \n",
    "                        'Iron_ppm_fe': 'Iron_ppm_Fe', \n",
    "                        'Manganese_ppm_mn': 'Manganese_ppm_Mn', \n",
    "                        'Copper_ppm_cu': 'Copper_ppm_Cu', \n",
    "                        'Boron_ppm_b': 'Boron_ppm_B'\n",
    "                    })\n",
    "                    drop_cols_list = ['Sample_type']\n",
    "                if entry == \"mgmt\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Experiment_code': 'Location', \n",
    "                        'Product/Nutrient_applied': 'Quantity_per_acre', \n",
    "                        'Application_unit\\n(lbs,_in,_oz_per_acre)': 'Application_unit'\n",
    "                    })\n",
    "                    drop_cols_list = ['Record_order', 'Record_ID']\n",
    "\n",
    "                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n",
    "\n",
    "                if drop_cols_list != []:\n",
    "                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\n",
    "class g2f_2015(g2f_year):\n",
    "    def standardize_all_transformed_names(self):\n",
    "        for entry in ['meta', 'phno', #'geno', \n",
    "                      'wthr', 'soil', 'mgmt']:\n",
    "            if type(getattr(self,  entry).data) == type(None):\n",
    "                print(entry+\" is not initialized and None type.\")\n",
    "            else:\n",
    "                rename_cols_dict = {}\n",
    "                drop_cols_list = []\n",
    "\n",
    "                if entry == \"meta\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Experiment': 'Location',  \n",
    "                        'WS_sN': 'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)', \n",
    "                        'WS_lat': 'Weather_station_latitude_(in_decimal_numbers_nOT_dMS)',\n",
    "                        'WS_lon': 'Weather_station_longitude_(in_decimal_numbers_nOT_dMS)', \n",
    "                        'DateIn': 'Date_weather_station_placed', \n",
    "                        'DateOut': 'Date_weather_station_removed', \n",
    "                        'Tillage': 'Pre-plant_tillage_method(s)', \n",
    "                        'PlotLen': 'Plot_length_(center-alley_to_center-alley_in_feet)', \n",
    "                        'AlleyLen': 'Alley_length_(in_inches)', \n",
    "                        'RowSp': 'Row_spacing_(in_inches)', \n",
    "                        'PlanterType': 'Type_of_planter_(fluted_cone;_belt_cone;_air_planter)', \n",
    "                        'KernelsPerPlot': 'Number_kernels_planted_per_plot_(>200_seed/pack_for_cone_planters)', \n",
    "                        'Moisture_meter': 'System_determining_moisture', \n",
    "                        'Corner1_lat': 'Latitude_of_field_corner_#1_(lower_left)', \n",
    "                        'Corner1_lon': 'Longitude_of_field_corner_#1_(lower_left)', \n",
    "                        'Corner_2lat': 'Latitude_of_field_corner_#2_(lower_right)', \n",
    "                        'Corner2_lon': 'Longitude_of_field_corner_#2_(lower_right)',\n",
    "                        'Corner3_lat': 'Latitude_of_field_corner_#3_(upper_right)', \n",
    "                        'Corner3_lon': 'Longitude_of_field_corner_#3_(upper_right)', \n",
    "                        'Corner4_lat': 'Latitude_of_field_corner_#4_(upper_left)', \n",
    "                        'Corner4_lon': 'Longitude_of_field_corner_#4_(upper_left)', \n",
    "                        'Cardinal': 'Cardinal_heading_pass_1', \n",
    "                        'Date_of_soil_sampling': 'Date_Collected', \n",
    "                    })\n",
    "                    \n",
    "                    drop_cols_list = []\n",
    "                if entry == \"phno\":\n",
    "                    rename_cols_dict.update({ \n",
    "                        'LOCAL_cHECK_(Yes,_no[Blank])': 'Local_check_(Yes,_no)', \n",
    "                        'Plot_length_field_(center_to_center_in_feet)': 'Plot_length_(center-alley_to_center-alley_in_feet)', \n",
    "                        'Alley_length_(feet)': 'Alley_length_(in_feet)', \n",
    "                        'Row_spacing_(inches)': 'Row_spacing_(in_inches)', \n",
    "                        'Date_planted': 'Date_plot_planted_[MM/DD/YY]', \n",
    "                        'Date_harvested': 'Date_plot_harvested_[MM/DD/YY]', \n",
    "                        'Anthesis_[date]': 'Anthesis_[MM/DD/YY]', \n",
    "                        'Silking_[date]': 'Silking_[MM/DD/YY]', \n",
    "                        'Pollen_dAP_[days]': 'Anthesis_[days]', \n",
    "                        'Silk_dAP_[days]': 'Silking_[days]', \n",
    "                        'Stand_count_[plants]': 'Stand_count_[#_of_plants]', \n",
    "                        'Root_lodging_[plants]': 'Root_lodging_[#_of_plants]', \n",
    "                        'Stalk_lodging_[plants]': 'Stalk_lodging_[#_of_plants]', \n",
    "                        'Test_weight_[lbs/bu]': 'Test_weight_[lbs]', \n",
    "                        'Grain_yield_[bu/A]': 'Grain_yield_(bu/A)', \n",
    "                        'Plot_discarded_[enter_\"yes\"_or_\"blank\"]': \"Plot_discarded_[enter_'yes'_or_blank]\", \n",
    "                        'Filler_[enter_\"filler\"_or_\"blank\"]': 'Filler', \n",
    "                        '[add_additional_measurements_here]': 'Additional_measurements'\n",
    "                    })\n",
    "                    drop_cols_list = []\n",
    "                if entry == \"wthr\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Record_number': 'Record_number', \n",
    "                        'Experiment(s)': 'Location(s)', \n",
    "                        'Day_of_year': 'DOY', \n",
    "                        'Time_[Local]': 'Time', \n",
    "                        'Datetime_[UTC]': 'Datetime_[UTC]', \n",
    "                        'Soil_moisture_[%]': 'Soil_moisture_[%VWC]', \n",
    "                        'Photoperiod_[hours]': 'Photoperiod_[hours]'\n",
    "                    })\n",
    "                    drop_cols_list = []\n",
    "                if entry == \"soil\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'LabID': 'Lab_id', \n",
    "                        'LabSmplID': 'Lab_sample_id', \n",
    "                        'SmplDate': 'Date_Collected', \n",
    "                        'CoopName': 'Cooperator', \n",
    "                        'Experiment': 'Location', \n",
    "                        'PlowDepth': 'Plow_depth', \n",
    "                    })\n",
    "                    drop_cols_list = []\n",
    "                if entry == \"mgmt\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "                    drop_cols_list = []\n",
    "\n",
    "                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n",
    "\n",
    "                if drop_cols_list != []:\n",
    "                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\n",
    "                    \n",
    "class g2f_2014(g2f_year):\n",
    "    def standardize_all_transformed_names(self):\n",
    "        for entry in ['meta', 'phno', #'geno', \n",
    "                      'wthr', 'soil', 'mgmt']:\n",
    "            if type(getattr(self,  entry).data) == type(None):\n",
    "                print(entry+\" is not initialized and None type.\")\n",
    "            else:\n",
    "                rename_cols_dict = {}\n",
    "                drop_cols_list = []\n",
    "\n",
    "                if entry == \"meta\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Location_name': 'Location_short', \n",
    "                        'Experiment': 'Location', \n",
    "                        'Long': 'Weather_station_longitude_(in_decimal_numbers_nOT_dMS)', \n",
    "                        'Lat': 'Weather_station_latitude_(in_decimal_numbers_nOT_dMS)', \n",
    "                        'Plot_length_(center_to_center_in_feet)': 'Plot_length_(center-alley_to_center-alley_in_feet)', \n",
    "                        'Alley_length_(inches)': 'Alley_length_(in_inches)', \n",
    "                        'Row_spacing_(inches)': 'Row_spacing_(in_inches)', \n",
    "                        'Number_kernels_planted': 'Number_kernels_planted_per_plot_(>200_seed/pack_for_cone_planters)', \n",
    "                        'Planter_type': 'Type_of_planter_(fluted_cone;_belt_cone;_air_planter)', \n",
    "                        'Harvest_date': 'Date_plot_harvested_[MM/DD/YY]', \n",
    "                        'Planting_date': 'Date_plot_planted_[MM/DD/YY]',\n",
    "                        'Folder': 'State', \n",
    "                        'Weather_station_serial_number': 'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)'\n",
    "                    })\n",
    "                    drop_cols_list = ['Traits', 'Data_file_name', 'Metadata_file_name', \n",
    "                        'Additional_metadata', \n",
    "                        'Weather-folder']\n",
    "\n",
    "                if entry == \"phno\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'LOCAL_cHECK_(Yes,_no[Blank])': 'Local_check_(Yes,_no)', \n",
    "                        'Alley_length': 'Alley_length_(in_inches)', \n",
    "                        'Row_spacing': 'Row_spacing_(in_inches)', \n",
    "                        'Date_planted': 'Date_plot_planted_[MM/DD/YY]', \n",
    "                        'Date_harvested': 'Date_plot_harvested_[MM/DD/YY]', \n",
    "                        'Anthesis_[date]': 'Anthesis_[MM/DD/YY]', \n",
    "                        'Silking_[date]': 'Silking_[MM/DD/YY]', \n",
    "                        'Pollen_dAP_[days]': 'Anthesis_[days]', \n",
    "                        'Silk_dAP_[days]': 'Silking_[days]', \n",
    "                        'Stand_count_[plants]': 'Stand_count_[#_of_plants]', \n",
    "                        'Test_weight_[lbs/bu]': 'Test_weight_[lbs]', \n",
    "                        'Grain_yield_[bu/A]': 'Grain_yield_(bu/A)', \n",
    "                        'Plot_discarded_[enter_\"yes\"_or_\"blank\"]': \"Plot_discarded_[enter_'yes'_or_blank]\"\n",
    "                    })\n",
    "                    drop_cols_list = ['Ï»¿Year']\n",
    "                if entry == \"wthr\":\n",
    "                    rename_cols_dict.update({\n",
    "                        'Day_[Local]': 'Day', \n",
    "                        'Month_[Local]': 'Month', \n",
    "                        'Year_[Local]': 'Year', \n",
    "                        'Day_of_year_[Local]': 'DOY', \n",
    "                        'Time_[Local]': 'Time'\n",
    "                    })\n",
    "                    drop_cols_list = ['Record_number', 'Datetime_[UTC]']\n",
    "                if entry == \"soil\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "                if entry == \"mgmt\":\n",
    "                    rename_cols_dict.update({\n",
    "                    })\n",
    "\n",
    "                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n",
    "\n",
    "                if drop_cols_list != []:\n",
    "                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af724661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_2021.meta.data.columns.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7858ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_2015.meta.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3427cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prlst2dct([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62668792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_2021.meta.data.columns.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_2016.phno.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3675e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0842778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all released datasets ----\n",
    "dat_2021 = g2f_2021(\n",
    "    meta_path = './data/raw/GenomesToFields_G2F_data_2021/z._2021_supplemental_info/g2f_2021_field_metadata.csv', \n",
    "    phno_path = './data/raw/GenomesToFields_G2F_data_2021/a._2021_phenotypic_data/g2f_2021_phenotypic_clean_data.csv', # also contains 'g2f_2021_phenotypic_raw_data.csv' \n",
    "    geno_path = None,  \n",
    "    wthr_path = './data/raw/GenomesToFields_G2F_data_2021/b._2021_weather_data/g2f_2021_weather_cleaned.csv',\n",
    "    soil_path = './data/raw/GenomesToFields_G2F_data_2021/c._2021_soil_data/g2f_2021_soil_data.csv',\n",
    "    mgmt_path = './data/raw/GenomesToFields_G2F_data_2021/z._2021_supplemental_info/g2f_2021_agronomic_information.csv',\n",
    "    year = 2021\n",
    ")\n",
    "\n",
    "dat_2020 = g2f_2020(\n",
    "    meta_path = './data/raw/GenomesToFields_G2F_data_2020/z._2020_supplemental_info/g2f_2020_field_metadata.csv', \n",
    "    phno_path = './data/raw/GenomesToFields_G2F_data_2020/a._2020_phenotypic_data/g2f_2020_phenotypic_clean_data.csv', # also contains 'g2f_2020_phenotypic_raw_data.csv' \n",
    "    geno_path = None,  \n",
    "    wthr_path = './data/raw/GenomesToFields_G2F_data_2020/b._2020_weather_data/2020_weather_cleaned.csv',\n",
    "    soil_path = './data/raw/GenomesToFields_G2F_data_2020/c._2020_soil_data/g2f_2020_soil_data.csv',\n",
    "    mgmt_path = './data/raw/GenomesToFields_G2F_data_2020/z._2020_supplemental_info/g2f_2020_agronomic_information.csv',\n",
    "    year = 2020\n",
    ")\n",
    "\n",
    "dat_2019 = g2f_2019(\n",
    "    meta_path = './data/raw/GenomesToFields_data_2019/z._2019_supplemental_info/g2f_2019_field_metadata.csv', \n",
    "    phno_path = './data/raw/GenomesToFields_data_2019/a._2019_phenotypic_data/g2f_2019_phenotypic_clean_data.csv', \n",
    "    geno_path = None,  \n",
    "    wthr_path = './data/raw/GenomesToFields_data_2019/b._2019_weather_data/2019_weather_cleaned.csv',\n",
    "    soil_path = './data/raw/GenomesToFields_data_2019/c._2019_soil_data/g2f_2019_soil_data.csv',\n",
    "    mgmt_path = './data/raw/GenomesToFields_data_2019/z._2019_supplemental_info/g2f_2019_agronomic_information.csv',\n",
    "    year = 2019\n",
    ")\n",
    "\n",
    "dat_2018 = g2f_2018(\n",
    "    meta_path = './data/raw/GenomesToFields_G2F_Data_2018/e._2018_supplemental_info/g2f_2018_field_metadata.csv', \n",
    "    phno_path = './data/raw/GenomesToFields_G2F_Data_2018/a._2018_hybrid_phenotypic_data/g2f_2018_hybrid_data_clean.csv', \n",
    "    geno_path = None,  \n",
    "    wthr_path = './data/raw/GenomesToFields_G2F_Data_2018/b._2018_weather_data/g2f_2018_weather_clean.csv',\n",
    "    soil_path = './data/raw/GenomesToFields_G2F_Data_2018/c._2018_soil_data/g2f_2018_soil_data.csv',\n",
    "    mgmt_path = './data/raw/GenomesToFields_G2F_Data_2018/e._2018_supplemental_info/g2f_2018_agronomic information.csv',\n",
    "    year = 2018\n",
    ")\n",
    "\n",
    "dat_2017 = g2f_2017(\n",
    "    meta_path = './data/raw/G2F_Planting_Season_2017_v1/z._2017_supplemental_info/g2f_2017_field_metadata.csv', \n",
    "    phno_path = './data/raw/G2F_Planting_Season_2017_v1/a._2017_hybrid_phenotypic_data/g2f_2017_hybrid_data_clean.csv', \n",
    "    geno_path = None,  \n",
    "    wthr_path = './data/raw/G2F_Planting_Season_2017_v1/b._2017_weather_data/g2f_2017_weather_data.csv',\n",
    "    soil_path = './data/raw/G2F_Planting_Season_2017_v1/c._2017_soil_data/g2f_2017_soil_data_clean.csv',\n",
    "    mgmt_path = './data/raw/G2F_Planting_Season_2017_v1/z._2017_supplemental_info/g2f_2017_agronomic information.csv',\n",
    "    year = 2017\n",
    ")\n",
    "\n",
    "dat_2016 = g2f_2016(\n",
    "    meta_path = './data/raw/G2F_Planting_Season_2016_v2/z._2016_supplemental_info/g2f_2016_field_metadata.csv', \n",
    "    phno_path = './data/raw/G2F_Planting_Season_2016_v2/a._2016_hybrid_phenotypic_data/g2f_2016_hybrid_data_clean.csv', \n",
    "    geno_path = None,  \n",
    "    wthr_path = './data/raw/G2F_Planting_Season_2016_v2/b._2016_weather_data/g2f_2016_weather_data.csv',\n",
    "    soil_path = './data/raw/G2F_Planting_Season_2016_v2/c._2016_soil_data/g2f_2016_soil_data_clean.csv',\n",
    "    mgmt_path = './data/raw/G2F_Planting_Season_2016_v2/z._2016_supplemental_info/g2f_2016_agronomic_information.csv',\n",
    "    year = 2016\n",
    ")\n",
    "\n",
    "dat_2015 = g2f_2015(\n",
    "              # print('Note! Many management factors are recorded in 2015!')\n",
    "    meta_path = './data/raw/G2F_Planting_Season_2015_v2/z._2015_supplemental_info/g2f_2015_field_metadata.csv', \n",
    "    phno_path = './data/raw/G2F_Planting_Season_2015_v2/a._2015_hybrid_phenotypic_data/g2f_2015_hybrid_data_clean.csv', \n",
    "    geno_path = None,  \n",
    "    wthr_path = './data/raw/G2F_Planting_Season_2015_v2/b._2015_weather_data/g2f_2015_weather.csv',\n",
    "    soil_path = './data/raw/G2F_Planting_Season_2015_v2/d._2015_soil_data/g2f_2015_soil_data.csv',\n",
    "              # There is data to be had but it's not formatted in a machine friendly way.\n",
    "              # I've reformatted it to be easy to read in.\n",
    "    mgmt_path = './data/Manual_old/g2f_2015_agronomic information.csv',\n",
    "    year = 2015\n",
    ")\n",
    "\n",
    "dat_2014 = g2f_2014(\n",
    "    meta_path = './data/raw/G2F_Planting_Season_2014_v4/z._2014_supplemental_info/g2f_2014_field_characteristics.csv', \n",
    "    phno_path = './data/raw/G2F_Planting_Season_2014_v4/a._2014_hybrid_phenotypic_data/g2f_2014_hybrid_data_clean.csv', \n",
    "    geno_path = None,  \n",
    "    wthr_path = './data/raw/G2F_Planting_Season_2014_v4/b._2014_weather_data/g2f_2014_weather.csv',\n",
    "              # no soil 2014, some info in metadata\n",
    "    soil_path = None, \n",
    "              # no agro 2014, some info in metadata\n",
    "    mgmt_path = None,\n",
    "    year = 2014\n",
    ")\n",
    "\n",
    "# # Placeholder for GENETICS data ----\n",
    "# import re\n",
    "# newDecoderNames={\n",
    "#          'Sample Names': 'Sample',\n",
    "# 'Inbred Genotype Names': 'InbredGenotype',\n",
    "#                'Family': 'Family'\n",
    "# }\n",
    "\n",
    "# geno2018= pd.read_table(\n",
    "#     data_adapter_path+'../data/raw/GenomesToFields_G2F_Data_2018/d._2018_genotypic_data/G2F_PHG_minreads1_Mo44_PHW65_MoG_assemblies_14112019_filtered_plusParents_sampleDecoder.txt',\n",
    "#     delimiter= ' ', \n",
    "#     header=None, \n",
    "#     names= ['Sample Names', 'Inbred Genotype Names'])#.sort_values('Inbred Genotype Names')\n",
    "\n",
    "# # Add to enable join to phenotype data\n",
    "# def clip_2018_family(i):\n",
    "#     # for i in range(geno2018.shape[0]):\n",
    "#     if(re.search('_', geno2018['Inbred Genotype Names'][i]) == None):\n",
    "#         newName = str(geno2018['Inbred Genotype Names'].str.split(\"_\")[i][0])\n",
    "#     else:\n",
    "#         newName = str(geno2018['Inbred Genotype Names'].str.split(\"_\")[i][0:1][0]\n",
    "#                      )+'_'+str(geno2018['Inbred Genotype Names'].str.split(\"_\")[i][1:2][0])\n",
    "#     return(newName)\n",
    "    \n",
    "# geno2018['Family'] = [clip_2018_family(i) for i in range(len(geno2018[['Inbred Genotype Names']]))]\n",
    "\n",
    "# # Data is in AGP format saved as a h5\n",
    "# geno2017= pd.read_excel(data_adapter_path+'../data/raw/G2F_Planting_Season_2017_v1/d._2017_genotypic_data/g2f_2017_gbs_hybrid_codes.xlsx')\n",
    "# geno2017['Year']= '2017'\n",
    "\n",
    "\n",
    "# TODO\n",
    "# dat_sets = [dat_2014, dat_2015, dat_2016, dat_2017, dat_2018, dat_2019, dat_2021]\n",
    "# for e in dat_sets:\n",
    "#     e.ready_csvs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9411ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2b4da81",
   "metadata": {},
   "source": [
    "the big thing to check is that each table has the key columns below. \n",
    "With those we should be able to access each value and can turn to stardardization of terms.\n",
    "The key columns for the data are \n",
    "\n",
    "\n",
    "| meta   | phno      | geno | wthr    | soil   | mgmt      |\n",
    "|--------|-----------|------|---------|--------|-----------|\n",
    "|Year    |Year       |      |Year     |Year    |Year       |\n",
    "|Location|Location   |      |Location |Location|Location   |\n",
    "|        |Pedigree   |      |         |        |           |\n",
    "|        |Replication|      |         |        |           |\n",
    "|        |           |      |         |        |Month      |\n",
    "|        |           |      |         |        |Day        |\n",
    "|        |           |      |Date_Time|        |           |\n",
    "|        |           |      |         |        |Application|\n",
    "|        |           |      |         |        |Product    |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf79901",
   "metadata": {},
   "source": [
    "# Prep year by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eeb31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geno is not initialized and None type.\n",
      "geno is not initialized and None type.\n",
      "meta ids renamed.\n",
      "phno ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr ids renamed.\n",
      "soil ids renamed.\n",
      "mgmt ids renamed.\n",
      "geno is not initialized and None type.\n"
     ]
    }
   ],
   "source": [
    "# 2021\n",
    "dat_2021.ready_csvs()\n",
    "dat_2021.transform_all_names()\n",
    "dat_2021.transform_all_names_check()\n",
    "dat_2021.add_year_col_to_all()\n",
    "dat_2021.rename_all_id_cols()\n",
    "dat_2021.check_for_id_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06230c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geno is not initialized and None type.\n",
      "geno is not initialized and None type.\n",
      "meta ids renamed.\n",
      "phno ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr ids renamed.\n",
      "soil ids renamed.\n",
      "mgmt ids renamed.\n",
      "geno is not initialized and None type.\n"
     ]
    }
   ],
   "source": [
    "# 2020\n",
    "dat_2020.ready_csvs()\n",
    "dat_2020.transform_all_names()\n",
    "dat_2020.transform_all_names_check()\n",
    "dat_2020.add_year_col_to_all()\n",
    "dat_2020.rename_all_id_cols()\n",
    "dat_2020.check_for_id_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9c78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geno is not initialized and None type.\n",
      "geno is not initialized and None type.\n",
      "meta ids renamed.\n",
      "phno ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr ids renamed.\n",
      "soil ids renamed.\n",
      "mgmt ids renamed.\n",
      "geno is not initialized and None type.\n"
     ]
    }
   ],
   "source": [
    "# 2019\n",
    "dat_2019.ready_csvs()\n",
    "dat_2019.transform_all_names()\n",
    "dat_2019.standardize_all_transformed_names()\n",
    "dat_2019.transform_all_names_check()\n",
    "dat_2019.add_year_col_to_all()\n",
    "dat_2019.rename_all_id_cols()\n",
    "dat_2019.check_for_id_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b857dacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geno is not initialized and None type.\n",
      "geno is not initialized and None type.\n",
      "meta ids renamed.\n",
      "phno ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr ids renamed.\n",
      "soil ids renamed.\n",
      "mgmt ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr\n",
      "Missing id columns: \"Date_Time\"\n"
     ]
    }
   ],
   "source": [
    "dat_2018.ready_csvs()\n",
    "dat_2018.transform_all_names()\n",
    "dat_2018.standardize_all_transformed_names()\n",
    "dat_2018.transform_all_names_check()\n",
    "dat_2018.add_year_col_to_all()\n",
    "dat_2018.rename_all_id_cols()\n",
    "dat_2018.check_for_id_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600d945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geno is not initialized and None type.\n",
      "geno is not initialized and None type.\n",
      "meta ids renamed.\n",
      "phno ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr ids renamed.\n",
      "soil ids renamed.\n",
      "mgmt ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr\n",
      "Missing id columns: \"Date_Time\"\n"
     ]
    }
   ],
   "source": [
    "dat_2017.ready_csvs()\n",
    "dat_2017.transform_all_names()\n",
    "dat_2017.standardize_all_transformed_names()\n",
    "dat_2017.transform_all_names_check()\n",
    "dat_2017.add_year_col_to_all()\n",
    "dat_2017.rename_all_id_cols()\n",
    "dat_2017.check_for_id_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e699a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58005bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2c5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geno is not initialized and None type.\n",
      "geno is not initialized and None type.\n",
      "meta ids renamed.\n",
      "phno ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr ids renamed.\n",
      "soil ids renamed.\n",
      "mgmt ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr\n",
      "Missing id columns: \"Date_Time\"\n",
      "mgmt\n",
      "Missing id columns: \"Product\"\n"
     ]
    }
   ],
   "source": [
    "dat_2016.ready_csvs()\n",
    "dat_2016.transform_all_names()\n",
    "dat_2016.standardize_all_transformed_names()\n",
    "dat_2016.transform_all_names_check()\n",
    "dat_2016.add_year_col_to_all()\n",
    "dat_2016.rename_all_id_cols()\n",
    "dat_2016.check_for_id_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758472d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geno is not initialized and None type.\n",
      "geno is not initialized and None type.\n",
      "meta ids renamed.\n",
      "phno ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr ids renamed.\n",
      "soil ids renamed.\n",
      "mgmt ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr\n",
      "Missing id columns: \"Location\", \"Date_Time\"\n"
     ]
    }
   ],
   "source": [
    "dat_2015.ready_csvs()\n",
    "dat_2015.transform_all_names()\n",
    "dat_2015.standardize_all_transformed_names()\n",
    "dat_2015.transform_all_names_check()\n",
    "dat_2015.add_year_col_to_all()\n",
    "dat_2015.rename_all_id_cols()\n",
    "dat_2015.check_for_id_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af1d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1483c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soil is not initialized and None type.\n",
      "mgmt is not initialized and None type.\n",
      "geno is not initialized and None type.\n",
      "soil is not initialized and None type.\n",
      "mgmt is not initialized and None type.\n",
      "geno is not initialized and None type.\n",
      "soil is not initialized and None type.\n",
      "mgmt is not initialized and None type.\n",
      "meta ids renamed.\n",
      "phno ids renamed.\n",
      "geno is not initialized and None type.\n",
      "wthr ids renamed.\n",
      "soil is not initialized and None type.\n",
      "mgmt is not initialized and None type.\n",
      "geno is not initialized and None type.\n",
      "wthr\n",
      "Missing id columns: \"Date_Time\"\n",
      "soil is not initialized and None type.\n",
      "mgmt is not initialized and None type.\n"
     ]
    }
   ],
   "source": [
    "dat_2014.ready_csvs()\n",
    "dat_2014.transform_all_names()\n",
    "dat_2014.standardize_all_transformed_names()\n",
    "dat_2014.transform_all_names_check()\n",
    "dat_2014.add_year_col_to_all()\n",
    "dat_2014.rename_all_id_cols()\n",
    "dat_2014.check_for_id_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9528ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating workflow\n",
    "# run to find unexpect columns (e.g. 'CO2_[ppm]') and if they are names with the correct convention\n",
    "# add them into the columns in transform_all_names_check()\n",
    "# if they are instead _NOT_ in the standardized pattern add \n",
    "# Rerun to confirm it's okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe6598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9dafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37b113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e70592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634be631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal here is to define column names and contents that \n",
    "\n",
    "\n",
    "def check_contract(dataframe, contract, enforce = False):\n",
    "    \"\"\"\n",
    "    This function will check a dataframe for agreement with a contract.\n",
    "    To pass it must contain all the columns specified and they must be of the right data type\n",
    "    If `enforce = True` then a dataframe containing only the columns in the contract will be returned (if it passed)\n",
    "    \"\"\"\n",
    "    contract_passed = True\n",
    "    \n",
    "    # Get information on contract\n",
    "    contract_df =  pd.DataFrame(\n",
    "        zip(contract.keys(), \n",
    "            [True if e in dataframe.columns else False for e in contract.keys()],\n",
    "            contract.values()           \n",
    "           ), \n",
    "        columns= [\"Contract\", \n",
    "                  \"DataFrame\",\n",
    "                  \"dtype\"])\n",
    "    # Check contract compliance\n",
    "    contract_df.loc[:, \"ColType\"] = \"\"\n",
    "    contract_df.loc[:, \"dtypeMatch\"] = False\n",
    "    \n",
    "    # This is an inefficient way to do this but it should be okay for this purpose\n",
    "    check_cols = contract_df.loc[contract_df.DataFrame, 'Contract']\n",
    "    for check in check_cols:\n",
    "        val = dataframe.loc[:, check].dtype\n",
    "        contract_df.loc[contract_df.Contract == check, 'ColType'] = val\n",
    "        \n",
    "    contract_df.loc[contract_df.dtype == contract_df.ColType, \"dtypeMatch\"] = True\n",
    "    \n",
    "    # Calculate contract compliance stats\n",
    "    passing_percent = float(sum(contract_df.dtypeMatch)/(contract_df.shape[0]))\n",
    "    passing_status = (passing_percent == 1.0)\n",
    "    \n",
    "    if passing_status:\n",
    "        contract_passed = True\n",
    "        print(\"Passed contract comparison\")\n",
    "        if not contract_passed:\n",
    "            return(contract_df)\n",
    "        else:\n",
    "            if not enforce:\n",
    "                print(\"Return dataframe with contract enforced by passing `enforce = True`\")\n",
    "            else:\n",
    "                return(dataframe.loc[:, list(contract.keys())])\n",
    "    else:\n",
    "        print(str(sum(contract_df.DataFrame))+\"/\"+str(contract_df.shape[0])+\" Columns match.\")\n",
    "        print(str(sum(contract_df.dtypeMatch))+\"/\"+str(contract_df.shape[0])+\" Types match.\") \n",
    "        return(contract_df)\n",
    "    #TODO allow for enforcing contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4028d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The first contract to check is easy.\n",
    "# # 1. Do the data types and columns in each of 2021's data objects match themselves?\n",
    "\n",
    "\n",
    "\n",
    "# meta = dat_2021.meta.data\n",
    "# phno = dat_2021.phno.data\n",
    "# wthr = dat_2021.wthr.data\n",
    "# soil = dat_2021.soil.data\n",
    "# mgmt = dat_2021.mgmt.data\n",
    "\n",
    "# # check_contract(dataframe = mgmt, contract = mgmt_contract, enforce = False)\n",
    "\n",
    "# # 1. are there the right columns?\n",
    "# # [list(mgmt.columns)]\n",
    "\n",
    "\n",
    "\n",
    "# # meta.Location = meta.Location.astype(\"string\")\n",
    "# # # meta.Date = meta.Date.astype(\"datetime64[ns]\")\n",
    "\n",
    "# # print(list(phno.columns))\n",
    "# # meta.info()\n",
    "\n",
    "# # methead.head()\n",
    "# # # meta.Date.astype('datetime64[ns]')\n",
    "\n",
    "\n",
    "# # # nans = pd.to_numeric(s, errors=\"coerce\").isna()\n",
    "# # fail_dates = pd.to_datetime(mgmt.Date, errors='coerce').isna()\n",
    "\n",
    "# # # set(meta.loc[fail_dates, \"Date\"])\n",
    "\n",
    "\n",
    "# # mgmt.loc[:, 'ImputationNotes'] = ''\n",
    "\n",
    "# # '6/24/21 for all but plots in pass 2; 7/5/21 for pass 2',\n",
    "# #  'Before Planting',\n",
    "\n",
    "# # 'Local_check_#1_pedigree', 'Local_check_#1_source', 'Local_check_#2_pedigree', 'Local_check_#2_source', 'Local_check_#3_pedigree', 'Local_check_#3_source', 'Local_check_#4_pedigree', 'Local_check_#4_source', 'Local_check_#5_pedigree', 'Local_check_#5_source', \n",
    "# # 'Issue/comment_#1', 'Issue/comment_#2', 'Issue/comment_#3', 'Issue/comment_#4', 'Issue/comment_#5', 'Issue/comment_#6', 'Issue/comment_#7', 'Issue/comment_#8', 'Issue/comment_#9', 'Issue/comment_#10', \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_contract = {\n",
    "#                                                   'Year': 'int',\n",
    "#                                               'Location': 'string',  \n",
    "#                                                   'City': 'string',\n",
    "#                                                   'Farm': 'string',\n",
    "#                                                  'Field': 'string',\n",
    "#                                              'Treatment': 'string',\n",
    "#                                                         #\n",
    "#                                          'Previous_crop': 'string',\n",
    "#                            'Pre-plant_tillage_method(s)': 'string',\n",
    "#                            'In-season_tillage_method(s)': 'string',\n",
    "#     'Plot_length_(center-alley_to_center-alley_in_feet)': 'string',\n",
    "#                               'Alley_length_(in_inches)': 'string',\n",
    "#                                'Row_spacing_(in_inches)': 'string',\n",
    "#  'Type_of_planter_(fluted_cone;_belt_cone;_air_planter)': 'string',\n",
    "# 'Number_kernels_planted_per_plot_(>200_seed/pack_for_cone_planters)': 'string',\n",
    "#                            'System_determining_moisture': 'string',\n",
    "#                            'Pounds_needed_soil_moisture': 'string',\n",
    "#                                                         #\n",
    "#               'Latitude_of_field_corner_#1_(lower_left)': 'string',\n",
    "#              'Longitude_of_field_corner_#1_(lower_left)': 'string',\n",
    "#              'Latitude_of_field_corner_#2_(lower_right)': 'string',\n",
    "#             'Longitude_of_field_corner_#2_(lower_right)': 'string',\n",
    "#              'Latitude_of_field_corner_#3_(upper_right)': 'string',\n",
    "#             'Longitude_of_field_corner_#3_(upper_right)': 'string',\n",
    "#               'Latitude_of_field_corner_#4_(upper_left)': 'string',\n",
    "#              'Longitude_of_field_corner_#4_(upper_left)': 'string',\n",
    "#                                                         #\n",
    "#                                'Cardinal_heading_pass_1': 'string',\n",
    "#                                                         #\n",
    "#                                          'Local_check_#': 'int',\n",
    "#                                   'Local_check_pedigree': 'string',\n",
    "#                                     'Local_check_source': 'string',    \n",
    "#                                        'Issue/comment_#': 'int',\n",
    "#                                          'Issue/comment': 'string',\n",
    "#                                                         #    \n",
    "# 'Trial_ID_(Assigned_by_collaborator_for_internal_reference)': 'string',\n",
    "#        'Soil_taxonomic_ID_and_horizon_description,_if_known': 'string',\n",
    "# 'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)': 'string',\n",
    "#      'Weather_station_latitude_(in_decimal_numbers_nOT_dMS)': 'string',\n",
    "#     'Weather_station_longitude_(in_decimal_numbers_nOT_dMS)': 'string',\n",
    "#                                                         #   \n",
    "#                            'Date_weather_station_placed': 'datetime64[ns]',\n",
    "#                           'Date_weather_station_removed': 'datetime64[ns]',\n",
    "#                 'In-field_weather_station_serial_number': 'string',\n",
    "#         'In-field_weather_station_latitude_(in_decimal)': 'string',\n",
    "#        'In-field_weather_station_longitude_(in_decimal)': 'string',\n",
    "#                                                         #   \n",
    "#                                       'Imputation_notes': 'string'\n",
    "# }\n",
    "\n",
    "\n",
    "# # meta remove local checks and issues\n",
    "# cols_checks_and_issues_list = [\n",
    "#     'Local_check_#1_pedigree', 'Local_check_#1_source',\n",
    "#     'Local_check_#2_pedigree', 'Local_check_#2_source',\n",
    "#     'Local_check_#3_pedigree', 'Local_check_#3_source',\n",
    "#     'Local_check_#4_pedigree', 'Local_check_#4_source',\n",
    "#     'Local_check_#5_pedigree', 'Local_check_#5_source', \n",
    "#     'Issue/comment_#1', 'Issue/comment_#2', 'Issue/comment_#3', 'Issue/comment_#4', 'Issue/comment_#5',\n",
    "#     'Issue/comment_#6', 'Issue/comment_#7', 'Issue/comment_#8', 'Issue/comment_#9', 'Issue/comment_#10']\n",
    "\n",
    "# temp = meta.loc[:, ['Location'] + cols_checks_and_issues_list ]\n",
    "# # check_contract(dataframe = meta, contract = meta_contract)\n",
    "\n",
    "# temp1 = pd.melt(\n",
    "#     temp, \n",
    "#     id_vars=['Location'], \n",
    "#     value_vars=[\n",
    "#         'Local_check_#1_source',\n",
    "#         'Local_check_#2_source',\n",
    "#         'Local_check_#3_source',\n",
    "#         'Local_check_#4_source',\n",
    "#         'Local_check_#5_source'], \n",
    "#     ignore_index=False\n",
    "# ).rename(columns={\n",
    "#     'variable': 'Local_check_#',\n",
    "#     'value': 'Local_check_source'})\n",
    "\n",
    "# temp2 = pd.melt(\n",
    "#     temp, \n",
    "#     id_vars=['Location'], \n",
    "#     value_vars=[\n",
    "#         'Local_check_#1_pedigree', \n",
    "#         'Local_check_#2_pedigree', \n",
    "#         'Local_check_#3_pedigree', \n",
    "#         'Local_check_#4_pedigree', \n",
    "#         'Local_check_#5_pedigree'], \n",
    "#     ignore_index=False\n",
    "# ).rename(columns={\n",
    "#     'variable': 'Local_check_#',\n",
    "#     'value': 'Local_check_pedigree'})\n",
    "\n",
    "# temp3 = pd.melt(\n",
    "#     temp, \n",
    "#     id_vars=['Location'], \n",
    "#     value_vars=[\n",
    "#         'Issue/comment_#1', \n",
    "#         'Issue/comment_#2', \n",
    "#         'Issue/comment_#3', \n",
    "#         'Issue/comment_#4', \n",
    "#         'Issue/comment_#5', \n",
    "#         'Issue/comment_#6', \n",
    "#         'Issue/comment_#7', \n",
    "#         'Issue/comment_#8', \n",
    "#         'Issue/comment_#9', \n",
    "#         'Issue/comment_#10'], \n",
    "#     ignore_index=False\n",
    "# ).rename(columns={\n",
    "#     'variable': 'Issue/comment_#',\n",
    "#     'value': 'Issue/comment'})\n",
    "\n",
    "# temp1.loc[:, 'Local_check_#'] = [re.findall('#[0-9]+', e)[0].replace('#', '') for e in list(temp1.loc[:, 'Local_check_#'])]\n",
    "# temp2.loc[:, 'Local_check_#'] = [re.findall('#[0-9]+', e)[0].replace('#', '') for e in list(temp2.loc[:, 'Local_check_#'])]\n",
    "# temp3.loc[:, 'Issue/comment_#'] = [re.findall('#[0-9]+', e)[0].replace('#', '') for e in list(temp3.loc[:, 'Issue/comment_#'])]\n",
    "\n",
    "# temp_check = temp1.merge(temp2, how = 'outer')\n",
    "# mask = (temp_check.Local_check_source.isna() & temp_check.Local_check_pedigree.isna())\n",
    "# temp_check = temp_check.loc[~mask, :]\n",
    "\n",
    "# temp_issue = temp3.loc[temp3['Issue/comment'].notna(), :]\n",
    "\n",
    "\n",
    "# # drop cols in cols_checks_and_issues_list and merge in long \n",
    "# meta = meta.drop(columns=cols_checks_and_issues_list).merge(temp_check, how = 'outer').merge(temp_issue, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bfee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_timedelta()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4465e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in meta_contract.keys():\n",
    "#     print(key, meta_contract[key])\n",
    "# #     'datetime64[ns]'\n",
    "# #     pd.to_numeric(meta.loc[:, key], errors=\"coerce\").isna()\n",
    "#     meta.loc[:, key]  #astype(meta_contract[key], coerce = True)\n",
    "# #     meta.loc[:, key] = meta.loc[:, key].astype(meta_contract[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4689fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_contract(dataframe = meta, contract = meta_contract, enforce = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b6673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab1192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c4ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d373419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504cf76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe569133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f60b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c577c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phno_contract = {\n",
    "#                               'Year': 'int',\n",
    "#                           'Location': 'string',\n",
    "#                                     #\n",
    "#                              'State': 'string',\n",
    "#                               'City': 'string',\n",
    "#                                     #\n",
    "#                         'Experiment': 'string',    \n",
    "#                                     #\n",
    "# 'Plot_length_(center-center_in_feet)': 'string',\n",
    "#                    'Plot_area_(ft2)': 'string',\n",
    "#           'Alley_length_(in_inches)': 'string',\n",
    "#            'Row_spacing_(in_inches)': 'string',\n",
    "#                      'Rows_per_plot': 'string',\n",
    "#                    '#_seed_per_plot': 'string',\n",
    "#                                     #\n",
    "#                             'Source': 'string',\n",
    "#                           'Pedigree': 'string',\n",
    "#                             'Family': 'string',\n",
    "#                             'Tester': 'string',\n",
    "#                                     #\n",
    "#                        'Replication': 'string',\n",
    "#                              'Block': 'string',\n",
    "#                               'Plot': 'string',\n",
    "#                            'Plot_ID': 'string',\n",
    "#                              'Range': 'string',\n",
    "#                               'Pass': 'string',\n",
    "#                                     #\n",
    "#       'Date_plot_planted_[MM/DD/YY]': 'datetime64[ns]',\n",
    "#     'Date_plot_harvested_[MM/DD/YY]': 'datetime64[ns]',\n",
    "#                'Anthesis_[MM/DD/YY]': 'datetime64[ns]',\n",
    "#                 'Silking_[MM/DD/YY]': 'datetime64[ns]',\n",
    "#                    'Anthesis_[days]': 'string',\n",
    "#                     'Silking_[days]': 'string',\n",
    "#                                     #\n",
    "#                  'Plant_height_[cm]': 'string',\n",
    "#                    'Ear_height_[cm]': 'string',\n",
    "#          'Stand_count_[#_of_plants]': 'string',\n",
    "#         'Root_lodging_[#_of_plants]': 'string',\n",
    "#        'Stalk_lodging_[#_of_plants]': 'string',\n",
    "#                                     #\n",
    "#                 'Grain_moisture_[%]': 'string',\n",
    "#                  'Test_weight_[lbs]': 'string',\n",
    "#                  'Plot_weight_[lbs]': 'string',\n",
    "#                 'Grain_yield_(bu/A)': 'string',\n",
    "#                                     #\n",
    "#     \"Plot_discarded_[enter_'yes'_or_blank]\": 'string',\n",
    "#                           'Comments': 'string',\n",
    "#                             'Filler': 'string',\n",
    "#                 'Snap_[#_of_plants]': 'string',\n",
    "#                                     #\n",
    "#                   'Imputation_notes': 'string'\n",
    "#     }\n",
    "\n",
    "# wthr_contract = {\n",
    "#                         'Year': 'int',\n",
    "#                     'Location': 'string',\n",
    "#                               #\n",
    "#                   'Station_ID': 'string',\n",
    "#                  'NWS_network': 'string',\n",
    "#                  'NWS_station': 'string',\n",
    "#                    'Date_Time': 'datetime64[ns]',\n",
    "#                        'Month': 'string',\n",
    "#                          'Day': 'string',\n",
    "#                         'Time': 'string',\n",
    "#              'Temperature_[C]': 'string',\n",
    "#                'Dew_point_[C]': 'string',\n",
    "#        'Relative_humidity_[%]': 'string',\n",
    "#       'Solar_radiation_[W/m2]': 'string',\n",
    "#                'Rainfall_[mm]': 'string',\n",
    "#             'Wind_speed_[m/s]': 'string',\n",
    "#     'Wind_direction_[degrees]': 'string',\n",
    "#                               #\n",
    "#              'Wind_gust_[m/s]': 'string',\n",
    "#         'Soil_temperature_[C]': 'string',\n",
    "#         'Soil_moisture_[%VWC]': 'string',\n",
    "#              'Soil_eC_[mS/cm]': 'string',\n",
    "#            'UV_light_[uM/m2s]': 'string',\n",
    "#                 'PAR_[uM/m2s]': 'string',\n",
    "#                               #\n",
    "#             'Imputation_notes': 'string'\n",
    "#     }\n",
    "\n",
    "# soil_contract = {\n",
    "#                           'Year': 'int',\n",
    "#                       'Location': 'string',\n",
    "#                                 #\n",
    "#                  'Date_received': 'string',\n",
    "#                  'Date_reported': 'string',\n",
    "#                                 #\n",
    "#                         'Grower': 'string',\n",
    "#                                 #\n",
    "#                        'E_depth': 'string',\n",
    "#                    '1:1_soil_pH': 'string',\n",
    "#                 'WDRF_buffer_pH': 'string',\n",
    "#            '1:1_S_salts_mmho/cm': 'string',\n",
    "#                     'Texture_no': 'string',\n",
    "#            'Organic_matter_LOI%': 'string',\n",
    "#                'Nitrate-N_ppm_N': 'string',\n",
    "#                        'Lbs_N/A': 'string',\n",
    "#                'Potassium_ppm_K': 'string',\n",
    "#                'Sulfate-S_ppm_S': 'string',\n",
    "#                 'Calcium_ppm_Ca': 'string',\n",
    "#               'Magnesium_ppm_Mg': 'string',\n",
    "#                  'Sodium_ppm_Na': 'string',\n",
    "#     'CEC/Sum_of_cations_me/100g': 'string',\n",
    "#                         '%H_sat': 'string',\n",
    "#                         '%K_sat': 'string',\n",
    "#                        '%Ca_sat': 'string',\n",
    "#                        '%Mg_sat': 'string',\n",
    "#                        '%Na_sat': 'string',\n",
    "#            'Mehlich_P-III_ppm_P': 'string',\n",
    "#                          '%Sand': 'string',\n",
    "#                          '%Silt': 'string',\n",
    "#                          '%Clay': 'string',\n",
    "#                        'Texture': 'string',\n",
    "#                       'Comments': 'string',\n",
    "#                                 #\n",
    "#               'Imputation_notes': 'string'\n",
    "#     }\n",
    "\n",
    "\n",
    "# mgmt_contract = {\n",
    "#                 'Year': 'int',\n",
    "#             'Location': 'string', \n",
    "#                       #\n",
    "#          'Application': 'string', \n",
    "#              'Product': 'string', \n",
    "#                 'Date': 'datetime64[ns]', \n",
    "#        'Quantity/acre': 'float', \n",
    "#                 'Unit': 'string', \n",
    "#                       #\n",
    "#     'Imputation_notes':'string'\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1bb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73af7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import great_expectations as ge\n",
    "# context = ge.get_context()\n",
    "# my_df = ge.from_pandas(dat_2021.mgmt.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0efd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_df.head()\n",
    "# # my_df.Location.value_counts()\n",
    "\n",
    "# # my_df.expect_column_to_exist(\"Location\")\n",
    "# my_df.Date.expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63408d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_contract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_contract(dataframe = df, contract = df_contract, enforce = False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908828d8",
   "metadata": {},
   "source": [
    "# Prepare and write minimally altered G2F database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128543d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # contract for compataility with dataframes that _precede_ Y, G, W, S ---\n",
    "# # NOT the most efficient way to get what we want.\n",
    "# # These are the column names and datatypes we want to enforce to ensure compatability with previous model\n",
    "\n",
    "# # metadata\n",
    "# # <class 'pandas.core.frame.DataFrame'>\n",
    "# # RangeIndex: 296 entries, 0 to 295\n",
    "# # Data columns (total 10 columns):\n",
    "# #  #   Column           Non-Null Count  Dtype  \n",
    "# # ---  ------           --------------  -----  \n",
    "# #  0   ExperimentCode   296 non-null    object \n",
    "# #  1   Year             296 non-null    object \n",
    "# #  2   PreviousCrop     215 non-null    object \n",
    "# #  3   PreplantTillage  119 non-null    object \n",
    "# #  4   InseasonTillage  119 non-null    object \n",
    "# #  5   MoistureSystem   173 non-null    object \n",
    "# #  6   MoistureNeeded   173 non-null    object \n",
    "# #  7   ExpLon           296 non-null    float64\n",
    "# #  8   ExpLat           296 non-null    float64\n",
    "# #  9   Location         42 non-null     object \n",
    "# # dtypes: float64(2), object(8)\n",
    "# # memory usage: 23.2+ KB\n",
    "\n",
    "# meta_contract = {\n",
    "#     'ExperimentCode': 'O', \n",
    "#     'Year': 'O', \n",
    "#     'PreviousCrop': 'O', \n",
    "#     'PreplantTillage': 'O', \n",
    "#     'InseasonTillage': 'O', \n",
    "#     'MoistureSystem': 'O', \n",
    "#     'MoistureNeeded': 'O', \n",
    "#     'ExpLon': 'float64', \n",
    "#     'ExpLat': 'float64', \n",
    "#     'Location': 'O'\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# # phenotype\n",
    "# # <class 'pandas.core.frame.DataFrame'>\n",
    "# # RangeIndex: 98996 entries, 0 to 98995\n",
    "# # Data columns (total 19 columns):\n",
    "# #  #   Column                Non-Null Count  Dtype         \n",
    "# # ---  ------                --------------  -----         \n",
    "# #  0   Pedigree              98996 non-null  object        \n",
    "# #  1   F                     98996 non-null  object        \n",
    "# #  2   M                     98996 non-null  object        \n",
    "# #  3   ExperimentCode        98996 non-null  object        \n",
    "# #  4   Year                  98996 non-null  object        \n",
    "# #  5   DatePlanted           98996 non-null  datetime64[ns]\n",
    "# #  6   DateHarvested         98497 non-null  datetime64[ns]\n",
    "# #  7   DaysToPollen          98996 non-null  object        \n",
    "# #  8   DaysToSilk            98996 non-null  object        \n",
    "# #  9   Height                98996 non-null  object        \n",
    "# #  10  EarHeight             98996 non-null  object        \n",
    "# #  11  StandCount            98996 non-null  object        \n",
    "# #  12  RootLodging           98996 non-null  object        \n",
    "# #  13  StalkLodging          98996 non-null  object        \n",
    "# #  14  PercentGrainMoisture  98996 non-null  object        \n",
    "# #  15  TestWeight            98996 non-null  object        \n",
    "# #  16  PlotWeight            98996 non-null  object        \n",
    "# #  17  GrainYield            98996 non-null  object        \n",
    "# #  18  PercentStand          24002 non-null  object        \n",
    "# # dtypes: datetime64[ns](2), object(17)\n",
    "# # memory usage: 14.4+ MB\n",
    "# phno_contract = {'Pedigree': 'O', \n",
    "#                  'F': 'O', 'M': \n",
    "#                  'O', \n",
    "#                  'ExperimentCode': 'O', \n",
    "#                  'Year': 'O', \n",
    "#                  'DatePlanted': 'datetime64[ns]', \n",
    "#                  'DateHarvested': 'datetime64[ns]', \n",
    "#                  'DaysToPollen': 'O', \n",
    "#                  'DaysToSilk': 'O', \n",
    "#                  'Height': 'O', \n",
    "#                  'EarHeight': 'O', \n",
    "#                  'StandCount': 'O', \n",
    "#                  'RootLodging': 'O', \n",
    "#                  'StalkLodging': 'O', \n",
    "#                  'PercentGrainMoisture': 'O', \n",
    "#                  'TestWeight': 'O', \n",
    "#                  'PlotWeight': 'O', \n",
    "#                  'GrainYield': 'O', \n",
    "#                  'PercentStand': 'O'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # weather (+ managment)\n",
    "# # <class 'pandas.core.frame.DataFrame'>\n",
    "# # RangeIndex: 300191 entries, 0 to 300190\n",
    "# # Data columns (total 22 columns):\n",
    "# #  #   Column                Non-Null Count   Dtype         \n",
    "# # ---  ------                --------------   -----         \n",
    "# #  0   ExperimentCode        300064 non-null  object        \n",
    "# #  1   Year                  300191 non-null  object        \n",
    "# #  2   Date                  300191 non-null  datetime64[ns]\n",
    "# #  3   N                     300191 non-null  float64       \n",
    "# #  4   P                     300191 non-null  float64       \n",
    "# #  5   K                     300191 non-null  float64       \n",
    "# #  6   TempMin               300191 non-null  float64       \n",
    "# #  7   TempMean              300191 non-null  float64       \n",
    "# #  8   TempMax               300191 non-null  float64       \n",
    "# #  9   DewPointMean          300191 non-null  float64       \n",
    "# #  10  RelativeHumidityMean  300191 non-null  float64       \n",
    "# #  11  SolarRadiationMean    300191 non-null  float64       \n",
    "# #  12  WindSpeedMax          300191 non-null  float64       \n",
    "# #  13  WindDirectionMean     300191 non-null  float64       \n",
    "# #  14  WindGustMax           300191 non-null  float64       \n",
    "# #  15  SoilTempMean          300191 non-null  float64       \n",
    "# #  16  SoilMoistureMean      300191 non-null  float64       \n",
    "# #  17  UVLMean               300191 non-null  float64       \n",
    "# #  18  PARMean               300191 non-null  float64       \n",
    "# #  19  PhotoperiodMean       300191 non-null  float64       \n",
    "# #  20  VaporPresEst          300191 non-null  float64       \n",
    "# #  21  WaterTotalInmm        300191 non-null  float64       \n",
    "# # dtypes: datetime64[ns](1), float64(19), object(2)\n",
    "# # memory usage: 50.4+ MB\n",
    "# wthr_contract = {'ExperimentCode': 'O', \n",
    "#                  'Year': 'O', \n",
    "#                  'Date': 'datetime64[ns]', \n",
    "#                  'N': 'float64', \n",
    "#                  'P': 'float64', \n",
    "#                  'K': 'float64', \n",
    "#                  'TempMin': 'float64', \n",
    "#                  'TempMean': 'float64', \n",
    "#                  'TempMax': 'float64', \n",
    "#                  'DewPointMean': 'float64', \n",
    "#                  'RelativeHumidityMean': 'float64', \n",
    "#                  'SolarRadiationMean': 'float64', \n",
    "#                  'WindSpeedMax': 'float64', \n",
    "#                  'WindDirectionMean': 'float64', \n",
    "#                  'WindGustMax': 'float64', \n",
    "#                  'SoilTempMean': 'float64', \n",
    "#                  'SoilMoistureMean': 'float64', \n",
    "#                  'UVLMean': 'float64', \n",
    "#                  'PARMean': 'float64', \n",
    "#                  'PhotoperiodMean': 'float64', \n",
    "#                  'VaporPresEst': 'float64', \n",
    "#                  'WaterTotalInmm': 'float64'}\n",
    "\n",
    "\n",
    "\n",
    "# # soil\n",
    "# # <class 'pandas.core.frame.DataFrame'>\n",
    "# # RangeIndex: 274 entries, 0 to 273\n",
    "# # Data columns (total 23 columns):\n",
    "# #  #   Column                  Non-Null Count  Dtype  \n",
    "# # ---  ------                  --------------  -----  \n",
    "# #  0   ExperimentCode          274 non-null    object \n",
    "# #  1   Year                    274 non-null    object \n",
    "# #  2   SoilpH                  274 non-null    float64\n",
    "# #  3   WDRFpH                  274 non-null    float64\n",
    "# #  4   SSalts                  274 non-null    float64\n",
    "# #  5   PercentOrganic          274 non-null    float64\n",
    "# #  6   ppmNitrateN             274 non-null    float64\n",
    "# #  7   NitrogenPerAcre         274 non-null    float64\n",
    "# #  8   ppmK                    274 non-null    float64\n",
    "# #  9   ppmSulfateS             274 non-null    float64\n",
    "# #  10  ppmCa                   274 non-null    float64\n",
    "# #  11  ppmMg                   274 non-null    float64\n",
    "# #  12  ppmNa                   274 non-null    float64\n",
    "# #  13  CationExchangeCapacity  274 non-null    float64\n",
    "# #  14  PercentH                274 non-null    float64\n",
    "# #  15  PercentK                274 non-null    float64\n",
    "# #  16  PercentCa               274 non-null    float64\n",
    "# #  17  PercentMg               274 non-null    float64\n",
    "# #  18  PercentNa               274 non-null    float64\n",
    "# #  19  ppmP                    274 non-null    float64\n",
    "# #  20  PercentSand             274 non-null    float64\n",
    "# #  21  PercentSilt             274 non-null    float64\n",
    "# #  22  PercentClay             274 non-null    float64\n",
    "# # dtypes: float64(21), object(2)\n",
    "# # memory usage: 49.4+ KB\n",
    "# soil_contract = {'ExperimentCode': 'O', \n",
    "#                  'Year': 'O', \n",
    "#                  'SoilpH': 'float64', \n",
    "#                  'WDRFpH': 'float64', \n",
    "#                  'SSalts': 'float64', \n",
    "#                  'PercentOrganic': 'float64', \n",
    "#                  'ppmNitrateN': 'float64', \n",
    "#                  'NitrogenPerAcre': 'float64', \n",
    "#                  'ppmK': 'float64', \n",
    "#                  'ppmSulfateS': 'float64', \n",
    "#                  'ppmCa': 'float64', \n",
    "#                  'ppmMg': 'float64', \n",
    "#                  'ppmNa': 'float64', \n",
    "#                  'CationExchangeCapacity': 'float64', \n",
    "#                  'PercentH': 'float64', \n",
    "#                  'PercentK': 'float64', \n",
    "#                  'PercentCa': 'float64', \n",
    "#                  'PercentMg': 'float64', \n",
    "#                  'PercentNa': 'float64', \n",
    "#                  'ppmP': 'float64', \n",
    "#                  'PercentSand': 'float64', \n",
    "#                  'PercentSilt': 'float64', \n",
    "#                  'PercentClay': 'float64'}\n",
    "\n",
    "\n",
    "\n",
    "# # phenotypeGBS\n",
    "\n",
    "# def check_contract(dataframe, contract, enforce = False):\n",
    "#     \"\"\"\n",
    "#     This function will check a dataframe for agreement with a contract.\n",
    "#     To pass it must contain all the columns specified and they must be of the right data type\n",
    "#     If `enforce = True` then a dataframe containing only the columns in the contract will be returned (if it passed)\n",
    "#     \"\"\"\n",
    "#     contract_passed = True\n",
    "    \n",
    "#     # Get information on contract\n",
    "#     contract_df =  pd.DataFrame(\n",
    "#         zip(contract.keys(), \n",
    "#             [True if e in dataframe.columns else False for e in contract.keys()],\n",
    "#             contract.values()           \n",
    "#            ), \n",
    "#         columns= [\"Contract\", \n",
    "#                   \"DataFrame\",\n",
    "#                   \"dtype\"])\n",
    "#     # Check contract compliance\n",
    "#     contract_df.loc[:, \"ColType\"] = \"\"\n",
    "#     contract_df.loc[:, \"dtypeMatch\"] = False\n",
    "    \n",
    "#     # This is an inefficient way to do this but it should be okay for this purpose\n",
    "#     check_cols = contract_df.loc[contract_df.DataFrame, 'Contract']\n",
    "#     for check in check_cols:\n",
    "#         val = dataframe.loc[:, check].dtype\n",
    "#         contract_df.loc[contract_df.Contract == check, 'ColType'] = val\n",
    "        \n",
    "#     contract_df.loc[contract_df.dtype == contract_df.ColType, \"dtypeMatch\"] = True\n",
    "    \n",
    "#     # Calculate contract compliance stats\n",
    "#     passing_percent = float(sum(contract_df.dtypeMatch)/(contract_df.shape[0]))\n",
    "#     passing_status = (passing_percent == 1.0)\n",
    "    \n",
    "#     if passing_status:\n",
    "#         contract_passed = True\n",
    "#         print(\"Passed contract comparison\")\n",
    "#         if not contract_passed:\n",
    "#             return(contract_df)\n",
    "#         else:\n",
    "#             if not enforce:\n",
    "#                 print(\"Return dataframe with contract enforced by passing `enforce = True`\")\n",
    "#             else:\n",
    "#                 return(dataframe.loc[:, list(contract.keys())])\n",
    "#     else:\n",
    "#         print(str(sum(contract_df.DataFrame))+\"/\"+str(contract_df.shape[0])+\" Columns match.\")\n",
    "#         print(str(sum(contract_df.dtypeMatch))+\"/\"+str(contract_df.shape[0])+\" Types match.\") \n",
    "#         return(contract_df)\n",
    "#     #TODO allow for enforcing contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta = dat_2021.meta.data\n",
    "# phno = dat_2021.phno.data\n",
    "# wthr = dat_2021.wthr.data\n",
    "# soil = dat_2021.soil.data\n",
    "# mgmt = dat_2021.mgmt.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_2021.rename_id_cols(\"meta\")\n",
    "# dat_2021.rename_id_cols(\"phno\")\n",
    "# dat_2021.rename_id_cols(\"wthr\")\n",
    "# dat_2021.rename_id_cols(\"soil\")\n",
    "# dat_2021.rename_id_cols(\"mgmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_2020.ready_csvs()\n",
    "# dat_2020.transform_all_names()\n",
    "# dat_2020.transform_all_names_check()\n",
    "\n",
    "#weather\n",
    "# 'CO2_[ppm]'\n",
    "\n",
    "# dat_2019.ready_csvs()\n",
    "# dat_2019.transform_all_names()\n",
    "# dat_2019.transform_all_names_check()\n",
    "#phno \n",
    "# ['Kernels/Packet', \"Filler_[enter_'filler'_or_blank]\", 'Possible_subs', 'Confirmed_subs', 'Single_plant_biomass_in_july(g)', 'Single_plant_biomass_in_august(g)', 'RootPullingForce(kgf)_july', 'RootPullingForce(kgf)_august']\n",
    "\n",
    "# TODO check other years\n",
    "# dat_2018.ready_csvs()\n",
    "# dat_2018.transform_all_names()\n",
    "# dat_2018.transform_all_names_check()\n",
    "\n",
    "# dat_2017.ready_csvs()\n",
    "# dat_2017.transform_all_names()\n",
    "# dat_2017.transform_all_names_check()\n",
    "\n",
    "# dat_2016.ready_csvs()\n",
    "# dat_2016.transform_all_names()\n",
    "# dat_2016.transform_all_names_check()\n",
    "\n",
    "# dat_2015.ready_csvs()\n",
    "# dat_2015.transform_all_names()\n",
    "# dat_2015.transform_all_names_check()\n",
    "\n",
    "# dat_2014.ready_csvs()\n",
    "# dat_2014.transform_all_names()\n",
    "# dat_2014.transform_all_names_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20314dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720747b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699d919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9f581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52330bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0256c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200a99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61e3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt.rename({\n",
    "#     #'Location': '', \n",
    "#     'Application_or_treatment': 'Application', \n",
    "#     'Product_or_nutrient_applied': 'Product',  \n",
    "#     'Date_of_application': 'Date', \n",
    "#     'Quantity_per_acre': 'Quantity/acre', \n",
    "#     'Application_unit': 'Unit'\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61213134",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e1e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e565ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224cf1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9615caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_2021.soil.data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd8522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # focus on one year and get that organized\n",
    "\n",
    "# # todo add as method in the class\n",
    "\n",
    "# dat_2021.meta.data.rename(columns=dict(zip(\n",
    "#     list(dat_2021.meta.data.columns),\n",
    "#     transform_name(input = list(dat_2021.meta.data.columns)))))\n",
    "\n",
    "# dat_2021.soil.data.rename(columns=dict(zip(\n",
    "#     list(dat_2021.soil.data.columns),\n",
    "#     transform_name(input = list(dat_2021.soil.data.columns)))))\n",
    "\n",
    "# dat_2021.soil.data.rename(columns=dict(zip(\n",
    "#     list(dat_2021.soil.data.columns),\n",
    "#     transform_name(input = list(dat_2021.soil.data.columns)))))\n",
    "\n",
    "# dat_2021.soil.data.rename(columns=dict(zip(\n",
    "#     list(dat_2021.soil.data.columns),\n",
    "#     transform_name(input = list(dat_2021.soil.data.columns)))))\n",
    "\n",
    "# dat_2021.soil.data.rename(columns=dict(zip(\n",
    "#     list(dat_2021.soil.data.columns),\n",
    "#     transform_name(input = list(dat_2021.soil.data.columns)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e74e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ad9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a9e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467658e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99068670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep it simple: \n",
    "# # Start by getting the minimum data that we want from 2021 in the past format.\n",
    "# # After we can use these minimum columns, think about expanding to other columns\n",
    "# meta = dat_2021.meta.data\n",
    "# phno = dat_2021.phno.data\n",
    "# wthr = dat_2021.wthr.data\n",
    "# soil = dat_2021.soil.data\n",
    "# mgmt = dat_2021.mgmt.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # meta\n",
    "# df = meta\n",
    "# df_contract = meta_contract\n",
    "# cols_from_to_dict = {\n",
    "#     # From                                                        To\n",
    "#     'Location':                                                   'ExperimentCode',\n",
    "#     'Previous_crop':                                              'PreviousCrop',\n",
    "#     'Pre-plant_tillage_method(s)':                                'PreplantTillage',\n",
    "#     'In-season_tillage_method(s)':                                'InseasonTillage',\n",
    "#     'System_determining_moisture':                                'MoistureSystem',\n",
    "#     'Pounds_needed_soil_moisture':                                'MoistureNeeded',\n",
    "#     'Longitude_of_field_corner_#1_(lower_left)':                  'ExpLon', #FIXME\n",
    "#     'Latitude_of_field_corner_#1_(lower_left)':                   'ExpLat', #FIXME\n",
    "#     'Trial_ID_(Assigned_by_collaborator_for_internal_reference)': 'Location' #Confirm\n",
    "#                            }\n",
    "\n",
    "\n",
    "# # phno\n",
    "# df = phno\n",
    "# df_contract = phno_contract\n",
    "# cols_from_to_dict = {\n",
    "#     # From                                                        To\n",
    "# #     'XXXXXXXXX': 'Pedigree', \n",
    "#     'XXXXXXXXX': 'F', \n",
    "#     'XXXXXXXXX': 'M', \n",
    "#     'XXXXXXXXX': 'ExperimentCode', \n",
    "# #     'XXXXXXXXX': 'Year', \n",
    "#     'Date_plot_planted_[MM/DD/YY]': 'DatePlanted', \n",
    "#     'Date_plot_harvested_[MM/DD/YY]': 'DateHarvested', \n",
    "#     'Anthesis_[days]': 'DaysToPollen', \n",
    "#     'Silking_[days]': 'DaysToSilk', \n",
    "#     'Plant_height_[cm]': 'Height', \n",
    "#     'Ear_height_[cm]': 'EarHeight', \n",
    "#     'Stand_count_[#_of_plants]': 'StandCount', \n",
    "#     'Root_lodging_[#_of_plants]': 'RootLodging', \n",
    "#     'Stalk_lodging_[#_of_plants]': 'StalkLodging', \n",
    "#     'Grain_moisture_[%]': 'PercentGrainMoisture', \n",
    "#     'Test_weight_[lbs]': 'TestWeight', \n",
    "#     'Plot_weight_[lbs]': 'PlotWeight', \n",
    "#     'Grain_yield_(bu/A)': 'GrainYield', \n",
    "#     'XXXXXXXXX': 'PercentStand'\n",
    "#                            }\n",
    "\n",
    "# ['Year',\n",
    "#  'Location',\n",
    "#  'State',\n",
    "#  'City',\n",
    "#  'Plot_length_(center-center_in_feet)',\n",
    "#  'Plot_area_(ft2)',\n",
    "#  'Alley_length_(in_inches)',\n",
    "#  'Row_spacing_(in_inches)',\n",
    "#  'Rows_per_plot',\n",
    "#  '#_seed_per_plot',\n",
    "#  'Experiment',\n",
    "#  'Source',\n",
    "#  'Pedigree',\n",
    "#  'Family',\n",
    "#  'Tester',\n",
    "#  'Replication',\n",
    "#  'Block',\n",
    "#  'Plot',\n",
    "#  'Plot_ID',\n",
    "#  'Range',\n",
    "#  'Pass',\n",
    "#  '',\n",
    "#  '',\n",
    "#  'Anthesis_[MM/DD/YY]',\n",
    "#  'Silking_[MM/DD/YY]',\n",
    "#  '',\n",
    "#  '',\n",
    "#  '',\n",
    "#  '',\n",
    "#  '',\n",
    "#  '',\n",
    "#  '',\n",
    "#  '',\n",
    "#  '',\n",
    "#  '',\n",
    "#  '',\n",
    "#  \"Plot_discarded_[enter_'yes'_or_blank]\",\n",
    "#  'Comments',\n",
    "#  'Filler',\n",
    "#  'Snap_[#_of_plants]']\n",
    "\n",
    "\n",
    "\n",
    "# # soil\n",
    "# # df = phno\n",
    "# # df_contract = soil_contract\n",
    "# # cols_from_to_dict = {\n",
    "# #                            }\n",
    "# # wthr # mgmt\n",
    "# # df = phno\n",
    "# # df_contract = wthr_contract\n",
    "# # cols_from_to_dict = {\n",
    "# #                            }\n",
    "\n",
    "\n",
    "# # pull columns that are needed from different datasets\n",
    "\n",
    "\n",
    "# # set the names\n",
    "# df = df.rename(columns=cols_from_to_dict)\n",
    "\n",
    "# # use the contract to update the dtypes\n",
    "# for key in df_contract.keys():\n",
    "#     if key in list(df.columns):\n",
    "#         df.loc[:, key] = df.loc[:, key].astype(df_contract[key])\n",
    "\n",
    "# list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a58c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_contract(dataframe = df, contract = df_contract, enforce = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8e9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check_contract(dataframe = meta, contract = meta_contract, enforce = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def quick_df(lst, nom): return pd.DataFrame(zip([nom for i in range(len(lst))], lst), columns = ['Source', 'Column'])\n",
    "# meta_2021\n",
    "# phno_2021\n",
    "# wthr_2021\n",
    "# soil_2021\n",
    "# mgmt_2021\n",
    "\n",
    "# quick_df(lst = meta_2021, nom = 'meta')\n",
    "# # quick_df(lst = phno_2021, nom = 'phno')\n",
    "# # quick_df(lst = wthr_2021, nom = 'wthr')\n",
    "# # quick_df(lst = soil_2021, nom = 'soil')\n",
    "# # quick_df(lst = mgmt_2021, nom = 'mgmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d754f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_meta = [list(e.meta.data.columns) for e in dat_sets]\n",
    "# cols_phno = [list(e.phno.data.columns) for e in dat_sets]\n",
    "# cols_wthr = [list(e.wthr.data.columns) for e in dat_sets]\n",
    "# cols_soil = [list(e.soil.data.columns) if type(e.soil.data) != type(None) else list() for e in dat_sets]\n",
    "# cols_mgmt = [list(e.mgmt.data.columns) if type(e.mgmt.data) != type(None) else list() for e in dat_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get list of list with years for one of the above column name lists\n",
    "# def get_year_lol(l = cols_mgmt):\n",
    "#     lol = [[[2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021][j] for i in range(len(l[j]))] for j in range(len(l))]\n",
    "#     return(lol)\n",
    "\n",
    "# def flatten_lol(lol): return [item for listWithin in lol for item in listWithin]\n",
    "\n",
    "# def cols_list_to_df(in_cols_lol = cols_soil,\n",
    "#                     data_type = 'soil'):\n",
    "#     year_list = flatten_lol(get_year_lol(in_cols_lol))\n",
    "#     cols_list = flatten_lol(in_cols_lol)\n",
    "#     name_list = [data_type for e in cols_list]\n",
    "#     dat = pd.DataFrame(\n",
    "#         zip(name_list,\n",
    "#             year_list,\n",
    "#             cols_list\n",
    "#            ),\n",
    "#         columns=[\"Type\", \"Year\", \"Col\"]\n",
    "#     )\n",
    "#     return dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dcaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pd.DataFrame({year : list(dat_obj.soil.data.columns)})\n",
    "# dat_obj = dat_2021\n",
    "# year = \"2021\"\n",
    "\n",
    "# list(dat_obj.soil.data.columns)\n",
    "\n",
    "# active_attr = dat_obj._get_active_attr()\n",
    "\n",
    "# #findme was working on collecting all the columns so they can be easily compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8edc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat = cols_list_to_df(in_cols_lol = cols_soil, data_type = 'soil')\n",
    "# # out of the box correspondance\n",
    "# # dat.pivot(index='Col', columns='Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb1c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # making W\n",
    "\n",
    "# # filter down input datasets\n",
    "# wthr.loc[:, [\n",
    "#     'Location', 'Date_Time', \n",
    "#     #NPK,\n",
    "#     # Temp MIN/mean/max\n",
    "#     'Temperature_[C]', \n",
    "#     'Dew_point_[C]',\n",
    "#     'Relative_humidity_[%]', \n",
    "#     'Solar_radiation_[W/m2]', \n",
    "\n",
    "#     'Rainfall_[mm]',\n",
    "#     'Wind_speed_[m/s]', 'Wind_direction_[degrees]', 'Wind_gust_[m/s]',\n",
    "#     #temp mean\n",
    "#     'Soil_temperature_[C]', 'Soil_moisture_[%VWC]', \n",
    "#     #'Soil_eC_[mS/cm]',\n",
    "#     'UV_light_[uM/m2s]', \n",
    "#     'PAR_[uM/m2s]'\n",
    "#     # other\n",
    "#             ]]\n",
    "\n",
    "# #TODO DAYMET\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620965c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05f061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80d2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706e886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ee465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a168fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8a4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f979896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # update names ----\n",
    "# dat_2018.rename_all_columns(\n",
    "#     colNamePath='./data/manual/UpdateColNames.csv',\n",
    "#     logfileName='2018_meta')\n",
    "\n",
    "# # regroup columns ----\n",
    "# # for 2016 and on\n",
    "# out = dat_2018.sort_cols_in_dfs(colGroupingsPath = './data/manual/UpdateColGroupings.csv')\n",
    "\n",
    "# # TODO\n",
    "# # for 2014, 2015 this df should be used\n",
    "# # pd.read_csv(data_adapter_path+'../data/external/UpdateColGroupings.csv').drop(columns = ['Genotype', 'GenotypeType'])\n",
    "\n",
    "# # Try to fix experiment codes --\n",
    "# for dfKey in list(out.keys()):\n",
    "#     df = out[dfKey]\n",
    "#     if 'ExperimentCode' in list(df):\n",
    "#         for key in list(replaceExperimentCodes.keys()):\n",
    "#             df.loc[df.ExperimentCode == key, 'ExperimentCode'] = replaceExperimentCodes[key]\n",
    "#     out[dfKey] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39aa187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f5325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88eecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO I was working on converting prep_data() below to be method\n",
    "#I had just gotten to \"# separate to make these easier to work with ----\"\n",
    "#After checking the previous cell sould be merged into the class too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
