[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "g2fd",
    "section": "",
    "text": "from g2fd.core import *\nThis file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "g2fd",
    "section": "Install",
    "text": "Install\npip install g2fd"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "g2fd",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "g2fd is the fast and easy way to access Genomes to Fields data.",
    "section": "",
    "text": "# def foo(): pass\n\n\nimport nbdev; nbdev.nbdev_export()\n\nData represented here is found at https://www.genomes2fields.org/resources/ Additionally you will need the file g2f_2015_agronomic information.csv which is reformatted to be more easily read in.\nCode is to be run using the conda g2fd enviroment, which can be built from the “g2fd_env.yml” file provided. 1. create the enviorment from the yml conda env create -f g2fd_env.yml 2. activate it conda activate g2fd 3. if changes are made, export the new version conda env export > g2fd_env.yml"
  },
  {
    "objectID": "01_mk_settings_documents.html",
    "href": "01_mk_settings_documents.html",
    "title": "g2fd",
    "section": "",
    "text": "import pickle\n# For all datasets except geno ----\n# ExperimentCode should be in this list\nexpectedExperimentCodes = [\"ARH1\",\n \"ARH2\",\n \"AZH1\",\n \"AZI1\",\n \"AZI1  AZI2\",\n \"AZI2\",\n \"COH1\",\n \"DEH1\",\n \"DEI1\",\n \"G2FDE1\",\n \"G2FIA3\",\n \"G2FIL1\",\n \"G2FIN1\",\n \"G2FMN2\",\n \"G2FNE1\",\n \"G2FNY1\",\n \"G2FWI-HYB\",\n \"G2FWI1\",\n \"G2FWI2\",\n \"G2F_IN_TX1\",\n \"GAH1\",\n \"GAH2\",\n \"GAI1\",\n \"GAI2\",\n \"GXE_inb_BO2\",\n \"GXE_inb_IA1\",\n \"GXE_inb_IA2\",\n \"GXE_inb_MO1\",\n \"GXE_inb_MO3\",\n \"GxE_inb_PA1\",\n \"IA(?)2\",\n \"IA(?)3\",\n \"IA(H4)\",\n \"IAH1\",\n \"IAH1 IAI1\",\n \"IAH1a\",\n \"IAH1b\",\n \"IAH1c\",\n \"IAH2\",\n \"IAH3\",\n \"IAH4\",\n \"IAI1\",\n \"IAI2\",\n \"IAI3\",\n \"IAI4\",\n \"ILH1\",\n \"ILH1  ILI1  ILH2\",\n \"ILH1 ILI1\",\n \"ILH2\",\n \"ILI1\",\n \"INH1\",\n \"INH1  INI1\",\n \"INH1 INI1\",\n \"INI1\",\n \"KSH1\",\n \"KSH1  KSI1\",\n \"KSH2\",\n \"KSH3\",\n \"KSI1\",\n \"MIH1\",\n \"MN(?)1\",\n \"MNH1\",\n \"MNI1\",\n \"MNI2\",\n \"MOH1\",\n \"MOH1  MOI1  MOH2  MOI2\",\n \"MOH1 MOI1\",\n \"MOH1-rep1\",\n \"MOH1-rep2\",\n \"MOH2\",\n \"MOH2 MOI2 MOI3\",\n \"MOI1\",\n \"MOI2\",\n \"NC1\",\n \"NCH1\",\n \"NCI1\",\n \"NEH1\",\n \"NEH1  NEH4\",\n \"NEH1 NEI1\",\n \"NEH2\",\n \"NEH3\",\n \"NEH3-Irrigated\",\n \"NEH4\",\n \"NEH4-NonIrrigated\",\n \"NY?\",\n \"NYH1\",\n \"NYH1  NYI1\",\n \"NYH1 NYI1\",\n \"NYH2\",\n \"NYH3  NYI2\",\n \"NYH3OHH1\",\n \"NYH4\",\n \"NYI1\",\n \"NYI2\",\n \"ONH1\",\n \"ONH2\",\n \"PAI1\",\n \"PAI1  PAI2\",\n \"PAI2\",\n \"SCH1\",\n \"SDH1\",\n \"SDI1\",\n \"TXH1\",\n \"TXH1  TXI1  TXI2\",\n \"TXH1-Dry\",\n \"TXH1-Early\",\n \"TXH1-Late\",\n \"TXH2\",\n \"TXH2  TXI3\",\n \"TXI1\",\n \"TXI2\",\n \"TXI3\",\n \"WIH1\",\n \"WIH1  WII1\",\n \"WIH1 WII1\",\n \"WIH2\",\n \"WIH2  WII2\",\n \"WII1\",\n \"WII2\",\n \"nan\"]\n\nreplaceExperimentCodes = {\n    'MOH1- rep 1': 'MOH1-rep1',\n    'MOH1- rep 2': 'MOH1-rep2',\n    'TXH1- Early': 'TXH1-Early',\n    'TXH1- Late': 'TXH1-Late',\n    'TXH1- Dry': 'TXH1-Dry',\n    'NEH3 (IRRIGATED)': 'NEH3-Irrigated',\n    'NEH4 (NON IRRIGATED)': 'NEH4-NonIrrigated'\n\n}\n\n\n# For specific datasets ----\n# For management ====\nexpectedManagementCols = [\n    'ExperimentCode',\n    'Application',\n    'Product',\n    'Date',\n    'QuantityPerAcre',\n    'ApplicationUnit',\n    'Year',\n    'Plot']\n\n\n# management.Application ====\nnewApplicationNames = {\n    'irrigation': ['irrigation',\n                   'Furrow Irrigation'],\n\n    'fertilizer': ['Post-fertilization',\n                   'Post-fertilization for DG2F',\n                   'Pre-emergent Herbicide',\n                   'Pre-plant fertilization',\n                   'broadcast fertilizer',\n                   'fertigation',\n                   'fertilize',\n                   'sidedress Nitrogen application',\n                   'Fertlizer',\n                  'soil remediant'],\n\n    'herbicide': ['Pre-emergent Herbicide',\n                  'Herbicide pre-emergence application',\n                  'Post-Herbicide',\n                  'pre-plant herbicide',\n                  'post-plant herbicide',\n                  'Post-Plant Herbicide',\n                  'adjuvant'],\n\n    'insecticide': ['insecticide'],\n    'fungicide': ['Fungicide '],\n\n    'ignored': ['FOR G2LA', 'FOR G2FE', 'For DG2F']\n}\n\n\n# management.ApplicationUnit ====\nexpectedApplicationUnits = ['lbs/Acre', 'oz/Acre',\n                            'in/Acre', 'gal/Acre', 'units/Acre', 'ton/Acre', 'GPA']\n\nreplaceApplicationUnits = {'lbs': 'lbs/Acre',\n                           'oz': 'oz/Acre',\n                           'gallons/acre': 'gal/Acre', \n                           'Gallons/acre': 'gal/Acre'\n                           }\n\n\n# management.Product ====\nnewIrrigationNames = {\n    'Water': ['water',\n              'Water',\n              'preirrigation',\n              'Overhead; recorded by weather station',\n              'irrigate',\n              'irrigated',\n              'RECORDED BY WEATHER STATION!',\n              'H2O']\n}\n \nnewFertilizerNames = {\n    'Manure Poultry': ['Poultry manure'],\n    # npk like\n    'nkpLike21-0-0-24': ['21-0-0-24'],\n    'nkpLike28-0-0-0.5': ['28-0-0-0.5'],\n    'nkpLike28-0-0-5': ['28-0-0-5'],\n    'Starter nkpLike50-45-10-10-2': ['Starter 50-45-10-10-2'],\n\n    'npkLike24-0-0-3': ['24-0-0-3'],\n    'npkLike20-10-0-1': ['20-10-0-1s'],\n    'npkLike27-0-0-6': ['27-0-0-6s'],\n    'npkLike30-26-0-6': ['npkLike30-26-0-6', 'Starter2x2', 'Liquid Starter 2x2 planter', 'Planter starter 2x2'],\n    # per 2019\n    # Location  Application_or_treatment    Product_or_nutrient_applied Date_of_application Quantity_per_acre   Application_unit\n    # OHH1  fertilize   Planter starter 2x2 Friday, June 7, 2019    30-26-0-6S  lbs/Acre\n    # \"Planter starter 2x2\" may == \"30-26-0-6S\"\n    \n    'npkLike6.7-13-33.5-4.4': ['6.7-13-33.5 4.4S dry fertilizer broadcast incorp', '6.7-13-33.5 with 4.4 sulfur'],\n\n    # Readily Converted NPK ----------------------------------------------------------------------------\n    'npk0-0-32': ['0-0-32'],\n    'npk0-0-60': ['0-0-60',\n                  '0-0-60 (NPK)',\n                  '0-0-60, preplant incorporated',\n                  '0-0-60 / pre plant incorporate'],\n    'npk4-10-24': ['4-10-24'],\n    'npk5.5-26-30': ['5.5-26-30 fertilizer'],\n    'npk7-0-40': ['npk7-0-40'],\n    'npk7.5-30-30': ['npk7.5-30-30'],\n    'npk7-33-21': ['7-33-21 broadcast prior to planting',\n                   '7-33-21 broadcasted',\n                   '7-33-21�', \n                   '7-33-21Ê' ],\n    'npk8-20-30': ['8/20/1930'],\n    'npk8.9-42.3-48.8': ['npk8.9-42.3-48.8'],\n    'npk9-24-27': ['9/24/2027'],\n    'npk9.3-31.7-0': ['9.3-31.7-0'],\n    'npk10-34-0': ['10-34-0'],\n    'npk10-10-30': ['npk10-10-30', '10-10-30 N-P-K'],\n    'npk11-37-0': ['11-37-0'],\n\n\n\n    'npk11-52-0': ['11-52-0, preplant incorporated', '11-52-0 / pre plant incorporate'],\n    'npk12.72-9.8-23.57': ['12.72-9.8-23.57'],\n    'npk13-19-24': ['13-19-24',\n                    '13-19-24�', \n                    '13-19-24Ê'],\n    'npk14-4-29': ['14-4-29'],\n    'npk14-0-0': ['140+0+0'],  # NOTE! Presumed typo. 140 -> 14\n    'npk15-18-24': ['15-18-24'],\n    'npk15-34-0': ['15-34-0'],\n    'npk16-0-24': ['16-0-24',\n                   'Applied 16-0-24'],\n    'npk18-46-0': ['18-46-0'],\n    'npk19-18-0': ['19-18-0� (liquid planter starter)', \n                   '19-18-0\\xa0 (liquid planter starter)', \n                   '19-18-0Ê (liquid planter starter)'\n                  ],\n     \n    'npk20-9-3': ['npk20-9-3'],\n    'npk20-10-0': ['20-10-0 (NPK)'],\n    'npk27-26-0': ['npk27-26-0'],\n    'npk28-0-0': ['28-0-0', '28%'],\n\n    'Manure npk31.5-4.3-16.3': ['Manure N31.5-P4.3-K16.3 from Digester'],\n    'npk32-0-0': ['32-0-0'],\n    'npk46-0-0': ['46-0-0'],\n\n    'npk50-50-0': ['npk50-50-0'],\n\n\n    'Starter 16-16-16': ['Starter 16-16-16'],\n    'Starter 16.55-6.59-2.03': ['Starter Fertilizer 16.55-6.59-2.03'],\n    'Starter2x2 16-16-16': ['Starter ferilizer 16-16-16 2x2'],\n    'Starter 20-10-0': ['Starter fertilizer 20-10-0'],\n\n    'npk10-0-30+12S': ['10-0-30-12%S'],  # Note extra Sulfer %\n    # Note extra Molybdenum\n    'Mono-Ammonium Phosphate npk11-52-0+Mo0.001': ['MAP 11-52-0'],\n    'npk11-37-0+4Zn': ['11-37-0 + 4 Zn'],  # Note extra zinc\n\n    # Nitrogen -----------------------------------------------------------------------------------------\n    'N28% npk28-0-0': ['28% Nitrogen',\n                       '28% Liquid Nitrogen',\n                       '28% Nitrogen',\n                       r'28% Nitrogen, Sidedress coulter injected',\n                       '28% Liquid Nitrogen',\n                      '28% N sidedress'],\n\n    'NH3': ['NH3', 'Anhydrous Ammonia'],\n    'NH3 N-serve': ['NH3 with N-serve'],\n \n\n    'N': ['N',\n          'Nitrogen',\n          'Nitrogen sidedress',\n          'Side dressed Nitrogen',\n          'Nitrogen - SuperU',\n          'nitrogen knifed',\n          'nitrogen (date approximate, pre plant)', \n          'nitrogen (date approximate, post plant)'],\n\n\n    'Urea46%': ['46% Urea'],\n    'Super Urea': ['Super U',\n                   'Super Urea'],\n\n    'Urea': ['Urea', 'urea', 'Granual Urea', 'Granular Urea'],\n\n    'UAN': ['UAN', 'UAN ', 'Nitrogen base (UAN)'],\n\n    'UAN28% npk28-0-0': ['UAN 28%',\n                         '28% UAN'],\n\n    'UAN30% npk30-0-0': ['30% UAN',\n                         'Side Dressed N with 30% UAN',\n                         'UAN side dressed  30-0-0',\n                         '30% UAN protected with nitrapyrin nitrification inhibitor (Nitrogen 42 GPA knifed between rows)',\n                         'UAN 30%'],\n    'UAN32% npk32-0-0': ['UAN; 32-0-0'\n                         ],\n\n    # Phosphorus ---------------------------------------------------------------------------------------\n    'P': ['P2O5', 'Phosphorous', 'P2O'],\n    'P+K': ['Phosphorus & Potassium (unknown form)'],\n\n    # Potassium ----------------------------------------------------------------------------------------\n    'Potash': ['Pot Ash', 'potash'],\n    'Potash npk0-0-60': ['potash 0-0-60'],\n    'K': ['K', 'K2O'],\n    # Other --------------------------------------------------------------------------------------------\n    'Agrotain': ['Agrotain'],\n    # Boron ===========================================================================================\n    'B10%': ['10% Boron', 'Boron 10%'],\n    # Zinc ============================================================================================\n    'Zn': ['Zinc', 'zinc', 'Zinc\\xa0\\xa0 (with planter fertilizer)'],\n    'Zn planter fertilizer': ['Zinc�� (with planter fertilizer)', 'ZincÊÊ (with planter fertilizer)'],\n    'Zn sulfate': ['zinc sulfate'],\n\n    # Sulfer ==========================================================================================\n    '24S': ['24S'],\n    '24S': ['Applied 24S'],\n    'S': ['S'],\n    # Manganese =======================================================================================\n    'KickStand Manganese 4% Xtra': ['KickStand� Manganese 4% Xtra', 'KickStand® Manganese 4% Xtra'],\n    # Ammonium Sulfate ================================================================================\n    'Ammonium sulfate': ['AMS 21-0-0-24',\n                         'Am Sulfate',\n                         'Ammonium sulfate',\n                         'ammonium sulfate'\n                         ],\n\n    # Multiple Components =============================================================================\n    'Orangeburg Mix': ['Orangeburg Mix'],\n    # pH ==============================================================================================\n    'Lime': ['Lime', 'lime', 'Ag Lime', 'Lime / pre plant incorporate']\n\n}\n\nnewHerbicideNames = {\n    # Aatrex -------------------------------------------------------------------------------------------\n    'Aatrex': ['Aatrex',\n               'Attrex',\n               'post emergence- Aatrex'],\n\n    # Atrazine -----------------------------------------------------------------------------------------\n    'Atrazine': ['Atrazine',\n                 'Atrazine ',\n                 'Atrazine 4L',\n                 'atrazine',\n                 'Atrazine ',\n                 'Applied Atrazine',\n                 'Atrazine '],\n\n    # Ammonium sulfate ---------------------------------------------------------------------------------\n    'Ammonium sulfate': ['AMS',\n                         'Am Sulfate',\n                         'ammonium sulfate',\n                         'AMS 21-0-0-24'],\n    'Ammonium sulfate .5%': ['AMS@.5%'],\n    # Accent Q -----------------------------------------------------------------------------------------\n    'Accent Q': ['Accent',\n                 'Accent Q',\n                 'Accent Q (DuPont)'],\n\n    # Acuron -------------------------------------------------------------------------------------------\n    'Acuron': ['Accuron',\n               'Acuron',\n               'Acuron (S-Metalochlor, Atrazine, Mesotrione)',\n               'Acuron herbicide'],\n    # Banvel -------------------------------------------------------------------------------------------\n    'Banvel': ['Banvel',\n               'post emergece- Banvel'],\n    # Bicep --------------------------------------------------------------------------------------------\n\n    'Bicep': ['Bicep', 'bicep'],\n    'Bicep Magnum': ['Bicep Magnum'],\n    'Bicep II': ['Bicep II'],\n    'Bicep II Magnum': ['Bicep II Magnum',\n                        'pre-emergence -Bicep II Magnum'],\n\n    'Bicep II Magnum Lite': ['Bicep Lite II magnum'],\n    # Buctril ------------------------------------------------------------------------------------------\n    'Buctril': ['Buctril',\n                'Buctril 4EC'],\n\n    # Callisto -----------------------------------------------------------------------------------------\n    'Callisto': ['Calisto', 'Callisto'],\n    'Callisto+Atrazine+AMS+COC': ['Calisto/Atrazine/AMS/COC'],\n\n    # Crop Oil -----------------------------------------------------------------------------------------\n    'Crop Oil': ['Crop Oil',\n                 'crop oil'],\n\n    # Dual ---------------------------------------------------------------------------------------------\n    'Dual': ['Daul ',\n             'Dual'],\n\n    'Dual Magnum': ['Dual Magnum',\n                    'Applied Dual Magnum'],\n\n    'Dual II Magnum': ['Dual II - Magnum',\n                       'Dual II Mag.',\n                       'Dual II Magnum',\n                       'Dual 2 Magnum'],\n\n    'Degree Xtra': ['Degree Extra',\n                    'Degree Xtra; Acetochlor + Atrazine'],\n    # Harness ------------------------------------------------------------------------------------------\n    'Harness': ['Harness'],\n    'Harness Xtra': ['Harness Xtra'],\n    'Harness Xtra 5.6L': ['Harness Xtra 5.6L; Acetachlor and Atrazine', 'Harness Xtra 5.6L;  Acetachlor and Atrazine'],\n    # Liberty 280SL ------------------------------------------------------------------------------------\n    'Liberty 280SL': ['Liberty 280SL'],\n    # Medal II -----------------------------------------------------------------------------------------\n    'Medal II EC': ['Medal II EC', 'Medal II (S- Metolachlor)'],\n    # NOTE: [],Presumed typo\n    'Medal II EC+Simazine+Explorer': ['Medal II, sirrazine, Explorer'],\n    # Primextra ----------------------------------------------------------------------------------------\n    'Primextra': ['Primextra'],\n    'Primextra Magnum': ['Primextra Magnum'],\n    # Prowl H2O ----------------------------------------------------------------------------------------\n    'Prowl H2O': ['Prowl',\n                  'Prowl H20 ',\n                  'Prowl H2O'],\n\n    # Roundup + Roundup PowerMAX ---------------------------------------------------------------------------------\n    'Roundup': ['Roundup', \n                'Round up'],\n    'Roundup PowerMAX': ['Roundup PowerMAX',\n                         'Roundup PowerMax',\n                         'Applied Round Up Pwrmax',\n                         'Roundup Power Max'],\n    'Roundup PowerMax II': ['Roundup PowerMax II'],\n    # Simazine -----------------------------------------------------------------------------------------\n    'Simazine': ['Simazine',\n                 'Simizine'],\n    'Simazine 4L': ['Simazine 4L'],\n\n    # Lexar --------------------------------------------------------------------------------------------\n    'Lexar': ['Lexar'],\n    'Lexar EZ': ['Lexar EZ',\n                 'Lexar E Z'],\n    # Status -------------------------------------------------------------------------------------------\n    'Status': ['Status',\n               'status'],\n\n    'Warrant': ['Warrant',\n                'warrant'],\n    # Only One Entry -----------------------------------------------------------------------------------\n    '24D': ['24D', '24-D'],\n    'AD-MAX 90': ['AD-MAX 90'],\n    'AccuQuest WM': ['AccuQuest WM'],\n    'Aremzon': ['Aremzon'],\n    'Balance Flex herbicide': ['Balance Flex herbicide'],\n    'Basagran': ['Basagran'],\n    'Bicep Light II': ['Bicep Light II'],\n    'Brawl': ['Brawl'],\n    'Broadloom': ['Broadloom'],\n    'Buccaneer Plus': ['Buccaneer Plus'],\n    'Evik DF': ['Evik DF', 'Applied Evik DF'],\n    'Explorer': ['Explorer'],  \n    'Glyphosate': ['Glyphosate'],  \n    'Evik': ['Evik'],\n    'Integrity': ['Integrity'],\n    'Gramoxone': ['Gramoxone'],\n    'Guardsman': ['Guardsman (atrazine, metalochlor)'],\n    'Impact': ['Impact', 'Applied Impact'],\n\n    'Keystone': ['Keystone'],\n    'Laudis': ['Laudis'],\n\n    'Methylated Seed Oil 1%': ['MSO @1%'],\n    'Me Too Lachlor II': ['Me Too Lachlor II'],\n    'Nonionic Surfactant': ['NIS'],  # adjuvant\n    'Option': ['Option'],\n    'Outlook+Infantry': ['Outlook/Infantry'],\n    'Permit': ['Permit'],\n    'Princep': ['Princep', 'Pricep'],\n    'Satellite HydroCap': ['Satellite HydroCap'],  # Pendimethalin 38.7%\n\n    'Steadfast Q': ['Steadfast Q'],\n\n    # has mixed components\n    'Makaze+Glyphosate+Medal II EC+S-metolachlor+Eptam+S-ethyl dipropylthiocarbamate+Trifluralin HF+Trifluralin+Atrazine 4L+Atrazine': ['Makaze, Glyphosate (isopropylamine salt); Medal II EC, S-metolachlor; Eptam, S-ethyl dipropylthiocarbamate; Trifluralin HF, Trifluralin; Atrazine 4L, Atrazine'],\n    'Cadet+Fluthiacet-methyl': ['Cadet; Fluthiacet-methyl'],\n    'Coragen+Chlorantraniliprole': ['Coragen; Chlorantraniliprole'],\n    'Prowl+Atrazine+Basagran': ['Prowl, Atrazine, Basagran'],\n    'Impact+Atrazine': ['Impact and Atrazine'],\n    'Instigate+Bicep': ['Instigate and Bicep'],\n    'Glyphosate+2,4D+Dicamba': ['Glyphosato, 2,4D, Dicambo', 'Glyphosate, 2,4D, Dicambo'],\n    'harness+atrazine': ['harness (acetechlor) + atrazine'],\n    '2-4,D+Round Up': ['2-4,D, Round Up'],\n    'Bicep II+RoundUp': ['Bicep II, Round Up'],\n    'Laudis+Atrazine': ['Laudis, Atrazine'],\n    'Counter+Radiant+Asana XL+Prevathon': ['Counter(terbufos), Radiant (spinetoram), Asana XL (Esfenvalerate), Prevathon (Chlorantraniliprole)'],\n    'Status+Accent': ['Status + Accent (diflufenzopyr, dicamba, nicosulfuron)'],\n    'Primextra+Callisto': ['Primextra+Callisto(s-metolachlor/benoxacor/atrazine+mesotrione)'],\n    'Accent+Banvel': ['Accent and Banvel - Nicosulfuron and Dicamba'],\n    'Callisto+AI Mesotrione+Dual 2 Magnum+AI S-metolachlor+Simazine+AI Simazine': ['Tank Mix (Callisto, AI Mesotrione) (Dual 2 Magnum, AI S-metolachlor) (Simazine, AI Simazine:2-chloro-4 6-bis(ethylamino)-s-triazine)'],\n    'Lumax+Atrazine': ['Lumax + Atrazine'],\n    'Lumax+glyphosate': ['Lumax + glyphosate'],\n    'Converge Flexx+Converge 480': ['Converge Flexx; Converge 480'],\n    'Callisto+Dual 2 Magnum+Simazine 4L': ['Tank Mix (Callisto; Dual 2 Magnum; Simazine 4L)'],\n    'Roundup+Evik': ['Roundup and Evik hand sprayed as needed'],\n    'Atrazine+DualIIMagnum': ['Atrazine 4L and Dual II Magnum']\n\n}\n\nnewInsecticideNames = {\n    'Counter 20G': ['Counter 20G',\n                    'Counter 20g',\n                    'Applied Counter 20G',\n                    'Coutner 20G'],\n    'Counter': ['Counter'],\n    'Force 3G': ['Force 3G',\n                 'Counter Force 3G',\n                 'Counter Force 3G (AI Tefluthrin)',\n                 'Force 3G (Tefluthrin)',\n                 'Force 3G; Tefluthrin'],\n    'Force 2G': ['Force 2G'],\n    'Lorsban 4E': ['Lorsban 4E'],\n    'Sevin XLR': ['Sevin XLR'],\n    'Sniper': ['Sniper', 'Sniper LFRÊ'],\n    # Liquid Fertilizer Ready #TODO how much fertilizer is being added through this?\n    'Sniper LFR': ['Sniper LFR�', 'Snipper LFR']\n}\n\nnewFungicideNames = {\n    'Delaro 325 SC': ['Fungicide - Delaro 325 SC']\n}\n\nnewMiscNames = {\n    # ignore\n    'nan': ['Disk',\n            'Field Cultivator',\n            'Hip and Rolled',\n            'Planted Corn Test',\n            'Planted Corn Filler Dyna-Grow D57VC51 RR',\n            'Begin Planting Research',\n            'nan']\n}\n\nnewProductNames = {}\nnewProductNames.update(newIrrigationNames)\nnewProductNames.update(newFertilizerNames)\nnewProductNames.update(newHerbicideNames)\nnewProductNames.update(newInsecticideNames)\nnewProductNames.update(newFungicideNames)\nnewProductNames.update(newMiscNames)\n\n\n# Renaming Dictionaries:\nsave_to_pkl = {\n    \"expectedExperimentCodes\" : expectedExperimentCodes,\n    \"replaceExperimentCodes\" : replaceExperimentCodes,\n    \"expectedManagementCols\" : expectedManagementCols,\n    \"newApplicationNames\" : newApplicationNames,\n    \"expectedApplicationUnits\" : expectedApplicationUnits,\n    \"replaceApplicationUnits\" : replaceApplicationUnits,\n    \"newProductNames\" : newProductNames}\n\nkeys = list(save_to_pkl.keys())\n\nfor key in keys:\n    with open('./data/manual/'+str(key)+'.pickle', 'wb') as f:\n        pickle.dump(save_to_pkl[key], f, pickle.HIGHEST_PROTOCOL)"
  },
  {
    "objectID": "standardize_deprecated.html",
    "href": "standardize_deprecated.html",
    "title": "Setup",
    "section": "",
    "text": "# Imports ----\nimport re\nimport numpy as np # for np.nan\nimport pandas as pd\npd.set_option('display.max_columns', None)\n# import os   # for write_log, delete_logs\nimport glob # for delete_all_logs\nfrom datetime import date, timedelta\n\nimport json # for saving a dict to txt with json.dumps\n\nimport pickle\n# import matplotlib as mpl\n# import matplotlib.pyplot as plt"
  },
  {
    "objectID": "standardize_deprecated.html#custom-functions",
    "href": "standardize_deprecated.html#custom-functions",
    "title": "Setup",
    "section": "Custom Functions",
    "text": "Custom Functions\n\n# def add_SiteYear(df):\n#     df['SiteYear'] = df['ExperimentCode'].astype(str) + '_' + df['Year'].astype(str)\n#     return(df)\n\n\n# metadata, 'Metadata', colGroupings\ndef check_col_types(data, grouping, colGroupings\n):\n#     data = metadata\n#     grouping = 'Metadata'\n#     colGroupings = pd.read_csv('../data/external/UpdateColGroupings.csv')\n\n    considerCols = colGroupings.loc[colGroupings[grouping].notna(), (grouping, grouping+'Type')]\n    considerCols= considerCols.reset_index().drop(columns = 'index')\n    changeMessages = []\n    issueMessages  = []\n\n    for i in range(considerCols.shape[0]):\n        considerCol = considerCols.loc[i, grouping]\n        considerColType = considerCols.loc[i, grouping+'Type']\n        # print(i, considerCol, considerColType)\n        \n        if considerCol in list(data):\n            # handle nans, datetime\n            if not pd.isna(considerColType):\n                try:\n                    if considerColType == 'datetime':\n                        data[considerCol]= pd.to_datetime(data[considerCol])\n                    else:\n                        data[considerCol]= data[considerCol].astype(considerColType)\n                    changeMessage = (considerCol+' -> type '+considerColType+'')\n                    changeMessages = changeMessages + [changeMessage]\n\n                except (ValueError, TypeError):\n                    data[considerCol]= data[considerCol].astype('string')\n                    issueMessage = ('Returned as string. '+considerCol+' could not be converted to '+considerColType)\n                    issueMessages = issueMessages + [issueMessage]\n\n            else: # i.e. if type == na\n                data[considerCol]= data[considerCol].astype('string')\n                issueMessage = (considerCol+' has no type specified (na). Returned as string.')\n                issueMessages = issueMessages + [issueMessage]\n\n    return(data, changeMessages, issueMessages)\n\n\n# 'metadata2018', colGroupings\ndef check_col_types_all_groupings(\n    dfNameStr, \n    colGroupings\n):\n    reTypedColumns = {\n        'Metadata': [], \n        'Soil': [],\n        'Phenotype': [],\n        'Genotype': [],\n        'Management': [],\n        'Weather': []\n    }\n\n    for grouping in reTypedColumns.keys(): #['Metadata', 'Soil', 'Phenotype', 'Genotype', 'Management', 'Weather']:\n    #     print(grouping)\n\n        outputList = check_col_types(data = eval(dfNameStr), grouping = grouping, colGroupings = colGroupings)\n    #     updatedDf = outputList[0]\n\n        reTypedColumns[grouping] = outputList[0]\n\n        if outputList[1] != []:\n            write_log(pathString= '../data/external/'+'log_'+dfNameStr+'_changes.txt', \n                      message = (grouping+' ========').encode())\n            for change in outputList[1]:\n                write_log(pathString= '../data/external/'+'log_'+dfNameStr+'_changes.txt', \n                          message = change.encode())\n\n        if outputList[2] != []:\n            write_log(pathString= '../data/external/'+'log_'+dfNameStr+'_issues.txt', \n                      message = (grouping+' ========').encode())\n            for issue in outputList[2]:\n                write_log(pathString= '../data/external/'+'log_'+dfNameStr+'_issues.txt', \n                          message = issue.encode())\n\n    return(reTypedColumns)\n\n\ndef check_col_types_all_groupings_across_tables(\n    metadataDfNameStr, # = 'metadata2018',\n    soilDfNameStr, # = 'soil2018',\n    weatherDfNameStr, # = 'weather2018',\n    managementDfNameStr, # = 'agro2018',\n    phenotypeDfNameStr, # = 'pheno2018',\n    genotypeDfNameStr, # = '',\n    colGroupings # = colGroupings\n):\n    metadataDict = []\n    soilDict = []\n    phenoDict = []\n    genoDict = []\n    agroDict = []\n    weatherDict = []\n\n    if metadataDfNameStr != '':\n        metadataDict = check_col_types_all_groupings(dfNameStr = metadataDfNameStr, \n                                                     colGroupings = colGroupings)\n    if soilDfNameStr != '':\n        soilDict = check_col_types_all_groupings(dfNameStr = soilDfNameStr, \n                                                 colGroupings = colGroupings)\n    if phenotypeDfNameStr != '':\n        phenoDict = check_col_types_all_groupings(dfNameStr = phenotypeDfNameStr, \n                                                  colGroupings = colGroupings)\n    if genotypeDfNameStr != '':\n        genoDict = check_col_types_all_groupings(dfNameStr = genotypeDfNameStr, \n                                                 colGroupings = colGroupings)\n    if managementDfNameStr != '':\n        agroDict = check_col_types_all_groupings(dfNameStr = managementDfNameStr , \n                                                 colGroupings = colGroupings)\n    if weatherDfNameStr != '':\n        weatherDict = check_col_types_all_groupings(dfNameStr = weatherDfNameStr, \n                                                    colGroupings = colGroupings)\n\n    outputList = [entry for entry in [metadataDict,\n                                      soilDict,\n                                      phenoDict,\n                                      genoDict, \n                                      agroDict, \n                                      weatherDict] if entry != {}]\n\n    return(outputList)\n\ndef combine_dfDicts(\n    key, # = 'Metadata', \n    dfDictList, # = [], #[metadataDict, soilDict, phenoDict, agroDict, weatherDict]\n    logfileName, # = 'combineMetadata',\n    colGroupings #= colGroupings\n):\n\n    desiredCols = list(colGroupings[key].dropna())\n    accumulator = pd.DataFrame()\n    \n    for dfDict in dfDictList:\n        if dfDict != []:\n            if accumulator.shape == (0,0):\n                accumulator = dfDict[key].loc[:, [col for col in list(dfDict[key]) if col in desiredCols]].drop_duplicates()\n            else:\n                temp = dfDict[key].loc[:, [col for col in list(dfDict[key]) if col in desiredCols]].drop_duplicates()\n                \n                # Only proceed if if there are more columns shared than just identifiers.\n                if [entry for entry in list(temp) if entry not in list(colGroupings['Keys'].dropna())] != []:\n\n                    # find mismatched columns\n                    downgradeToStr = [col for col in list(temp) if col in list(accumulator)]    \n#                     downgradeToStr = [col for col in downgradeToStr if type(temp[col][0]) != type(accumulator[col][0])]\n                    # This is not done as \n                    # downgradeToStr = [col for col in downgradeToStr if temp[col].dtype != accumulator[col].dtype]\n                    # because doing so causes TypeError: Cannot interpret 'StringDtype' as a data type\n                    # see also https://stackoverflow.com/questions/65079545/compare-dataframe-columns-typeerror-cannot-interpret-stringdtype-as-a-data-t\n\n                    for col in downgradeToStr:\n                        accumulator[col] = accumulator[col].astype(str)\n                        temp[col] = temp[col].astype(str)\n\n                        logMessage = \"Set to string:\"+col\n                        write_log(pathString= '../data/external/'+'log_'+logfileName+'_changes.txt', \n                                  message = logMessage.encode())\n\n                    # merge the now compatable dfs.\n                    accumulator= accumulator.merge(temp, how = 'outer').drop_duplicates()\n    return(accumulator)\n\n\n\n# combine_dfDicts(key, dfDictList, logfileName, colGroupings) -> accumulator\n#                   ^        ^\n#      e.g. 'metadata'       |_______________________________\n#                                                           \\  \n# check_col_types_all_groupings_across_tables(              |\n# metadataDfNameStr, ... weatherDfNameStr, colGroupings) -> [metadataDict ... weatherDict] if entry != {}]\n#                                 |                          ^           \n#                                 v                          |    \n# check_col_types_all_groupings(dfNameStr, colGroupings) -> {'Metadata': [], 'Soil': [], 'Phenotype': [], 'Genotype': [], 'Management': [], 'Weather': []}\n#                                 |                                      ^\n#                                 v                                      |\n#               check_col_types(data, grouping, colGroupings) -> return(data, changeMessages, issueMessages)"
  },
  {
    "objectID": "standardize_deprecated.html#remove-output-files",
    "href": "standardize_deprecated.html#remove-output-files",
    "title": "Setup",
    "section": "Remove output files",
    "text": "Remove output files"
  },
  {
    "objectID": "standardize_deprecated.html#clean-up-logs-intermediate-files",
    "href": "standardize_deprecated.html#clean-up-logs-intermediate-files",
    "title": "Setup",
    "section": "Clean up logs & intermediate files",
    "text": "Clean up logs & intermediate files\n\ndef delete_logs(pathString = '../data/external/'):\n    logs = glob.glob(pathString+'log*.txt') \n    for log in logs:\n        os.remove(log)\n\ndef write_log(pathString= '../data/external/logfile.txt', message = 'hello'):\n    # fileName = pathString.split(sep = '/')[-1]\n    # mk log file if it doesn't exist\n#     print(os.path.isfile(pathString))\n    if os.path.isfile(pathString) == False:\n        with open(pathString, 'wb') as textFile: \n            # wb allows for unicode \n            # only needed because there's a non-standard character in one of the column names\n            # https://www.kite.com/python/answers/how-to-write-unicode-text-to-a-text-file-in-python\n            textFile.write(''.encode(\"utf8\"))\n    else:    \n        with open(pathString, \"ab\") as textFile:\n            textFile.write(message)\n            textFile.write('\\n'.encode(\"utf8\"))\n\ndelete_logs(pathString = log_path)\n\n\n# rm database files\n# for filePath in glob.glob('../data/interim/*.db'):\n#     os.remove(filePath)"
  },
  {
    "objectID": "standardize_deprecated.html#data-objects",
    "href": "standardize_deprecated.html#data-objects",
    "title": "Setup",
    "section": "Data Objects",
    "text": "Data Objects\n\n## Pull in info/data objects for converting names\n\n# def get_json_dict(path):\n#     with open(path) as f:\n#         dat = json.load(f)\n#     return(dat)\n\n# # gets the first item of the dict, returns the assoicated list\n# def get_json_list(path):\n#     dat = get_json_dict(path)\n#     # get only the 0th value\n#     dat = dat[list(dat.keys())[0]]\n#     return(dat)\n\ndef load_pickle(path):\n    with open(path, 'rb') as f:\n        data = pickle.load(f)\n    return(data)\n\n# this is an unideal workaround that comes from some data having characters within the string (\",\",\"�\",\"Ê\",\"\\xa0\") that prevent using csv or json easily\n# Putting the conversions into a python script isn't as nice as a table but it's still plaintext so it can be version controlled easily.\nexpectedExperimentCodes = load_pickle(path = \"./data/manual/expectedExperimentCodes.pickle\")\nreplaceExperimentCodes  = load_pickle(path = \"./data/manual/replaceExperimentCodes.pickle\")\nexpectedManagementCols  = load_pickle(path = \"./data/manual/expectedManagementCols.pickle\")\nnewApplicationNames     = load_pickle(path = \"./data/manual/newApplicationNames.pickle\")\nexpectedApplicationUnits= load_pickle(path = \"./data/manual/expectedApplicationUnits.pickle\")\nreplaceApplicationUnits = load_pickle(path = \"./data/manual/replaceApplicationUnits.pickle\")\nnewProductNames         = load_pickle(path = \"./data/manual/newProductNames.pickle\")"
  },
  {
    "objectID": "standardize_deprecated.html#class-for-each-year.",
    "href": "standardize_deprecated.html#class-for-each-year.",
    "title": "Setup",
    "section": "Class for each year.",
    "text": "Class for each year.\nGoal: allow for reuse of methods on like data (e.g. metadata) while enabling year specific methods or data types. To accomplish this, each data type gets a class subclassed from the data_obj class. These are bundled into a generic g2f_year class which can be subclassed to apply to different years.\n\n## Generic data object template ====\nclass data_obj:\n    def __init__(self, path = None, data = None):\n        self.path = path\n        self.data = data \n        \n    def load_csv(self):\n        self.data = pd.read_csv(self.path, encoding = \"ISO-8859-1\", low_memory=False)\n\n\n## Data type specific classes ====\n# metadata\nclass meta_obj(data_obj):\n    pass\n\n# phenotype\nclass phno_obj(data_obj):\n    pass\n\n# genotype\nclass geno_obj(data_obj):\n    pass\n\n# weather\nclass wthr_obj(data_obj):\n    pass\n\n# soil\nclass soil_obj(data_obj):\n    pass\n\n# managment\nclass mgmt_obj(data_obj):\n    pass\n\n\n## Combined yearly data release ====\nclass g2f_year:\n    def __init__(self, \n                 meta_path = None, \n                 phno_path = None,  \n                 geno_path = None,  \n                 wthr_path = None, \n                 soil_path = None,\n                 mgmt_path = None,\n                 year = None\n                ):        \n        self.meta = meta_obj(meta_path)\n        self.phno = phno_obj(phno_path)\n        self.geno = geno_obj(geno_path)\n        self.wthr = wthr_obj(wthr_path)\n        self.soil = soil_obj(soil_path)\n        self.mgmt = mgmt_obj(mgmt_path)\n        self.year = year\n\n    # load all the csvs that are provided. Ignore genotypic data for now.\n    def ready_csvs(self):\n        for entry in ['meta', 'phno', #'geno', \n                                     'wthr', 'soil', 'mgmt']:\n            if getattr(self,  entry).path != None:\n                getattr(self, entry).load_csv()\n                \n    # Helper function mimicing `ready_csv`'s check for initialized \n    def _get_active_attr(self):\n        active_attr = [entry for entry in ['meta', 'phno', #'geno', \n                                          'wthr', 'soil', 'mgmt'\n                                         ] if type(getattr(self,  entry).data) != type(None)]\n        return(active_attr)\n                \n    \n    \n    \n    # want to standardize column names all at once _then_ we'll worry about bigger disagreements between years \n    # e.g. make `Location` and `LOCATION` equivalent then worry about equivalence with `ExperimentLocation`\n    # This will give us a capitalized snake case while allowing special names (e.g. elements) to stay capitalized\n    def _transform_name(self, input):\n        def _transform_string(input):\n            if type(input) != str:\n                print(\"input must be a string\")\n                return None\n            else:\n                transformed_string = input\n                # if there are multiple spaces in a replace them with 1\n                transformed_string = re.sub(' {2,}', ' ', transformed_string)\n                # remove space after % as in '% Silt'\n                transformed_string = re.sub('% ', '%', transformed_string)\n                transformed_string = re.sub(' %', '%', transformed_string)\n\n                # Split to Words\n                transformed_string = transformed_string.replace(' ', '_')\n                transformed_string = transformed_string.split('_')\n                # decapitalize words that are capitalized but a few words (e.g. elements)\n                # should be capitalized will be uncapitalized. Correct for this.\n                def _decapitalize(string): return (string[0].lower()+string[1:])\n                \"\"\"\n                This list holds all of the okayed strings that we want to stay uppercase\n                \"\"\"\n                ok_cap = [\"ID\",\"NWS\", \"LOI%\", \"N\", \"N/A\", \"K\", \"S\", \"H\", \"K\", \"Ca\", \"Mg\", \"Na\", \"P-III\", \"P\"]\n                transformed_string = [e if e in ok_cap else _decapitalize(e)  for e in transformed_string]\n                transformed_string = '_'.join(transformed_string)\n                # Now recapitalize the first letter\n                transformed_string = transformed_string[0].capitalize()+transformed_string[1:]\n\n                return transformed_string\n\n        if type(input) == list:\n            return [_transform_string(e) for e in input]\n        else:\n            return _transform_string(input)\n    \n    # walk over all active dataframes and apply simple renaming transformations\n    def transform_all_names(self):\n        for entry in self._get_active_attr():\n                cols_list = getattr(self, entry).data.columns\n                cols_list = list(cols_list) \n                getattr(self, entry).data = getattr(self, entry).data.rename(\n                    columns=dict(zip(cols_list,\n                                     self._transform_name(input = cols_list))))\n    \n    # This function acts to check if there are names in the csv that are not what we expect\n    # Year specific variations will be coerced to one of these when possible before standardizing them\n    def transform_all_names_check(self):\n        expected_cols_dict = {\n        'meta': [\n            'Experiment_code', \n            'Treatment', \n            'City', \n            'Farm', \n            'Field', \n            'Trial_ID_(Assigned_by_collaborator_for_internal_reference)', \n            'Soil_taxonomic_ID_and_horizon_description,_if_known', \n            'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)', \n            'Weather_station_latitude_(in_decimal_numbers_nOT_dMS)', \n            'Weather_station_longitude_(in_decimal_numbers_nOT_dMS)', \n            'Date_weather_station_placed', \n            'Date_weather_station_removed', \n            'In-field_weather_station_serial_number', \n            'In-field_weather_station_latitude_(in_decimal)', \n            'In-field_weather_station_longitude_(in_decimal)', \n            'Previous_crop', \n            'Pre-plant_tillage_method(s)', \n            'In-season_tillage_method(s)', \n            'Plot_length_(center-alley_to_center-alley_in_feet)', \n            'Alley_length_(in_inches)', \n            'Row_spacing_(in_inches)', \n            'Type_of_planter_(fluted_cone;_belt_cone;_air_planter)', \n            'Number_kernels_planted_per_plot_(>200_seed/pack_for_cone_planters)', \n            'System_determining_moisture', \n            'Pounds_needed_soil_moisture', \n            'Latitude_of_field_corner_#1_(lower_left)', \n            'Longitude_of_field_corner_#1_(lower_left)', \n            'Latitude_of_field_corner_#2_(lower_right)', \n            'Longitude_of_field_corner_#2_(lower_right)', \n            'Latitude_of_field_corner_#3_(upper_right)', \n            'Longitude_of_field_corner_#3_(upper_right)', \n            'Latitude_of_field_corner_#4_(upper_left)', \n            'Longitude_of_field_corner_#4_(upper_left)', \n            'Cardinal_heading_pass_1', \n            'Local_check_#1_pedigree', \n            'Local_check_#1_source', \n            'Local_check_#2_pedigree', \n            'Local_check_#2_source', \n            'Local_check_#3_pedigree', \n            'Local_check_#3_source', \n            'Local_check_#4_pedigree', \n            'Local_check_#4_source', \n            'Local_check_#5_pedigree', \n            'Local_check_#5_source', \n            'Issue/comment_#1', \n            'Issue/comment_#2', \n            'Issue/comment_#3', \n            'Issue/comment_#4', \n            'Issue/comment_#5', \n            'Issue/comment_#6', \n            'Issue/comment_#7', \n            'Issue/comment_#8', \n            'Issue/comment_#9', \n            'Issue/comment_#10',\n            #2018\n            'Station_ID',\n            #2015\n            'LBS_for_test', \n            'Date_Collected', \n            'Preplant_herb', \n            'Postplant_herb', \n            'Total_N', \n            'Total_P', \n            'Total_K', \n            'Fert_dates_1', \n            'Fert_dates_2', \n            'Fert_dates_3', \n            'Fert_dates_4', \n            'Fert_dates_5', \n            'Fert_dates_6', \n            'Fert_dates_7', \n            'Fert_dates_8', \n            'Type_of_fert',\n            # 2014\n            'Location_short', \n            'Type', \n            'Location', \n            'Insecticide', \n            'Pre-plant_herbicides', \n            'Post-plant_herbicides', \n            'Tillage_method', \n            'Soil_test_type', \n            'Soil_texture', \n            'Soil_pH', \n            'Total_nitrogen', \n            'Total_phosphorus', \n            'Total_potassium', \n            'Nutrient_application_schedule', \n            'Irrigated?', \n            'Weather_station_includes_irrigation', \n            'Fertigation_schedule', \n            'Irrigation_schedule', \n            'Local_check', \n            'Date_plot_harvested_[MM/DD/YY]', \n            'Date_plot_planted_[MM/DD/YY]', \n            'Inbred_reps', \n            'Inbred_plots', \n            'Collaborators', \n            'State'\n        ], \n        'phno': [\n            'Year', \n            'Field-Location', \n            'State', \n            'City', \n            'Plot_length_(center-center_in_feet)', \n            'Plot_area_(ft2)', \n            'Alley_length_(in_inches)', \n            'Row_spacing_(in_inches)', \n            'Rows_per_plot', \n            '#_seed_per_plot', \n            'Experiment', \n            'Source', \n            'Pedigree', \n            'Family', \n            'Tester', \n            'Replicate', \n            'Block', \n            'Plot', \n            'Plot_ID', \n            'Range', \n            'Pass', \n            'Date_plot_planted_[MM/DD/YY]', \n            'Date_plot_harvested_[MM/DD/YY]', \n            'Anthesis_[MM/DD/YY]', \n            'Silking_[MM/DD/YY]', \n            'Anthesis_[days]', \n            'Silking_[days]', \n            'Plant_height_[cm]', \n            'Ear_height_[cm]', \n            'Stand_count_[#_of_plants]', \n            'Root_lodging_[#_of_plants]', \n            'Stalk_lodging_[#_of_plants]', \n            'Grain_moisture_[%]', \n            'Test_weight_[lbs]', \n            'Plot_weight_[lbs]', \n            'Grain_yield_(bu/A)', \n            \"Plot_discarded_[enter_'yes'_or_blank]\", \n            'Comments', \n            'Filler', \n            'Snap_[#_of_plants]', \n            'Kernels/Packet', \n            \"Filler_[enter_'filler'_or_blank]\", \n            'Possible_subs', \n            'Confirmed_subs', \n            'Single_plant_biomass_in_july(g)', \n            'Single_plant_biomass_in_august(g)', \n            'RootPullingForce(kgf)_july', \n            'RootPullingForce(kgf)_august',\n            #2018\n            'RecId', \n            'Local_check_(Yes,_no)', \n            'Plot_length_field', \n            'Plot_area', \n            'Rows/Plot', \n            'Packet/Plot', \n            '#_seed', \n            'Stand_[%]', \n            'Root_lodging_[plants]', \n            'Stalk_lodging_[plants]', \n            'Plot_discarded_[enter_\"Yes\"_or_\"blank\"]', \n            'Filler_[enter_\"filler\"_or_\"blank\"]', \n            '[add_additional_measurements_here]',\n            #2016\n            'RecId', \n            'Plot_area', \n            'Rows/Plot', \n            'Packet/Plot', \n            'Kernels/Packet', \n            '#_seed', \n            'Plot_length_(center-alley_to_center-alley_in_feet)', \n            'Additional_measurements',\n            #2015\n            'Alley_length_(in_feet)'\n        ], \n        'geno': [], \n        'wthr': [\n            'Field_location', \n            'Station_ID', \n            'NWS_network', \n            'NWS_station', \n            'Date_key', \n            'Month', \n            'Day', \n            'Year', \n            'Time', \n            'Temperature_[C]', \n            'Dew_point_[C]', \n            'Relative_humidity_[%]', \n            'Solar_radiation_[W/m2]', \n            'Rainfall_[mm]', \n            'Wind_speed_[m/s]', \n            'Wind_direction_[degrees]', \n            'Wind_gust_[m/s]', \n            'Soil_temperature_[C]', \n            'Soil_moisture_[%VWC]', \n            'Soil_eC_[mS/cm]', \n            'UV_light_[uM/m2s]', \n            'PAR_[uM/m2s]',\n            # 2020\n            'CO2_[ppm]', \n            # 2018\n            'Photoperiod_[hours]',\n            'Column_altered', \n            'Altered_column_names', \n            'Cleaning_method', \n            'Comment',\n            # 2015\n            'Record_number', 'Location(s)', 'DOY', 'Datetime_[UTC]',\n            # 2014\n            'DOY'\n        ], \n        'soil': [\n            'Grower', \n            'Location', \n            'Date_received', \n            'Date_reported', \n            'E_depth', \n            '1:1_soil_pH', \n            'WDRF_buffer_pH', \n            '1:1_S_salts_mmho/cm', \n            'Texture_no', \n            'Organic_matter_LOI%', \n            'Nitrate-N_ppm_N', \n            'Lbs_N/A', \n            'Potassium_ppm_K', \n            'Sulfate-S_ppm_S', \n            'Calcium_ppm_Ca', \n            'Magnesium_ppm_Mg', \n            'Sodium_ppm_Na', \n            'CEC/Sum_of_cations_me/100g', \n            '%H_sat', \n            '%K_sat', \n            '%Ca_sat', \n            '%Mg_sat', \n            '%Na_sat', \n            'Mehlich_P-III_ppm_P', \n            '%Sand', \n            '%Silt', \n            '%Clay', \n            'Texture', \n            'Comments',\n            #2016\n            'Zinc_ppm_Zn', 'Iron_ppm_Fe', 'Manganese_ppm_Mn', 'Copper_ppm_Cu', 'Boron_ppm_B',\n            # 2015\n            'Lab_id', \n            'Lab_sample_id', \n            'Date_Collected', \n            'Cooperator', \n            'Plow_depth', \n            'PH', \n            'BpH', \n            'OM', \n            'P', \n            'K'\n        ], \n        'mgmt': [\n            'Location', \n            'Application_or_treatment', \n            'Product_or_nutrient_applied', \n            'Date_of_application', \n            'Quantity_per_acre', \n            'Application_unit',\n            # 2015\n            'Irrigation/Fertigation_(yes/no)', \n            'Weather_station_documents_irrigation?_(yes/no)', \n            'Nutrients_applied', \n            'Notes'\n        ]\n    }\n\n\n\n\n\n\n        expected_cols_lol = [expected_cols_dict[e] for e in list(expected_cols_dict.keys())]\n        def _flatten_lol(lol): return [item for listWithin in lol for item in listWithin]\n        expected_cols_all = _flatten_lol(expected_cols_lol)\n        \n        for entry in ['meta', 'phno', 'geno', 'wthr', 'soil', 'mgmt']:\n            if type(getattr(self,  entry).data) == type(None):\n                print(entry+\" is not initialized and None type.\")\n            else:\n                expected_cols = expected_cols_dict[entry]\n                found_cols = list(getattr(self,  entry).data.columns)\n                \n                unexpected_cols = [e for e in found_cols if e not in expected_cols]\n                unexpected_cols_all = [e for e in unexpected_cols if e not in expected_cols_all]\n                \n                if unexpected_cols != []:\n                    print(\"Unexpected in \"+entry)\n                    print(unexpected_cols)\n                if unexpected_cols_all != []:\n                    print(\"Unexpected in any table\")\n                    print(unexpected_cols_all)\n                    \n    \n    # This is a function I expect to be overwritten with year specific subclasses.\n    # Year specific naming conventions will likely require case by case handling   \n    def standardize_all_transformed_names(self, entry):\n        pass\n    \n#     def standardize_all_cols(self):\n#         for entry in ['meta', 'phno', 'geno', 'wthr', 'soil', 'mgmt']:\n#             if type(getattr(self,  entry).data) == type(None):\n#                 print(entry+\" is not initialized and None type.\")\n#             else:\n#                 self.rename_id_cols(entry = entry)\n#                 print(entry+\" ids renamed.\")  \n    \n    \n    \n    def add_year_col_to_all(self):\n        for entry in ['meta', 'phno', 'geno', 'wthr', 'soil', 'mgmt']:\n            if type(getattr(self,  entry).data) == type(None):\n                print(entry+\" is not initialized and None type.\")\n            else:\n                getattr(self,  entry).data.loc[:, \"Year\"] = self.year\n        \n        \n    # This is a function I expect to be overwritten with year specific subclasses.\n    # Year specific naming conventions will likely require case by case handling\n    def rename_id_cols(self, entry):\n        if type(getattr(self,  entry).data) == type(None):\n            print(entry+\" is not initialized and None type.\")\n        else:\n            rename_id_dict = {}\n            \n            if entry == \"meta\":\n                rename_id_dict.update({\n                    'Experiment_code': 'Location'\n                })\n            if entry == \"phno\":\n                rename_id_dict.update({\n                    'Field-Location':'Location',\n                    'Replicate':'Replication'\n                })\n            if entry == \"wthr\":\n                rename_id_dict.update({\n                    'Field_location':'Location' ,\n                    'Date_key':'Date_Time'\n                })\n            if entry == \"soil\":\n                pass\n#                 rename_id_dict = rename_id_dict.update({\n                     \n#                 })\n            if entry == \"mgmt\":\n                rename_id_dict.update({\n                    #'Location': '', \n                    'Application_or_treatment': 'Application', \n                    'Product_or_nutrient_applied': 'Product',  \n                    'Date_of_application': 'Date', \n                    'Quantity_per_acre': 'Quantity/acre', \n                    'Application_unit': 'Unit'\n                })\n            \n            getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_id_dict)\n            \n    def rename_all_id_cols(self):\n        for entry in ['meta', 'phno', 'geno', 'wthr', 'soil', 'mgmt']:\n            if type(getattr(self,  entry).data) == type(None):\n                print(entry+\" is not initialized and None type.\")\n            else:\n                self.rename_id_cols(entry = entry)\n                print(entry+\" ids renamed.\")        \n    \n    def check_for_id_cols(self):\n        expected_ids_dict = {\n            'meta': ['Year', 'Location'                           ], \n            'phno': ['Year', 'Location', 'Pedigree', 'Replication'],\n            'geno': [], \n            'wthr': ['Year', 'Location', 'Date_Time'   ],\n            'soil': ['Year', 'Location'                           ],\n            'mgmt': ['Year', 'Location', 'Date',           'Product']\n        }\n        expected_cols_lol = [expected_ids_dict[e] for e in list(expected_ids_dict.keys())]\n        def _flatten_lol(lol): return [item for listWithin in lol for item in listWithin]\n        expected_cols_all = _flatten_lol(expected_cols_lol)\n        \n        for entry in ['meta', 'phno', 'geno', 'wthr', 'soil', 'mgmt']:\n            if type(getattr(self,  entry).data) == type(None):\n                print(entry+\" is not initialized and None type.\")\n            else:\n                expected_but_not_found = [e for e in expected_ids_dict[entry] if e not in getattr(self,  entry).data.columns]\n                if expected_but_not_found == []:\n                    pass\n                    #print('No missing id columns')\n                else:\n                    print(entry)\n                    print('Missing id columns: \"'+'\", \"'.join(expected_but_not_found)+'\"')  \n        \n                \n                \n    def _hide_comments(self): \n        #     ## Update Column Names =====\n        #     # used in rename_columns()\n        #     def _update_col_names(self, df, nameDict):\n        #         changes = []\n\n        #         keysNotInDf = [key for key in nameDict.keys() if key not in list(df)]\n        #         for key in keysNotInDf:\n        #             for colName in [colName for colName in list(df) if colName in nameDict[key]]:\n        #                 df= df.rename(columns={colName: key})\n        #                 changes = changes + [(colName+' -> '+ key)]\n\n        #         issues = [colName for colName in list(df) if colName not in nameDict.keys()]\n\n        #         return([df, changes, issues])\n\n        #     # used in rename_all_columns()\n        #     def _rename_columns(\n        #         self,\n        #         dataIn,\n        #         colNamePath='./data/external/UpdateColNames.csv',\n        #         logfileName=''):\n\n        #         # Make col name update dict ===================================================\n        #         temp = pd.read_csv(colNamePath, low_memory=False) # <--- set because Rainfall is coded with numerics and string based na\n        #         temp = temp.loc[:, (['Standardized']+[entry for entry in list(temp) if entry.startswith('Alias')])]\n\n        #         # Turn slice of df into one to many dict\n        #         renamingDict = {}\n        #         for i in temp.index:\n        #             tempSlice = list(temp.loc[i, :].dropna())\n        #             renamingDict[tempSlice[0]] = tempSlice[1:]\n\n        #         # Update Column Names =========================================================\n        #         outputList = self._update_col_names(\n        #             df=dataIn,\n        #             nameDict= renamingDict)\n\n        #         updatedDf = outputList[0]\n        #         dfChanges = outputList[1]\n        #         dfIssues = outputList[2]\n\n        #         # Write logs ==================================================================\n        #         # TODO bring write_log into class\n        #         for change in dfChanges:\n        #             write_log(pathString='./data/logs/log_'+logfileName+'_changes.txt',\n        #                       message=str(change).encode(\"utf8\"))\n\n        #         for issue in dfIssues:\n        #             write_log(pathString='./data/logs/log_'+logfileName+'_issues.txt',\n        #                       message=('Not defined: \"'+str(issue)+'\"').encode(\"utf8\"))\n        #         return(updatedDf)\n\n        #     def rename_all_columns(self,\n        #                            colNamePath='./data/external/UpdateColNames.csv',\n        #                            logfileName=''\n        #                           ):\n        #         for entry in self._get_active_attr():\n        #             getattr(self, entry).data = self._rename_columns(dataIn=getattr(self, entry).data,\n        #                                                              colNamePath=colNamePath,\n        #                                                              logfileName=logfileName)\n        #     ## Regroup Columns ====\n        #     def sort_cols_in_dfs(self,\n        #                      colGroupingsPath = './data/external/UpdateColGroupings.csv' \n        #                     ):\n        #         colGroupings = pd.read_csv(colGroupingsPath)\n        #         inputData = [getattr(self, entry).data for entry in self._get_active_attr()]\n\n        #         keyCols= list((colGroupings.Keys).dropna())\n        #         groupings= [col for col in list(colGroupings) if ((col.endswith('Type') == False) & (col != 'Keys'))]\n\n        #         groupingAccumulators = {} #groupings.copy()\n\n        #         # i = 0\n        #         # j = 0\n\n        #         for j in range(len(groupings)):\n        #             #keepTheseCols = keyCols+list(colGroupings[groupings[j]].dropna())\n        #             keepTheseCols = list(colGroupings[groupings[j]].dropna())\n        #             accumulator = pd.DataFrame()\n        #             for i in range(len(inputData)):\n        #                 data = inputData[i]\n        #                 data = data.loc[:, [entry for entry in list(data) if entry in keepTheseCols]].drop_duplicates()\n\n        #                 # this will hopefully prevent dropping data that is text in datetime or numeric cols.\n        #                 # And then we don't have to worry about merging by like types here!\n        #                 data = data.astype(str)\n\n        #                 if ((accumulator.shape[0] == 0) | (accumulator.shape[1] == 0) ):\n        #                     accumulator = data\n        #                 elif ((data.shape[0] > 0) & (data.shape[1] > 0)) :\n        #     #                 print(list(accumulator))\n        #     #                 print(list(data))\n        #     #                 print('\\n')\n        #                     accumulator = accumulator.merge(data, how = 'outer')\n\n        #                 currentGrouping = groupings[j] # string about to be replace\n        #                 #     groupingAccumulators[j] = {currentGrouping : accumulator}\n        #                 #     groupingAccumulators = groupingAccumulators + {currentGrouping : accumulator}\n\n        #                 groupingAccumulators.update({currentGrouping : accumulator})\n\n        #         return(groupingAccumulators) \n        pass\n\n\ndef prlst(lst): \n    \"This is just a helper function to ease formating lists of strings with each entryon a different line.\"\n    print('[')\n    for e in lst:\n        if e != lst[-1]:\n            print(\"'\"+e+\"', \")\n        else:\n            print(\"'\"+e+\"'\")\n    print(']')\n    \ndef prlst2dct(lst): \n    \"This is just a helper function to ease formating lists of strings with each entryon a different line.\"\n    print('{')\n    for e in lst:\n        if e != lst[-1]:\n            print(\"'\"+e+\"': 'XXXXXXX', \")\n        else:\n            print(\"'\"+e+\"': 'XXXXXXX'\")\n    print('}')\n    \n    \nprlst([])\n# prlst2dct([])\n\n[\n]\n\n\n\n# Year specific subclasses ----\nclass g2f_2021(g2f_year):\n    # no year specific renames because we're bringing everything up to 2021 as a standard\n    pass\n\nclass g2f_2020(g2f_year):\n    pass\n\nclass g2f_2019(g2f_year):\n    def standardize_all_transformed_names(self):\n        for entry in ['meta', 'phno', #'geno', \n                      'wthr', 'soil', 'mgmt']:\n            if type(getattr(self,  entry).data) == type(None):\n                print(entry+\" is not initialized and None type.\")\n            else:\n                rename_cols_dict = {}\n                drop_cols_list = []\n\n                if entry == \"meta\":\n                    rename_cols_dict.update({\n                    })\n                if entry == \"phno\":\n                    rename_cols_dict.update({\n                        'Number_kernels_planted_per_plot_(>200_seed/pack_for_cone_planters)': 'Kernels/Packet', #2021.meta\n                        \"Filler_[enter_'filler'_or_blank]\": 'Filler'\n                    })\n                if entry == \"wthr\":\n                    rename_cols_dict.update({\n                    })\n                if entry == \"soil\":\n                    rename_cols_dict.update({\n                    })\n                if entry == \"mgmt\":\n                    rename_cols_dict.update({\n                    })\n\n                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n\n                if drop_cols_list != []:\n                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\n                \nclass g2f_2018(g2f_year):\n    def standardize_all_transformed_names(self):\n        for entry in ['meta', 'phno', #'geno', \n                      'wthr', 'soil', 'mgmt']:\n            if type(getattr(self,  entry).data) == type(None):\n                print(entry+\" is not initialized and None type.\")\n            else:\n                rename_cols_dict = {}\n                drop_cols_list = []\n\n                if entry == \"meta\":\n                    rename_cols_dict.update({\n                        'Weather_station_serial_number_(Last_four_digits,_e.g.\\xa0m2700s#####)': 'Station_ID'\n                    })\n                if entry == \"phno\":\n                    rename_cols_dict.update({\n                        # 'RecId': '', \n                        'Tester/Group': 'Tester', \n                        # 'Local_check_(Yes,_no)': '', \n                        # 'Plot_length_field': '', \n                        'Alley_length': 'Alley_length_(in_inches)', \n                        'Row_spacing': 'Row_spacing_(in_inches)', \n                        # 'Plot_area': '', \n                        # 'Rows/Plot': '', \n                        # 'Packet/Plot': '', \n                        # 'Kernels/Packet': '', \n                        # '#_seed': '', \n                        'Date_planted': 'Date_plot_planted_[MM/DD/YY]', \n                        'Date_harvested': 'Date_plot_harvested_[MM/DD/YY]', \n                        'Anthesis_[date]': 'Anthesis_[MM/DD/YY]', \n                        'Silking_[date]': 'Silking_[MM/DD/YY]', \n                        'Pollen_dAP_[days]': 'Anthesis_[days]', \n                        'Silk_dAP_[days]': 'Silking_[days]', \n                        'Stand_count_[plants]': 'Stand_count_[#_of_plants]', \n                        # 'Stand_[%]': '', \n                        # 'Root_lodging_[plants]': '', \n                        # 'Stalk_lodging_[plants]': '', \n                        'Test_weight_[lbs/bu]': 'Plot_weight_[lbs]', \n                        'Grain_yield_[bu/A]': 'Grain_yield_(bu/A)'#, \n                        # 'Plot_discarded_[enter_\"Yes\"_or_\"blank\"]': '', \n                        # 'Filler_[enter_\"filler\"_or_\"blank\"]': '', \n                        # '[add_additional_measurements_here]': '',\n                    })\n                if entry == \"wthr\":\n                    rename_cols_dict.update({\n                        'UVL_(uM/m^2s)': 'UV_light_[uM/m2s]', \n                        'Photoperiod_[_hours]': 'Photoperiod_[hours]'\n                    })\n\n                    drop_cols_list = ['Ï»¿Record_number']\n                if entry == \"soil\":\n                    rename_cols_dict.update({\n                        'Field_ID': 'Location',\n                        'Date_recieved': 'Date_received'\n                    })\n                if entry == \"mgmt\":\n                    rename_cols_dict.update({\n                    })\n\n                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n\n                if drop_cols_list != []:\n                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\n            \nclass g2f_2017(g2f_year):\n    def standardize_all_transformed_names(self):\n        for entry in ['meta', 'phno', #'geno', \n                      'wthr', 'soil', 'mgmt']:\n            if type(getattr(self,  entry).data) == type(None):\n                print(entry+\" is not initialized and None type.\")\n            else:\n                rename_cols_dict = {}\n                drop_cols_list = []\n\n                if entry == \"meta\":\n                    rename_cols_dict.update({\n                        'Weather_station_serial_number_(Last_four_digits,_e.g.\\xa0m2700s#####)': 'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)'\n                    })\n                if entry == \"phno\":\n                    rename_cols_dict.update({\n                        'LOCAL_cHECK_(Yes,_no[Blank])': 'Local_check_(Yes,_no)', \n                        'Alley_length': 'Alley_length_(in_inches)', \n                        'Row_spacing': 'Row_spacing_(in_inches)', \n                        'Date_planted': 'Date_plot_planted_[MM/DD/YY]', \n                        'Date_harvested': 'Date_plot_harvested_[MM/DD/YY]', \n                        'Anthesis_[date]': 'Anthesis_[MM/DD/YY]', \n                        'Silking_[date]': 'Silking_[MM/DD/YY]', \n                        'Pollen_dAP_[days]': 'Anthesis_[days]', \n                        'Silk_dAP_[days]': 'Silking_[days]', \n                        'Stand_count_[plants]': 'Stand_count_[#_of_plants]', \n                        'Test_weight_[lbs/bu]': 'Test_weight_[lbs]', \n                        'Grain_yield_[bu/A]': 'Grain_yield_(bu/A)', \n                        'Plot_discarded_[enter_\"yes\"_or_\"blank\"]': \"Plot_discarded_[enter_'yes'_or_blank]\"\n                    })\n                    drop_cols_list = ['Ï»¿Year']\n                if entry == \"wthr\":\n                    rename_cols_dict.update({\n                    })\n                if entry == \"soil\":\n                    rename_cols_dict.update({\n                    })\n                if entry == \"mgmt\":\n                    rename_cols_dict.update({\n                    })\n\n                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n\n                if drop_cols_list != []:\n                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\n                    \nclass g2f_2016(g2f_year):\n    def standardize_all_transformed_names(self):\n        for entry in ['meta', 'phno', #'geno', \n                      'wthr', 'soil', 'mgmt']:\n            if type(getattr(self,  entry).data) == type(None):\n                print(entry+\" is not initialized and None type.\")\n            else:\n                rename_cols_dict = {}\n                drop_cols_list = []\n\n                if entry == \"meta\":\n                    rename_cols_dict.update({\n                        'Weather_station_serial_number_(Last_four_digits,_e.g.\\xa0m2700s#####)': 'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)'\n                    })\n                if entry == \"phno\":\n                    rename_cols_dict.update({\n                        'LOCAL_cHECK_(Yes,_no[Blank])': 'Local_check_(Yes,_no)', \n                        'Plot_length_field': 'Plot_length_(center-alley_to_center-alley_in_feet)', \n                        'Alley_length': 'Alley_length_(in_inches)', \n                        'Row_spacing': 'Row_spacing_(in_inches)', \n                        'Date_planted': 'Date_plot_planted_[MM/DD/YY]', \n                        'Date_harvested': 'Date_plot_harvested_[MM/DD/YY]', \n                        'Anthesis_[date]': 'Anthesis_[MM/DD/YY]', \n                        'Silking_[date]': 'Silking_[MM/DD/YY]', \n                        'Pollen_dAP_[days]': 'Anthesis_[days]', \n                        'Silk_dAP_[days]': 'Silking_[days]', \n                        'Stand_count_[plants]': 'Stand_count_[#_of_plants]', \n                        'Root_lodging_[plants]': 'Root_lodging_[#_of_plants]', \n                        'Stalk_lodging_[plants]': 'Stalk_lodging_[#_of_plants]', \n                        'Test_weight_[lbs/bu]': 'Test_weight_[lbs]', \n                        'Grain_yield_[bu/A]': 'Grain_yield_(bu/A)', \n                        'Plot_discarded_[enter_\"yes\"_or_\"blank\"]': \"Plot_discarded_[enter_'yes'_or_blank]\", \n                        'Filler_[enter_\"filler\"_or_\"blank\"]': 'Filler', \n                        '[add_additional_measurements_here]': 'Additional_measurements'\n                    })\n                    drop_cols_list = ['Ï»¿Year']\n                if entry == \"wthr\":\n                    rename_cols_dict.update({\n                        'Time[Local]': 'Time', \n                        'Rainfall[mm]': 'Rainfall_[mm]', \n                        'Photoperiod[hours]': 'Photoperiod_[hours]'\n                    })\n                if entry == \"soil\":\n                    rename_cols_dict.update({\n                        'Zinc_ppm_zn': 'Zinc_ppm_Zn', \n                        'Iron_ppm_fe': 'Iron_ppm_Fe', \n                        'Manganese_ppm_mn': 'Manganese_ppm_Mn', \n                        'Copper_ppm_cu': 'Copper_ppm_Cu', \n                        'Boron_ppm_b': 'Boron_ppm_B'\n                    })\n                    drop_cols_list = ['Sample_type']\n                if entry == \"mgmt\":\n                    rename_cols_dict.update({\n                        'Experiment_code': 'Location', \n                        'Product/Nutrient_applied': 'Quantity_per_acre', \n                        'Application_unit\\n(lbs,_in,_oz_per_acre)': 'Application_unit'\n                    })\n                    drop_cols_list = ['Record_order', 'Record_ID']\n\n                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n\n                if drop_cols_list != []:\n                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\nclass g2f_2015(g2f_year):\n    def standardize_all_transformed_names(self):\n        for entry in ['meta', 'phno', #'geno', \n                      'wthr', 'soil', 'mgmt']:\n            if type(getattr(self,  entry).data) == type(None):\n                print(entry+\" is not initialized and None type.\")\n            else:\n                rename_cols_dict = {}\n                drop_cols_list = []\n\n                if entry == \"meta\":\n                    rename_cols_dict.update({\n                        'Experiment': 'Location',  \n                        'WS_sN': 'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)', \n                        'WS_lat': 'Weather_station_latitude_(in_decimal_numbers_nOT_dMS)',\n                        'WS_lon': 'Weather_station_longitude_(in_decimal_numbers_nOT_dMS)', \n                        'DateIn': 'Date_weather_station_placed', \n                        'DateOut': 'Date_weather_station_removed', \n                        'Tillage': 'Pre-plant_tillage_method(s)', \n                        'PlotLen': 'Plot_length_(center-alley_to_center-alley_in_feet)', \n                        'AlleyLen': 'Alley_length_(in_inches)', \n                        'RowSp': 'Row_spacing_(in_inches)', \n                        'PlanterType': 'Type_of_planter_(fluted_cone;_belt_cone;_air_planter)', \n                        'KernelsPerPlot': 'Number_kernels_planted_per_plot_(>200_seed/pack_for_cone_planters)', \n                        'Moisture_meter': 'System_determining_moisture', \n                        'Corner1_lat': 'Latitude_of_field_corner_#1_(lower_left)', \n                        'Corner1_lon': 'Longitude_of_field_corner_#1_(lower_left)', \n                        'Corner_2lat': 'Latitude_of_field_corner_#2_(lower_right)', \n                        'Corner2_lon': 'Longitude_of_field_corner_#2_(lower_right)',\n                        'Corner3_lat': 'Latitude_of_field_corner_#3_(upper_right)', \n                        'Corner3_lon': 'Longitude_of_field_corner_#3_(upper_right)', \n                        'Corner4_lat': 'Latitude_of_field_corner_#4_(upper_left)', \n                        'Corner4_lon': 'Longitude_of_field_corner_#4_(upper_left)', \n                        'Cardinal': 'Cardinal_heading_pass_1', \n                        'Date_of_soil_sampling': 'Date_Collected', \n                    })\n                    \n                    drop_cols_list = []\n                if entry == \"phno\":\n                    rename_cols_dict.update({ \n                        'LOCAL_cHECK_(Yes,_no[Blank])': 'Local_check_(Yes,_no)', \n                        'Plot_length_field_(center_to_center_in_feet)': 'Plot_length_(center-alley_to_center-alley_in_feet)', \n                        'Alley_length_(feet)': 'Alley_length_(in_feet)', \n                        'Row_spacing_(inches)': 'Row_spacing_(in_inches)', \n                        'Date_planted': 'Date_plot_planted_[MM/DD/YY]', \n                        'Date_harvested': 'Date_plot_harvested_[MM/DD/YY]', \n                        'Anthesis_[date]': 'Anthesis_[MM/DD/YY]', \n                        'Silking_[date]': 'Silking_[MM/DD/YY]', \n                        'Pollen_dAP_[days]': 'Anthesis_[days]', \n                        'Silk_dAP_[days]': 'Silking_[days]', \n                        'Stand_count_[plants]': 'Stand_count_[#_of_plants]', \n                        'Root_lodging_[plants]': 'Root_lodging_[#_of_plants]', \n                        'Stalk_lodging_[plants]': 'Stalk_lodging_[#_of_plants]', \n                        'Test_weight_[lbs/bu]': 'Test_weight_[lbs]', \n                        'Grain_yield_[bu/A]': 'Grain_yield_(bu/A)', \n                        'Plot_discarded_[enter_\"yes\"_or_\"blank\"]': \"Plot_discarded_[enter_'yes'_or_blank]\", \n                        'Filler_[enter_\"filler\"_or_\"blank\"]': 'Filler', \n                        '[add_additional_measurements_here]': 'Additional_measurements'\n                    })\n                    drop_cols_list = []\n                if entry == \"wthr\":\n                    rename_cols_dict.update({\n                        'Record_number': 'Record_number', \n                        'Experiment(s)': 'Location(s)', \n                        'Day_of_year': 'DOY', \n                        'Time_[Local]': 'Time', \n                        'Datetime_[UTC]': 'Datetime_[UTC]', \n                        'Soil_moisture_[%]': 'Soil_moisture_[%VWC]', \n                        'Photoperiod_[hours]': 'Photoperiod_[hours]'\n                    })\n                    drop_cols_list = []\n                if entry == \"soil\":\n                    rename_cols_dict.update({\n                        'LabID': 'Lab_id', \n                        'LabSmplID': 'Lab_sample_id', \n                        'SmplDate': 'Date_Collected', \n                        'CoopName': 'Cooperator', \n                        'Experiment': 'Location', \n                        'PlowDepth': 'Plow_depth', \n                    })\n                    drop_cols_list = []\n                if entry == \"mgmt\":\n                    rename_cols_dict.update({\n                    })\n                    drop_cols_list = []\n\n                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n\n                if drop_cols_list != []:\n                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\n                    \nclass g2f_2014(g2f_year):\n    def standardize_all_transformed_names(self):\n        for entry in ['meta', 'phno', #'geno', \n                      'wthr', 'soil', 'mgmt']:\n            if type(getattr(self,  entry).data) == type(None):\n                print(entry+\" is not initialized and None type.\")\n            else:\n                rename_cols_dict = {}\n                drop_cols_list = []\n\n                if entry == \"meta\":\n                    rename_cols_dict.update({\n                        'Location_name': 'Location_short', \n                        'Experiment': 'Location', \n                        'Long': 'Weather_station_longitude_(in_decimal_numbers_nOT_dMS)', \n                        'Lat': 'Weather_station_latitude_(in_decimal_numbers_nOT_dMS)', \n                        'Plot_length_(center_to_center_in_feet)': 'Plot_length_(center-alley_to_center-alley_in_feet)', \n                        'Alley_length_(inches)': 'Alley_length_(in_inches)', \n                        'Row_spacing_(inches)': 'Row_spacing_(in_inches)', \n                        'Number_kernels_planted': 'Number_kernels_planted_per_plot_(>200_seed/pack_for_cone_planters)', \n                        'Planter_type': 'Type_of_planter_(fluted_cone;_belt_cone;_air_planter)', \n                        'Harvest_date': 'Date_plot_harvested_[MM/DD/YY]', \n                        'Planting_date': 'Date_plot_planted_[MM/DD/YY]',\n                        'Folder': 'State', \n                        'Weather_station_serial_number': 'Weather_station_serial_number_(Last_four_digits,_e.g._m2700s#####)'\n                    })\n                    drop_cols_list = ['Traits', 'Data_file_name', 'Metadata_file_name', \n                        'Additional_metadata', \n                        'Weather-folder']\n\n                if entry == \"phno\":\n                    rename_cols_dict.update({\n                        'LOCAL_cHECK_(Yes,_no[Blank])': 'Local_check_(Yes,_no)', \n                        'Alley_length': 'Alley_length_(in_inches)', \n                        'Row_spacing': 'Row_spacing_(in_inches)', \n                        'Date_planted': 'Date_plot_planted_[MM/DD/YY]', \n                        'Date_harvested': 'Date_plot_harvested_[MM/DD/YY]', \n                        'Anthesis_[date]': 'Anthesis_[MM/DD/YY]', \n                        'Silking_[date]': 'Silking_[MM/DD/YY]', \n                        'Pollen_dAP_[days]': 'Anthesis_[days]', \n                        'Silk_dAP_[days]': 'Silking_[days]', \n                        'Stand_count_[plants]': 'Stand_count_[#_of_plants]', \n                        'Test_weight_[lbs/bu]': 'Test_weight_[lbs]', \n                        'Grain_yield_[bu/A]': 'Grain_yield_(bu/A)', \n                        'Plot_discarded_[enter_\"yes\"_or_\"blank\"]': \"Plot_discarded_[enter_'yes'_or_blank]\"\n                    })\n                    drop_cols_list = ['Ï»¿Year']\n                if entry == \"wthr\":\n                    rename_cols_dict.update({\n                        'Day_[Local]': 'Day', \n                        'Month_[Local]': 'Month', \n                        'Year_[Local]': 'Year', \n                        'Day_of_year_[Local]': 'DOY', \n                        'Time_[Local]': 'Time'\n                    })\n                    drop_cols_list = ['Record_number', 'Datetime_[UTC]']\n                if entry == \"soil\":\n                    rename_cols_dict.update({\n                    })\n                if entry == \"mgmt\":\n                    rename_cols_dict.update({\n                    })\n\n                getattr(self,  entry).data = getattr(self,  entry).data.rename(columns = rename_cols_dict)\n\n                if drop_cols_list != []:\n                    getattr(self,  entry).data = getattr(self,  entry).data.drop(drop_cols_list, axis=1)\n\n\n# dat_2021.meta.data.columns.sort_values()\n\n\n# dat_2015.meta.data.head()\n\n\nprlst2dct([])\n\n{\n}\n\n\n\n# dat_2021.meta.data.columns.sort_values()\n\n\n# dat_2016.phno.data.head()\n\n\n# Initialize all released datasets ----\ndat_2021 = g2f_2021(\n    meta_path = './data/raw/GenomesToFields_G2F_data_2021/z._2021_supplemental_info/g2f_2021_field_metadata.csv', \n    phno_path = './data/raw/GenomesToFields_G2F_data_2021/a._2021_phenotypic_data/g2f_2021_phenotypic_clean_data.csv', # also contains 'g2f_2021_phenotypic_raw_data.csv' \n    geno_path = None,  \n    wthr_path = './data/raw/GenomesToFields_G2F_data_2021/b._2021_weather_data/g2f_2021_weather_cleaned.csv',\n    soil_path = './data/raw/GenomesToFields_G2F_data_2021/c._2021_soil_data/g2f_2021_soil_data.csv',\n    mgmt_path = './data/raw/GenomesToFields_G2F_data_2021/z._2021_supplemental_info/g2f_2021_agronomic_information.csv',\n    year = 2021\n)\n\ndat_2020 = g2f_2020(\n    meta_path = './data/raw/GenomesToFields_G2F_data_2020/z._2020_supplemental_info/g2f_2020_field_metadata.csv', \n    phno_path = './data/raw/GenomesToFields_G2F_data_2020/a._2020_phenotypic_data/g2f_2020_phenotypic_clean_data.csv', # also contains 'g2f_2020_phenotypic_raw_data.csv' \n    geno_path = None,  \n    wthr_path = './data/raw/GenomesToFields_G2F_data_2020/b._2020_weather_data/2020_weather_cleaned.csv',\n    soil_path = './data/raw/GenomesToFields_G2F_data_2020/c._2020_soil_data/g2f_2020_soil_data.csv',\n    mgmt_path = './data/raw/GenomesToFields_G2F_data_2020/z._2020_supplemental_info/g2f_2020_agronomic_information.csv',\n    year = 2020\n)\n\ndat_2019 = g2f_2019(\n    meta_path = './data/raw/GenomesToFields_data_2019/z._2019_supplemental_info/g2f_2019_field_metadata.csv', \n    phno_path = './data/raw/GenomesToFields_data_2019/a._2019_phenotypic_data/g2f_2019_phenotypic_clean_data.csv', \n    geno_path = None,  \n    wthr_path = './data/raw/GenomesToFields_data_2019/b._2019_weather_data/2019_weather_cleaned.csv',\n    soil_path = './data/raw/GenomesToFields_data_2019/c._2019_soil_data/g2f_2019_soil_data.csv',\n    mgmt_path = './data/raw/GenomesToFields_data_2019/z._2019_supplemental_info/g2f_2019_agronomic_information.csv',\n    year = 2019\n)\n\ndat_2018 = g2f_2018(\n    meta_path = './data/raw/GenomesToFields_G2F_Data_2018/e._2018_supplemental_info/g2f_2018_field_metadata.csv', \n    phno_path = './data/raw/GenomesToFields_G2F_Data_2018/a._2018_hybrid_phenotypic_data/g2f_2018_hybrid_data_clean.csv', \n    geno_path = None,  \n    wthr_path = './data/raw/GenomesToFields_G2F_Data_2018/b._2018_weather_data/g2f_2018_weather_clean.csv',\n    soil_path = './data/raw/GenomesToFields_G2F_Data_2018/c._2018_soil_data/g2f_2018_soil_data.csv',\n    mgmt_path = './data/raw/GenomesToFields_G2F_Data_2018/e._2018_supplemental_info/g2f_2018_agronomic information.csv',\n    year = 2018\n)\n\ndat_2017 = g2f_2017(\n    meta_path = './data/raw/G2F_Planting_Season_2017_v1/z._2017_supplemental_info/g2f_2017_field_metadata.csv', \n    phno_path = './data/raw/G2F_Planting_Season_2017_v1/a._2017_hybrid_phenotypic_data/g2f_2017_hybrid_data_clean.csv', \n    geno_path = None,  \n    wthr_path = './data/raw/G2F_Planting_Season_2017_v1/b._2017_weather_data/g2f_2017_weather_data.csv',\n    soil_path = './data/raw/G2F_Planting_Season_2017_v1/c._2017_soil_data/g2f_2017_soil_data_clean.csv',\n    mgmt_path = './data/raw/G2F_Planting_Season_2017_v1/z._2017_supplemental_info/g2f_2017_agronomic information.csv',\n    year = 2017\n)\n\ndat_2016 = g2f_2016(\n    meta_path = './data/raw/G2F_Planting_Season_2016_v2/z._2016_supplemental_info/g2f_2016_field_metadata.csv', \n    phno_path = './data/raw/G2F_Planting_Season_2016_v2/a._2016_hybrid_phenotypic_data/g2f_2016_hybrid_data_clean.csv', \n    geno_path = None,  \n    wthr_path = './data/raw/G2F_Planting_Season_2016_v2/b._2016_weather_data/g2f_2016_weather_data.csv',\n    soil_path = './data/raw/G2F_Planting_Season_2016_v2/c._2016_soil_data/g2f_2016_soil_data_clean.csv',\n    mgmt_path = './data/raw/G2F_Planting_Season_2016_v2/z._2016_supplemental_info/g2f_2016_agronomic_information.csv',\n    year = 2016\n)\n\ndat_2015 = g2f_2015(\n              # print('Note! Many management factors are recorded in 2015!')\n    meta_path = './data/raw/G2F_Planting_Season_2015_v2/z._2015_supplemental_info/g2f_2015_field_metadata.csv', \n    phno_path = './data/raw/G2F_Planting_Season_2015_v2/a._2015_hybrid_phenotypic_data/g2f_2015_hybrid_data_clean.csv', \n    geno_path = None,  \n    wthr_path = './data/raw/G2F_Planting_Season_2015_v2/b._2015_weather_data/g2f_2015_weather.csv',\n    soil_path = './data/raw/G2F_Planting_Season_2015_v2/d._2015_soil_data/g2f_2015_soil_data.csv',\n              # There is data to be had but it's not formatted in a machine friendly way.\n              # I've reformatted it to be easy to read in.\n    mgmt_path = './data/Manual_old/g2f_2015_agronomic information.csv',\n    year = 2015\n)\n\ndat_2014 = g2f_2014(\n    meta_path = './data/raw/G2F_Planting_Season_2014_v4/z._2014_supplemental_info/g2f_2014_field_characteristics.csv', \n    phno_path = './data/raw/G2F_Planting_Season_2014_v4/a._2014_hybrid_phenotypic_data/g2f_2014_hybrid_data_clean.csv', \n    geno_path = None,  \n    wthr_path = './data/raw/G2F_Planting_Season_2014_v4/b._2014_weather_data/g2f_2014_weather.csv',\n              # no soil 2014, some info in metadata\n    soil_path = None, \n              # no agro 2014, some info in metadata\n    mgmt_path = None,\n    year = 2014\n)\n\n# # Placeholder for GENETICS data ----\n# import re\n# newDecoderNames={\n#          'Sample Names': 'Sample',\n# 'Inbred Genotype Names': 'InbredGenotype',\n#                'Family': 'Family'\n# }\n\n# geno2018= pd.read_table(\n#     data_adapter_path+'../data/raw/GenomesToFields_G2F_Data_2018/d._2018_genotypic_data/G2F_PHG_minreads1_Mo44_PHW65_MoG_assemblies_14112019_filtered_plusParents_sampleDecoder.txt',\n#     delimiter= ' ', \n#     header=None, \n#     names= ['Sample Names', 'Inbred Genotype Names'])#.sort_values('Inbred Genotype Names')\n\n# # Add to enable join to phenotype data\n# def clip_2018_family(i):\n#     # for i in range(geno2018.shape[0]):\n#     if(re.search('_', geno2018['Inbred Genotype Names'][i]) == None):\n#         newName = str(geno2018['Inbred Genotype Names'].str.split(\"_\")[i][0])\n#     else:\n#         newName = str(geno2018['Inbred Genotype Names'].str.split(\"_\")[i][0:1][0]\n#                      )+'_'+str(geno2018['Inbred Genotype Names'].str.split(\"_\")[i][1:2][0])\n#     return(newName)\n    \n# geno2018['Family'] = [clip_2018_family(i) for i in range(len(geno2018[['Inbred Genotype Names']]))]\n\n# # Data is in AGP format saved as a h5\n# geno2017= pd.read_excel(data_adapter_path+'../data/raw/G2F_Planting_Season_2017_v1/d._2017_genotypic_data/g2f_2017_gbs_hybrid_codes.xlsx')\n# geno2017['Year']= '2017'\n\n\n# TODO\n# dat_sets = [dat_2014, dat_2015, dat_2016, dat_2017, dat_2018, dat_2019, dat_2021]\n# for e in dat_sets:\n#     e.ready_csvs()\n\nthe big thing to check is that each table has the key columns below. With those we should be able to access each value and can turn to stardardization of terms. The key columns for the data are\n\n\n\nmeta\nphno\ngeno\nwthr\nsoil\nmgmt\n\n\n\n\nYear\nYear\n\nYear\nYear\nYear\n\n\nLocation\nLocation\n\nLocation\nLocation\nLocation\n\n\n\nPedigree\n\n\n\n\n\n\n\nReplication\n\n\n\n\n\n\n\n\n\n\n\nMonth\n\n\n\n\n\n\n\nDay\n\n\n\n\n\nDate_Time\n\n\n\n\n\n\n\n\n\nApplication\n\n\n\n\n\n\n\nProduct"
  },
  {
    "objectID": "freshstart_2020.html",
    "href": "freshstart_2020.html",
    "title": "Process Data from 2020 into a consistent format.",
    "section": "",
    "text": "# Imports ----\nimport re\nimport numpy as np \nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport pickle"
  },
  {
    "objectID": "freshstart_2020.html#sanitization-functions",
    "href": "freshstart_2020.html#sanitization-functions",
    "title": "Process Data from 2020 into a consistent format.",
    "section": "Sanitization functions",
    "text": "Sanitization functions\nThe pattern to use is: 1. Alter the dataframe 1. Test the dataframe against expectations\nThe main tasks that need to be completed are: 1. Identify values that can’t be converted to the expected data type. The “find_unconvertable_” family of functions should be used. 1. find_unconvertable_datetimes\n\nFor simple renaming (e.g. misspellings) or splitting non-tidy data into two rows (“entry1-entry2” -> “entry1”, “entry2”) use sanitize_col\nMove values that are ambigous but pertain to data imputation to “Imputation_Notes” using relocate_to_Imputation_Notes\nIf new columns need to be added (e.g. mgmt.Ingredient for parsed components of Product (e.g. elements) ) this should be accomplished with safe_create_col.\nAny one off changes should be accomplised manually.\nConfirm columns match the expected types with check_df_dtype_expectations, and report mismatches.\n\nThese steps should be completed for each dataframe in turn to minimize the cognitive load of the reader."
  },
  {
    "objectID": "freshstart_2020.html#sanitization-column-data-type-expectations",
    "href": "freshstart_2020.html#sanitization-column-data-type-expectations",
    "title": "Process Data from 2020 into a consistent format.",
    "section": "Sanitization: Column data type expectations",
    "text": "Sanitization: Column data type expectations\nNote: to handle missing values some columns that would otherwise be ints are floats\n\nsval_col_dtypes = mk_dtype_dict(name = 'sval')\nwthr_col_dtypes = mk_dtype_dict(name = 'wthr')\nmgmt_col_dtypes = mk_dtype_dict(name = 'mgmt')"
  },
  {
    "objectID": "freshstart_2020.html#static-values-within-season",
    "href": "freshstart_2020.html#static-values-within-season",
    "title": "Process Data from 2020 into a consistent format.",
    "section": "Static values (within season)",
    "text": "Static values (within season)\n\nDatetime containing columns\n\n# convert the date cols into datetime. Lean on pd.to_datetime() to infer the format, assume that each site uses the same format.\n\nfor e in ['Planted_Unit_Datetime', \n    'Harvested_Unit_Datetime', \n    'Anthesis_Unit_Datetime', \n    'Silking_Unit_Datetime', \n    'Recieved_Date_Unit_Datetime', \n    'Processed_Date_Unit_Datetime', \n    'Weather_Station_Placed_Unit_Datetime', \n    'Weather_Station_Removed_Unit_Datetime'\n    ]:\n\n    sval['Datetime_Temp'] = pd.to_datetime(np.nan)\n\n    for code in list(sval.Experiment_Code.drop_duplicates()):\n    # code = list(sval.Experiment_Code.drop_duplicates())[0]\n        sval.loc[sval.Experiment_Code == code, 'Datetime_Temp'\n                 ] = pd.to_datetime(sval.loc[sval.Experiment_Code == code, e])\n\n    sval.loc[:, e] = sval.loc[:, 'Datetime_Temp'] \n\nsval = sval.drop(columns = 'Datetime_Temp')\n\n\n# convert types\nfor e in ['Alley_Length_Unit_Inches', 'Row_Spacing_Unit_Inches', 'Pounds_Needed_Soil_Moisture',\n         'Anthesis_Unit_Days', 'Silking_Unit_Days', 'Kernels_Per_Plot']:\n    err_list = find_unconvertable_numerics(df_col = sval[e], index = False)\n    if err_list != []:\n        print(e)\n        print(err_list)\n    else:\n        sval[e] = sval[e].astype('float')\n\n\n# to bool\nsval = sanitize_col(\n    df = sval, \n    col = 'Discarded', \n    simple_renames= {\n        'Yes':'True',\n        'yes':'True'}, \n    split_renames= {})\n\n# set missing to false\nsval.loc[sval.Discarded.isna(), 'Discarded'] = 'False'\nsval.Discarded = sval.Discarded.map({'True': True, 'False': False})\n\n\n\nSimple Columns\n\n# to bool\nsval['phno'] = sval['phno'].astype('bool')\nsval['soil'] = sval['soil'].astype('bool')\nsval['meta'] = sval['meta'].astype('bool')\n\n# to string\nsval = cols_astype_string(\n    df = sval, \n    col_list = [key for key in sval_col_dtypes.keys() if sval_col_dtypes[key] == 'string'])\n\nsval.Year = year_string\nsval.Year = sval.Year.astype('string')\n\n\n\nCheck Success\n\ncheckpoint = check_df_dtype_expectations(df = sval, dtype_dct = sval_col_dtypes)\n\nif sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n    pass\nelse:\n    print(checkpoint.loc[~checkpoint.Pass, ]) #TODO add to template\n    print()\n\n120/120 Columns pass."
  },
  {
    "objectID": "freshstart_2020.html#weather",
    "href": "freshstart_2020.html#weather",
    "title": "Process Data from 2020 into a consistent format.",
    "section": "Weather",
    "text": "Weather\n\nDatetime\n\n# 183 records from MNH1 are missing a value for time. \n# These are not useful since we don't know if the values are daily averages or taken at a specific timepoint.\nwthr = wthr.loc[wthr.Time.notna(), :]\n\n# unfortunately these missing values also force ints (e.g. days) to be floats\nwthr.Year = wthr.Year.astype(float).astype(int).astype('string')\nwthr.Month = wthr.Month.astype(float).astype(int).astype('string')\nwthr.Day = wthr.Day.astype(float).astype(int).astype('string')\n\n\n# we use the fields in the df to make a consistent format\nwthr = cols_astype_string(\n    df = wthr, \n    col_list = ['Year', 'Month', 'Day', 'Time'])\n\nwthr = sanitize_col(\n    df = wthr,\n    col = 'Time', \n    simple_renames= {}, \n    split_renames= {})\n\nwthr['Datetime_Temp'] = wthr['Year']+'-'+wthr['Month']+'-'+wthr['Day']+' '+wthr['Time']\n\n# convert types\nerr_list = find_unconvertable_datetimes(df_col=wthr['Datetime_Temp'], pattern='%Y-%m-%d %H:%M', index=False)\nif err_list != []:\n    print(err_list)\nelse:\n    wthr.Datetime_Temp = pd.to_datetime(pd.Series(wthr.Datetime_Temp), errors='coerce')\n    wthr.Datetime = wthr.Datetime_Temp\n    wthr = wthr.drop(columns= 'Datetime_Temp')\n\n\n\nSimple Columns\n\n# to string\nwthr = cols_astype_string(\n    df = wthr, \n    col_list = [key for key in wthr_col_dtypes.keys() if wthr_col_dtypes[key] == 'string'])\n\nwthr.Year = year_string\nwthr.Year = wthr.Year.astype('string')\n\n\n\nCheck Success\n\ncheckpoint = check_df_dtype_expectations(df = wthr, dtype_dct = wthr_col_dtypes)\n\nif sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n    pass\nelse:\n    print(checkpoint.loc[~checkpoint.Pass, ])\n\n24/24 Columns pass."
  },
  {
    "objectID": "freshstart_2020.html#management",
    "href": "freshstart_2020.html#management",
    "title": "Process Data from 2020 into a consistent format.",
    "section": "Management",
    "text": "Management\n\nDate_Datetime\n\nmgmt = relocate_to_Imputation_Notes(df = mgmt, col = 'Date_Datetime', val_list= ['fall 2019'])\n\n\n# convert types\nerr_list = find_unconvertable_datetimes(df_col=mgmt.Date_Datetime, pattern='%m/%d/%y', index=False)\nif err_list != []:\n    print(err_list)\nelse:\n    mgmt.Date_Datetime = pd.to_datetime(pd.Series(mgmt.Date_Datetime), format = '%m/%d/%y', errors='coerce')\n\n\n\nAmount_Per_Acre\n\nmgmt = relocate_to_Imputation_Notes(df = mgmt, col = 'Amount_Per_Acre', val_list= ['variable rate according to soil sample'])\n\n\n# mgmt.loc[find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = True), ]\n\nmgmt.loc[mgmt.Amount_Per_Acre == '2 qts', :]\n\n\n\n\n\n  \n    \n      \n      Year\n      Experiment_Code\n      Range\n      Pass\n      Plot\n      Application\n      Product\n      Date_Datetime\n      Amount_Per_Acre\n      Unit\n      mgmt\n      Imputation_Notes\n    \n  \n  \n    \n      77802\n      2020\n      SCH1\n      1.0\n      1\n      1\n      Broadcast\n      BicepMagnum II\n      2020-05-12\n      2 qts\n      NaN\n      True\n      NaN\n    \n    \n      77803\n      2020\n      SCH1\n      1.0\n      1\n      1\n      Broadcast\n      Glyphosate\n      2020-05-12\n      2 qts\n      NaN\n      True\n      NaN\n    \n    \n      77804\n      2020\n      SCH1\n      1.0\n      1\n      1\n      Broadcast\n      Atrazine\n      2020-06-10\n      2 qts\n      NaN\n      True\n      NaN\n    \n    \n      77805\n      2020\n      SCH1\n      1.0\n      2\n      2\n      Broadcast\n      BicepMagnum II\n      2020-05-12\n      2 qts\n      NaN\n      True\n      NaN\n    \n    \n      77806\n      2020\n      SCH1\n      1.0\n      2\n      2\n      Broadcast\n      Glyphosate\n      2020-05-12\n      2 qts\n      NaN\n      True\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      79711\n      2020\n      SCH1\n      8.0\n      2\n      637\n      Broadcast\n      Glyphosate\n      2020-05-12\n      2 qts\n      NaN\n      True\n      NaN\n    \n    \n      79712\n      2020\n      SCH1\n      8.0\n      2\n      637\n      Broadcast\n      Atrazine\n      2020-06-10\n      2 qts\n      NaN\n      True\n      NaN\n    \n    \n      79713\n      2020\n      SCH1\n      8.0\n      1\n      638\n      Broadcast\n      BicepMagnum II\n      2020-05-12\n      2 qts\n      NaN\n      True\n      NaN\n    \n    \n      79714\n      2020\n      SCH1\n      8.0\n      1\n      638\n      Broadcast\n      Glyphosate\n      2020-05-12\n      2 qts\n      NaN\n      True\n      NaN\n    \n    \n      79715\n      2020\n      SCH1\n      8.0\n      1\n      638\n      Broadcast\n      Atrazine\n      2020-06-10\n      2 qts\n      NaN\n      True\n      NaN\n    \n  \n\n1914 rows × 12 columns\n\n\n\n\nmask = (mgmt.Amount_Per_Acre == '2 qts')\nmgmt.loc[mask, 'Unit'] = 'qts'\nmgmt.loc[mask, 'Amount_Per_Acre'] = '2'\n\nmask = (mgmt.Amount_Per_Acre == '1.5 qts')\nmgmt.loc[mask, 'Unit'] = 'qts'\nmgmt.loc[mask, 'Amount_Per_Acre'] = '1.5'\n\n\n# convert types\nerr_list = find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = False)\nif err_list != []:\n    print(err_list)\nelse:\n    mgmt.Amount_Per_Acre = pd.to_numeric(mgmt.Amount_Per_Acre, errors='coerce')\n\n\n\nSimple Columns\n\n# to bool\nmgmt['mgmt'] = mgmt['mgmt'].astype('bool')\n\n# to string\nfor e in [ee for ee in ['Application', 'Product', 'Ingredient', 'Unit', 'Imputation_Notes'] if ee in mgmt.columns]:\n    mgmt[e] = mgmt[e].astype('string')\n    \n\nmgmt.Year = year_string\nmgmt.Year = mgmt.Year.astype('string')\n\n\n\nCheck Success\n\ncheck_df_dtype_expectations(df = mgmt, dtype_dct = mgmt_col_dtypes)\n\n12/12 Columns pass.\n\n\n\n\n\n\n  \n    \n      \n      Column\n      dtype\n      Expected_dtype\n      Pass\n    \n  \n  \n    \n      0\n      Year\n      string\n      string\n      True\n    \n    \n      1\n      Experiment_Code\n      string\n      string\n      True\n    \n    \n      2\n      Range\n      string\n      string\n      True\n    \n    \n      3\n      Pass\n      string\n      string\n      True\n    \n    \n      4\n      Plot\n      string\n      string\n      True\n    \n    \n      5\n      Application\n      string\n      string\n      True\n    \n    \n      6\n      Product\n      string\n      string\n      True\n    \n    \n      7\n      Date_Datetime\n      datetime64[ns]\n      datetime64[ns]\n      True\n    \n    \n      8\n      Amount_Per_Acre\n      float64\n      float64\n      True\n    \n    \n      9\n      Unit\n      string\n      string\n      True\n    \n    \n      10\n      mgmt\n      bool\n      bool\n      True\n    \n    \n      11\n      Imputation_Notes\n      string\n      string\n      True"
  },
  {
    "objectID": "freshstart_2021.html",
    "href": "freshstart_2021.html",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "",
    "text": "# Imports ----\nimport re\nimport numpy as np # for np.nan\nimport pandas as pd\npd.set_option('display.max_columns', None)\n# import os   # for write_log, delete_logs\nimport glob # for delete_all_logs\nfrom datetime import date, timedelta\n\nimport json # for saving a dict to txt with json.dumps\n\nimport pickle\n# import matplotlib as mpl\n# import matplotlib.pyplot as plt"
  },
  {
    "objectID": "freshstart_2021.html#sanitization-functions",
    "href": "freshstart_2021.html#sanitization-functions",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "Sanitization functions",
    "text": "Sanitization functions\nThe pattern to use is: 1. Alter the dataframe 1. Test the dataframe against expectations\nThe main tasks that need to be completed are: 1. Identify values that can’t be converted to the expected data type. The “find_unconvertable_” family of functions should be used. 1. find_unconvertable_datetimes\n\nFor simple renaming (e.g. misspellings) or splitting non-tidy data into two rows (“entry1-entry2” -> “entry1”, “entry2”) use sanitize_col\nMove values that are ambigous but pertain to data imputation to “Imputation_Notes” using relocate_to_Imputation_Notes\nIf new columns need to be added (e.g. mgmt.Ingredient for parsed components of Product (e.g. elements) ) this should be accomplished with safe_create_col.\nAny one off changes should be accomplised manually.\nConfirm columns match the expected types with check_df_dtype_expectations, and report mismatches.\n\nThese steps should be completed for each dataframe in turn to minimize the cognitive load of the reader.\n\nimport pandas as pd\n# Make versions of `find_unconvertable_datetimes` for other datatype\n# make a function to find the unexpected entries so it's easy to write the santization code\n# in a column, report all the values causing errors OR an index of these values\ndef find_unconvertable_datetimes(df_col, pattern = '%m/%d/%y', index = False):\n    datetime_errors = pd.to_datetime(pd.Series(df_col), format = pattern, errors='coerce').isna()\n    if index == True:\n        return(datetime_errors)\n    else:\n        # This is a interesting trick. Python's nan is not equal to itself.\n        # missing values can't become datetimes so nan is returned if there's a missing value\n        # This list comprehension removes nan (which is otherwise stubborn to remove) because nan != nan\n        return([e for e in list(set(df_col[datetime_errors])) if e == e])\n\n\nimport pandas as pd\ndef find_unconvertable_numerics(df_col, # Dataframe column (e.g. df['example']) to be used.\n                                index = False # Return an index of unconveratbles or a list of unique values\n                               ):\n    \"Find the values or positions of values that cannot be converted to a numeric.\"\n    numeric_errors = pd.to_numeric(pd.Series(df_col), errors='coerce').isna()\n    if index == True:\n        return(numeric_errors) # a\n    else:\n        # This is a interesting trick. Python's nan is not equal to itself.\n        # missing values can't become datetimes so nan is returned if there's a missing value\n        # This list comprehension removes nan (which is otherwise stubborn to remove) because nan != nan\n        return([e for e in list(set(df_col[numeric_errors])) if e == e]) # b\n\n\n# generalized version of `sanitize_Experiment_Codes`\ndef sanitize_col(df, col, simple_renames= {}, split_renames= {}):\n    # simple renames\n    for e in simple_renames.keys():\n        mask = (df[col] == e)\n        df.loc[mask, col] = simple_renames[e]\n\n    # splits\n    # pull out the relevant multiname rows, copy, rename, append\n    for e in split_renames.keys():\n        mask = (df[col] == e)\n        temp = df.loc[mask, :] \n\n        df = df.loc[~mask, :]\n        for e2 in split_renames[e]:\n            temp2 = temp.copy()\n            temp2[col] = e2\n            df = df.merge(temp2, how = 'outer')\n\n    return(df)\n\n\nimport numpy as np\n# If the Imputation_Notes column doesnt exist, create it. So long as it wouldn't overwrite any imputation notes move each specified value and replace it with nan.\ndef relocate_to_Imputation_Notes(df, col, val_list):\n    if not 'Imputation_Notes' in df.columns:\n        df.loc[:, 'Imputation_Notes'] = np.nan\n\n    for relocate in val_list:\n        mask = (df.loc[:, col] == relocate)\n        mask_Impute_Full = ((df.loc[:, 'Imputation_Notes'] == '') | (df.loc[:, 'Imputation_Notes'].isna()))\n        # check if this contains anyting\n        overwrite_danger = df.loc[(mask & ~mask_Impute_Full), 'Imputation_Notes']\n        if overwrite_danger.shape[0] > 0:\n            print(\"Warning! The following values will be overwritten. Skipping relocation.\")\n            print(overwrite_danger)\n        else:\n            df.loc[(mask), 'Imputation_Notes'] = df.loc[(mask), col]\n            df.loc[(mask), col] = np.nan\n    return(df)\n\n\n# helper function so we can ask for a new column don't have to worry about overwritting a if it already exists \ndef safe_create_col(df, col_name):\n    if not col_name in df.columns:\n        df.loc[:, col_name] = np.nan\n    return(df)\n\n\n# little helper function to make this easier. Make all the columns in a list into dtype string.\n# require the column to exist to make this safe.\n# to make things even easier, use a list comprehension to pull out the keys in the *_col_dtype dict \n# that have value of 'string'!\ndef cols_astype_string(df, col_list):\n    for e in [ee for ee in col_list if ee in df.columns]:\n        df[e] = df[e].astype('string')\n    return(df)\n\n\nimport pandas as pd\n# Ignore columns that don't exist in the dataframe even if they're specified in the dict\n# For testing that sanitization was successful\n# a function to check the type of each column \n# shouldn't _change_ anything, just report what I need to fix\ndef check_df_dtype_expectations(df, dtype_dct):\n    found = pd.DataFrame(zip(\n        df.columns,\n        [str(df[e].dtype) for e in df.columns]\n    ), columns=['Column', 'dtype'])\n\n\n    expected = pd.DataFrame(zip(dtype_dct.keys(), dtype_dct.values()),\n                 columns=['Column', 'Expected_dtype']\n                )\n    mask = [True if e in df.columns else False for e in expected.Column]\n    expected = expected.loc[mask, ]\n    \n    out = found.merge(expected, how = 'outer')\n    out = out.assign(Pass = out.dtype == out.Expected_dtype)\n\n    print(str(sum(out.Pass))+'/'+str(len(out.Pass))+' Columns pass.')\n    return(out)\n\n# each df should get individual treatment with these steps. Probably most readable"
  },
  {
    "objectID": "freshstart_2021.html#sanitization-column-data-type-expectations",
    "href": "freshstart_2021.html#sanitization-column-data-type-expectations",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "Sanitization: Column data type expectations",
    "text": "Sanitization: Column data type expectations\nNote: to handle missing values some columns that would otherwise be ints are floats\n\ndef mk_dtype_dict(name # table sval, wthr, or mgmt\n                ):\n    'Easily share dictionaries of expected datatypes of the columns across scripts.'\n    sval_col_dtypes = {\n        'Year': 'string', \n        'Experiment_Code': 'string', \n        'State': 'string', \n        'City': 'string', \n        'Plot_Length_Unit_Feet': 'float64', \n        'Plot_Area_Unit_Feet2': 'float64', \n        'Alley_Length_Unit_Inches': 'float64', \n        'Row_Spacing_Unit_Inches': 'float64', \n        'Rows_Per_Plot': 'float64', \n        'Seeds_Per_Plot': 'float64', \n        'Experiment': 'string', \n        'Source': 'string', \n        'Pedigree': 'string', \n        'Family': 'string', \n        'Tester': 'string', \n        'Replicate': 'string', \n        'Block': 'string', \n        'Plot': 'string', \n        'Plot_ID': 'string', \n        'Range': 'string', \n        'Pass': 'string', \n        'Planted_Unit_Datetime': 'datetime64[ns]', \n        'Harvested_Unit_Datetime': 'datetime64[ns]', \n        'Anthesis_Unit_Datetime': 'datetime64[ns]', \n        'Silking_Unit_Datetime': 'datetime64[ns]', \n        'Anthesis_Unit_Days': 'float64', \n        'Silking_Unit_Days': 'float64', \n        'Plant_Height_Unit_cm': 'float64', \n        'Ear_Height_Unit_cm': 'float64', \n        'Stand_Count_Unit_Number': 'float64', \n        'Root_Lodging_Unit_Number': 'float64', \n        'Stalk_Lodging_Unit_Number': 'float64', \n        'Grain_Moisture_Unit_Percent': 'float64', \n        'Test_Weight_Unit_lbs': 'float64', \n        'Plot_Weight_Unit_lbs': 'float64', \n        'Grain_Yield_Unit_bu_Per_A': 'float64', \n        'Discarded': 'bool', \n        'Phenotype_Comments': 'string', \n        'Filler': 'string', \n        'Snap_Unit_Number': 'float64', \n    'phno': 'bool', \n        'Grower': 'string', \n        'Recieved_Date_Unit_Datetime': 'datetime64[ns]', \n        'Processed_Date_Unit_Datetime': 'datetime64[ns]', \n        'Depth_Unit_UNK': 'float64', \n        'Soil_1_to_1_Unit_pH': 'float64', \n        'WDRF_Buffer_Unit_pH': 'float64', \n        'Soluable_Salts_Unit_mmho_Per_cm': 'float64', \n        'Texture_Number': 'float64', \n        'Organic_Matter_Unit_Percent': 'float64', \n        'Nitrates_Unit_ppm': 'float64', \n        'N_per_Acre_Unit_lbs': 'float64', \n        'K_Unit_ppm': 'float64', \n        'Sulfate_Unit_ppm': 'float64', \n        'Ca_Unit_ppm': 'float64', \n        'Mg_Unit_ppm': 'float64', \n        'Na_Unit_ppm': 'float64', \n        'Cation_Exchange_Capacity': 'float64', \n        'H_Sat_Unit_Percent': 'float64', \n        'K_Sat_Unit_Percent': 'float64', \n        'Ca_Sat_Unit_Percent': 'float64', \n        'Mg_Sat_Unit_Percent': 'float64', \n        'Na_Sat_Unit_Percent': 'float64', \n        'Mehlich_PIII_P_Unit_ppm': 'float64', \n        'Sand_Unit_Percent': 'float64', \n        'Silt_Unit_Percent': 'float64', \n        'Clay_Unit_Percent': 'float64', \n        'Texture': 'string', \n        'Soil_Comments': 'string', \n    'soil': 'bool', \n        'Treatment': 'string', \n        'Farm': 'string', \n        'Field': 'string', \n        'Trial_ID': 'string', \n        'Soil_Taxonomic_ID': 'string', \n        'Weather_Station_Serial_Number': 'string', \n        'Weather_Station_Latitude_Unit_Decimal': 'float64', \n        'Weather_Station_Longitude_Unit_Decimal': 'float64', \n        'Weather_Station_Placed_Unit_Datetime': 'datetime64[ns]', \n        'Weather_Station_Removed_Unit_Datetime': 'datetime64[ns]', \n        'Weather_Station_In_Field_Serial_Number': 'string', \n        'Weather_Station_In_Field_Latitude_Unit_Decimal': 'float64', \n        'Weather_Station_In_Field_Longitude_Unit_Decimal': 'float64', \n        'Previous_Crop': 'string', \n        'Pre_Plant_Tillage': 'string', \n        'Post_Plant_Tillage': 'string', \n        'Planter_Type': 'string', \n        'Kernels_Per_Plot': 'float64', \n        'System_Determining_Moisture': 'string', \n        'Pounds_Needed_Soil_Moisture': 'float64', \n        'Field_Latitude_BL': 'float64', \n        'Field_Longitude_BL': 'float64', \n        'Field_Latitude_BR': 'float64', \n        'Field_Longitude_BR': 'float64', \n        'Field_Latitude_TR': 'float64', \n        'Field_Longitude_TR': 'float64', \n        'Field_Latitude_TL': 'float64', \n        'Field_Longitude_TL': 'float64', \n        'Cardinal_Heading': 'float64', \n        'Local_Check_Pedigree_1': 'string', \n        'Local_Check_Source_1': 'string', \n        'Local_Check_Pedigree_2': 'string', \n        'Local_Check_Source_2': 'string', \n        'Local_Check_Pedigree_3': 'string', \n        'Local_Check_Source_3': 'string', \n        'Local_Check_Pedigree_4': 'string', \n        'Local_Check_Source_4': 'string', \n        'Local_Check_Pedigree_5': 'string', \n        'Local_Check_Source_5': 'string', \n        'Comment_1': 'string', \n        'Comment_2': 'string', \n        'Comment_3': 'string', \n        'Comment_4': 'string', \n        'Comment_5': 'string', \n        'Comment_6': 'string', \n        'Comment_7': 'string', \n        'Comment_8': 'string', \n        'Comment_9': 'string', \n        'Comment_70': 'string', \n    'meta': 'bool',\n        'Imputation_Notes': 'string'\n    }\n\n    wthr_col_dtypes = {\n        'Experiment_Code': 'string', \n        'Weather_Station_ID': 'string', \n        'NWS_Network': 'string', \n        'NWS_Station': 'string', \n        'Datetime': 'datetime64[ns]', \n        'Month': 'string', \n        'Day': 'string', \n        'Year': 'string', \n        'Time': 'string', \n        'Temperature_Unit_C': 'float64', \n        'Dew_Point_Unit_C': 'float64', \n        'Relative_Humidity_Unit_Percent': 'float64', \n        'Solar_Radiation_Unit_W_per_m2': 'float64', \n        'Rainfall_Unit_mm': 'float64', \n        'Wind_Speed_Unit_m_per_s': 'float64', \n        'Wind_Direction_Unit_Degrees': 'float64', \n        'Wind_Gust_Unit_m_per_s': 'float64', \n        'Soil_Temperature_Unit_C': 'float64', \n        'Soil_Moisture_Unit_Percent_VWC': 'float64', \n        'Soil_EC_Unit_mS_per_cm': 'float64', \n        'UV_Light_Unit_uM_per_m2s': 'float64', \n        'PAR_Unit_uM_per_m2s': 'float64', \n    'wthr': 'bool',\n        'Imputation_Notes': 'string',\n        'CO2_Unit_ppm': 'float64'# 2020\n    }\n\n    mgmt_col_dtypes = {\n        'Year': 'string',   \n        'Experiment_Code': 'string', \n        'Range': 'string',\n        'Pass': 'string',\n        'Plot': 'string',\n        'Application': 'string', \n        'Product': 'string', \n        'Date_Datetime': 'datetime64[ns]', \n        'Amount_Per_Acre': 'float64', \n        'Unit': 'string', \n    'mgmt': 'bool',\n        'Imputation_Notes': 'string',\n        'Ingredient': 'string'\n    }\n   \n    if name == 'sval':\n        return sval_col_dtypes\n    elif name == 'wthr':\n        return wthr_col_dtypes\n    elif name == 'mgmt':\n        return mgmt_col_dtypes\n    else:\n        print('Requested name is not defined')\n\n\nsval_col_dtypes = mk_dtype_dict(name = 'sval')\nwthr_col_dtypes = mk_dtype_dict(name = 'wthr')\nmgmt_col_dtypes = mk_dtype_dict(name = 'mgmt')"
  },
  {
    "objectID": "freshstart_2021.html#static-values-within-season",
    "href": "freshstart_2021.html#static-values-within-season",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "Static values (within season)",
    "text": "Static values (within season)\n\nDatetime containing columns\n\n# convert the date cols into datetime. Lean on pd.to_datetime() to infer the format, assume that each site uses the same format.\n\nfor e in ['Planted_Unit_Datetime', \n    'Harvested_Unit_Datetime', \n    'Anthesis_Unit_Datetime', \n    'Silking_Unit_Datetime', \n    'Recieved_Date_Unit_Datetime', \n    'Processed_Date_Unit_Datetime', \n    'Weather_Station_Placed_Unit_Datetime', \n    'Weather_Station_Removed_Unit_Datetime'\n    ]:\n# find_unconvertable_datetimes(df_col=sval[e], pattern='%Y-%m-%d %H:%M', index=False)\n\n    sval['Datetime_Temp'] = pd.to_datetime(np.nan)\n\n    for code in list(sval.Experiment_Code.drop_duplicates()):\n    # code = list(sval.Experiment_Code.drop_duplicates())[0]\n        sval.loc[sval.Experiment_Code == code, 'Datetime_Temp'\n                 ] = pd.to_datetime(sval.loc[sval.Experiment_Code == code, e])\n\n    sval.loc[:, e] = sval.loc[:, 'Datetime_Temp'] \n\nsval = sval.drop(columns = 'Datetime_Temp')\n\n\n# -> floats\n\n# [find_unconvertable_numerics(df_col = sval[e], index = False) for e in [\n#     'Alley_Length_Unit_Inches',\n# 'Row_Spacing_Unit_Inches',\n# 'Pounds_Needed_Soil_Moisture'\n# ]]\n\nsval = sanitize_col(\n    df = sval, \n    col = 'Pounds_Needed_Soil_Moisture', \n    simple_renames= {'3 to 4':'3.5'}, \n    split_renames= {})\n\n\n# convert types\nfor e in ['Alley_Length_Unit_Inches', 'Row_Spacing_Unit_Inches', 'Pounds_Needed_Soil_Moisture',\n         'Anthesis_Unit_Days', 'Silking_Unit_Days', 'Kernels_Per_Plot']:\n    err_list = find_unconvertable_numerics(df_col = sval[e], index = False)\n    if err_list != []:\n        print(e)\n        print(err_list)\n    else:\n        sval[e] = sval[e].astype('float')\n\n\n# to bool\nsval = sanitize_col(\n    df = sval, \n    col = 'Discarded', \n    simple_renames= {\n        'Yes':'True',\n        'yes':'True'}, \n    split_renames= {})\n\n# set missing to false\nsval.loc[sval.Discarded.isna(), 'Discarded'] = 'False'\nsval.Discarded = sval.Discarded.map({'True': True, 'False': False})\n\n\n\nSimple Columns\n\n# to bool\nsval['phno'] = sval['phno'].astype('bool')\nsval['soil'] = sval['soil'].astype('bool')\nsval['meta'] = sval['meta'].astype('bool')\n\n# to string\nsval = cols_astype_string(\n    df = sval, \n    col_list = [key for key in sval_col_dtypes.keys() if sval_col_dtypes[key] == 'string'])\n\nsval.Year = year_string\nsval.Year = sval.Year.astype('string')\n\n\n\nCheck Success\n\ncheckpoint = check_df_dtype_expectations(df = sval, dtype_dct = sval_col_dtypes)\n\nif sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n    pass\nelse:\n    print(checkpoint.loc[~checkpoint.Pass, ]) \n    print()\n\n120/120 Columns pass."
  },
  {
    "objectID": "freshstart_2021.html#weather",
    "href": "freshstart_2021.html#weather",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "Weather",
    "text": "Weather\n\nDatetime\n\n# instead of writing regexes to figure out the mose likely format for each datetime, we assume each experiment will be consistent withing that experiment\n# and let pd figure it out.\n# wthr['Datetime_Temp'] = pd.to_datetime(np.nan)\n\n# for code in list(wthr.loc[:, 'Experiment_Code'].drop_duplicates()):\n#     wthr.loc[wthr.Experiment_Code == code, 'Datetime_Temp'] = pd.to_datetime(wthr.loc[wthr.Experiment_Code == code, 'Datetime'], errors='coerce')\n\n\n# ... or we use the fields in the df to make a consistent format\nwthr = cols_astype_string(\n    df = wthr, \n    col_list = ['Year', 'Month', 'Day', 'Time'])\n\nwthr = sanitize_col(\n    df = wthr,\n    col = 'Time', \n    simple_renames= {'24:00:00': '00:00:00'}, # this could be day + 24 h instead of a miscoded day + 0 h\n    split_renames= {})\n\nwthr['Datetime_Temp'] = wthr['Year']+'-'+wthr['Month']+'-'+wthr['Day']+' '+wthr['Time']\n\n# convert types\nerr_list = find_unconvertable_datetimes(df_col=wthr['Datetime_Temp'], pattern='%Y-%m-%d %H:%M', index=False)\nif err_list != []:\n    print(err_list)\nelse:\n    wthr.Datetime_Temp = pd.to_datetime(pd.Series(wthr.Datetime_Temp), errors='coerce')\n    wthr.Datetime = wthr.Datetime_Temp\n    wthr = wthr.drop(columns= 'Datetime_Temp')\n\n\n\nSimple Columns\n\n# to string\nwthr = cols_astype_string(\n    df = wthr, \n    col_list = [key for key in wthr_col_dtypes.keys() if wthr_col_dtypes[key] == 'string'])\n\nwthr.Year = year_string\nwthr.Year = wthr.Year.astype('string')\n\n\n\nCheck Success\n\ncheckpoint = check_df_dtype_expectations(df = wthr, dtype_dct = wthr_col_dtypes)\n\nif sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n    pass\nelse:\n    print(checkpoint.loc[~checkpoint.Pass, ])\n\n23/23 Columns pass."
  },
  {
    "objectID": "freshstart_2021.html#management",
    "href": "freshstart_2021.html#management",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "Management",
    "text": "Management\n\nDate_Datetime\n\nmgmt = relocate_to_Imputation_Notes(df = mgmt, col = 'Date_Datetime', val_list= ['Before Planting'])\n\n\nmgmt = sanitize_col(\n    df = mgmt, \n    col = 'Date_Datetime', \n    simple_renames= {}, \n    split_renames= {'6/24/21 for all but plots in pass 2; 7/5/21 for pass 2' : [\n                        '6/24/21 for all but plots in pass 2', '7/5/21 for pass 2']})\n\n\n# make corrections too one-off to fix with a funciton. \nmask = ((mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2') & (mgmt.Pass != 2.))\nmgmt.loc[mask, 'Date_Datetime'] = '6/24/21'\n# since we split without specifiying pass we need to remove any rows that still have the search string.\n# and overwrite the df\nmask = (mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2')\nmgmt = mgmt.loc[~mask, :].copy()\n\nmask = ((mgmt.Date_Datetime == '7/5/21 for pass 2') & (mgmt.Pass == 2.))\nmgmt.loc[mask, 'Date_Datetime'] = '7/5/21'\nmask = (mgmt.Date_Datetime == '7/5/21 for pass 2')\nmgmt = mgmt.loc[~mask, :].copy()\n\n\n# convert types\nerr_list = find_unconvertable_datetimes(df_col=mgmt.Date_Datetime, pattern='%m/%d/%y', index=False)\nif err_list != []:\n    print(err_list)\nelse:\n    mgmt.Date_Datetime = pd.to_datetime(pd.Series(mgmt.Date_Datetime), format = '%m/%d/%y', errors='coerce')\n\n\n\nAmount_Per_Acre\n\n# mgmt.loc[find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = True), ]\n\n\nmgmt = sanitize_col(\n    df = mgmt, \n    col = 'Amount_Per_Acre', \n    simple_renames= {'170 lb (actual N)': '170 (N)'}, \n    split_renames= {'51.75, 40.7, 111.7 (N,P,K)': ['51.75 (N)', '40.7 (P)', '111.7 (K)'],\n                    '31-150-138': ['31 (N)', '150 (P)', '138 (K)'],\n                    '16 (N), 41 (P)': ['16 (N)', '41 (P)']})\n\n\nmgmt = safe_create_col(mgmt, \"Ingredient\")\nmask = mgmt.Ingredient.isna()\nmgmt.loc[mask, 'Ingredient'] = mgmt.loc[mask, 'Product']\n\n# assume each string is formated as 'val (key)'. `sanitize_col` should be used to enforce this.\nfor e in ['150 (P)', '36.6 (N)', '138 (K)', '111.7 (K)', '41 (P)', '16 (N)', '170 (N)', '35.7 (N)', '51.75 (N)', '31 (N)', '40.7 (P)']:\n    val = re.findall('^\\d+[.]*\\d*', e)[0]\n    key = re.findall('\\(.+\\)',      e)[0].replace('(', '').replace(')', '')\n    \n    mask = (mgmt['Amount_Per_Acre'] == e)\n    mgmt.loc[mask, 'Ingredient'] = key\n    mgmt.loc[mask, 'Amount_Per_Acre'] = val\n\n\n# convert types\nerr_list = find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = False)\nif err_list != []:\n    print(err_list)\nelse:\n    mgmt.Amount_Per_Acre = pd.to_numeric(mgmt.Amount_Per_Acre, errors='coerce')\n\n\n\nIngredient\nThis is to be the cleaned up version of the “Product” column\n\n# list(mgmt.loc[:, 'Ingredient'].drop_duplicates())\n\n\n\nSimple Columns\n\n# to bool\nmgmt['mgmt'] = mgmt['mgmt'].astype('bool')\n\n# to string\nfor e in [ee for ee in ['Application', 'Product', 'Ingredient', 'Unit', 'Imputation_Notes'] if ee in mgmt.columns]:\n    mgmt[e] = mgmt[e].astype('string')\n    \n\nmgmt.Year = year_string\nmgmt.Year = mgmt.Year.astype('string')\n\n\n\nCheck Success\n\ncheck_df_dtype_expectations(df = mgmt, dtype_dct = mgmt_col_dtypes)\n\n13/13 Columns pass.\n\n\n\n\n\n\n  \n    \n      \n      Column\n      dtype\n      Expected_dtype\n      Pass\n    \n  \n  \n    \n      0\n      Year\n      string\n      string\n      True\n    \n    \n      1\n      Experiment_Code\n      string\n      string\n      True\n    \n    \n      2\n      Range\n      string\n      string\n      True\n    \n    \n      3\n      Pass\n      string\n      string\n      True\n    \n    \n      4\n      Plot\n      string\n      string\n      True\n    \n    \n      5\n      Application\n      string\n      string\n      True\n    \n    \n      6\n      Product\n      string\n      string\n      True\n    \n    \n      7\n      Date_Datetime\n      datetime64[ns]\n      datetime64[ns]\n      True\n    \n    \n      8\n      Amount_Per_Acre\n      float64\n      float64\n      True\n    \n    \n      9\n      Unit\n      string\n      string\n      True\n    \n    \n      10\n      mgmt\n      bool\n      bool\n      True\n    \n    \n      11\n      Imputation_Notes\n      string\n      string\n      True\n    \n    \n      12\n      Ingredient\n      string\n      string\n      True"
  },
  {
    "objectID": "freshstart_0000.html",
    "href": "freshstart_0000.html",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "",
    "text": "# Imports ----\nimport re\nimport numpy as np \nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport pickle"
  },
  {
    "objectID": "freshstart_0000.html#sanitization-functions",
    "href": "freshstart_0000.html#sanitization-functions",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "Sanitization functions",
    "text": "Sanitization functions\nThe pattern to use is: 1. Alter the dataframe 1. Test the dataframe against expectations\nThe main tasks that need to be completed are: 1. Identify values that can’t be converted to the expected data type. The “find_unconvertable_” family of functions should be used. 1. find_unconvertable_datetimes\n\nFor simple renaming (e.g. misspellings) or splitting non-tidy data into two rows (“entry1-entry2” -> “entry1”, “entry2”) use sanitize_col\nMove values that are ambigous but pertain to data imputation to “Imputation_Notes” using relocate_to_Imputation_Notes\nIf new columns need to be added (e.g. mgmt.Ingredient for parsed components of Product (e.g. elements) ) this should be accomplished with safe_create_col.\nAny one off changes should be accomplised manually.\nConfirm columns match the expected types with check_df_dtype_expectations, and report mismatches.\n\nThese steps should be completed for each dataframe in turn to minimize the cognitive load of the reader."
  },
  {
    "objectID": "freshstart_0000.html#sanitization-column-data-type-expectations",
    "href": "freshstart_0000.html#sanitization-column-data-type-expectations",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "Sanitization: Column data type expectations",
    "text": "Sanitization: Column data type expectations\nNote: to handle missing values some columns that would otherwise be ints are floats\n\nsval_col_dtypes = mk_dtype_dict(name = 'sval')\nwthr_col_dtypes = mk_dtype_dict(name = 'wthr')\nmgmt_col_dtypes = mk_dtype_dict(name = 'mgmt')"
  },
  {
    "objectID": "freshstart_0000.html#static-values-within-season",
    "href": "freshstart_0000.html#static-values-within-season",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "Static values (within season)",
    "text": "Static values (within season)\n\nDatetime containing columns\n\n# convert the date cols into datetime. Lean on pd.to_datetime() to infer the format, assume that each site uses the same format.\n\nfor e in ['Planted_Unit_Datetime', \n    'Harvested_Unit_Datetime', \n    'Anthesis_Unit_Datetime', \n    'Silking_Unit_Datetime', \n    'Recieved_Date_Unit_Datetime', \n    'Processed_Date_Unit_Datetime', \n    'Weather_Station_Placed_Unit_Datetime', \n    'Weather_Station_Removed_Unit_Datetime'\n    ]:\n# find_unconvertable_datetimes(df_col=sval[e], pattern='%Y-%m-%d %H:%M', index=False)\n\n    sval['Datetime_Temp'] = pd.to_datetime(np.nan)\n\n    for code in list(sval.Experiment_Code.drop_duplicates()):\n    # code = list(sval.Experiment_Code.drop_duplicates())[0]\n        sval.loc[sval.Experiment_Code == code, 'Datetime_Temp'\n                 ] = pd.to_datetime(sval.loc[sval.Experiment_Code == code, e])\n\n    sval.loc[:, e] = sval.loc[:, 'Datetime_Temp'] \n\nsval = sval.drop(columns = 'Datetime_Temp')\n\n\n# -> floats\n\n# [find_unconvertable_numerics(df_col = sval[e], index = False) for e in [\n#     'Alley_Length_Unit_Inches',\n# 'Row_Spacing_Unit_Inches',\n# 'Pounds_Needed_Soil_Moisture'\n# ]]\n\nsval = sanitize_col(\n    df = sval, \n    col = 'Pounds_Needed_Soil_Moisture', \n    simple_renames= {'3 to 4':'3.5'}, \n    split_renames= {})\n\n\n# convert types\nfor e in ['Alley_Length_Unit_Inches', 'Row_Spacing_Unit_Inches', 'Pounds_Needed_Soil_Moisture',\n         'Anthesis_Unit_Days', 'Silking_Unit_Days', 'Kernels_Per_Plot']:\n    err_list = find_unconvertable_numerics(df_col = sval[e], index = False)\n    if err_list != []:\n        print(e)\n        print(err_list)\n    else:\n        sval[e] = sval[e].astype('float')\n\n\n# to bool\nsval = sanitize_col(\n    df = sval, \n    col = 'Discarded', \n    simple_renames= {\n        'Yes':'True',\n        'yes':'True'}, \n    split_renames= {})\n\n# set missing to false\nsval.loc[sval.Discarded.isna(), 'Discarded'] = 'False'\nsval.Discarded = sval.Discarded.map({'True': True, 'False': False})\n\n\n\nSimple Columns\n\n# to bool\nsval['phno'] = sval['phno'].astype('bool')\nsval['soil'] = sval['soil'].astype('bool')\nsval['meta'] = sval['meta'].astype('bool')\n\n# to string\nsval = cols_astype_string(\n    df = sval, \n    col_list = [key for key in sval_col_dtypes.keys() if sval_col_dtypes[key] == 'string'])\n\nsval.Year = year_string\nsval.Year = sval.Year.astype('string')\n\n\n\nCheck Success\n\ncheckpoint = check_df_dtype_expectations(df = sval, dtype_dct = sval_col_dtypes)\n\nif sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n    pass\nelse:\n    print(checkpoint.loc[~checkpoint.Pass, ]) \n    print()\n\n120/120 Columns pass."
  },
  {
    "objectID": "freshstart_0000.html#weather",
    "href": "freshstart_0000.html#weather",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "Weather",
    "text": "Weather\n\nDatetime\n\n# instead of writing regexes to figure out the mose likely format for each datetime, we assume each experiment will be consistent withing that experiment\n# and let pd figure it out.\n# wthr['Datetime_Temp'] = pd.to_datetime(np.nan)\n\n# for code in list(wthr.loc[:, 'Experiment_Code'].drop_duplicates()):\n#     wthr.loc[wthr.Experiment_Code == code, 'Datetime_Temp'] = pd.to_datetime(wthr.loc[wthr.Experiment_Code == code, 'Datetime'], errors='coerce')\n\n\n# ... or we use the fields in the df to make a consistent format\nwthr = cols_astype_string(\n    df = wthr, \n    col_list = ['Year', 'Month', 'Day', 'Time'])\n\nwthr = sanitize_col(\n    df = wthr,\n    col = 'Time', \n    simple_renames= {'24:00:00': '00:00:00'}, # this could be day + 24 h instead of a miscoded day + 0 h\n    split_renames= {})\n\nwthr['Datetime_Temp'] = wthr['Year']+'-'+wthr['Month']+'-'+wthr['Day']+' '+wthr['Time']\n\n# convert types\nerr_list = find_unconvertable_datetimes(df_col=wthr['Datetime_Temp'], pattern='%Y-%m-%d %H:%M', index=False)\nif err_list != []:\n    print(err_list)\nelse:\n    wthr.Datetime_Temp = pd.to_datetime(pd.Series(wthr.Datetime_Temp), errors='coerce')\n    wthr.Datetime = wthr.Datetime_Temp\n    wthr = wthr.drop(columns= 'Datetime_Temp')\n\n\n\nSimple Columns\n\n# to string\nwthr = cols_astype_string(\n    df = wthr, \n    col_list = [key for key in wthr_col_dtypes.keys() if wthr_col_dtypes[key] == 'string'])\n\nwthr.Year = year_string\nwthr.Year = wthr.Year.astype('string')\n\n\n\nCheck Success\n\ncheckpoint = check_df_dtype_expectations(df = wthr, dtype_dct = wthr_col_dtypes)\n\nif sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n    pass\nelse:\n    print(checkpoint.loc[~checkpoint.Pass, ])\n\n23/23 Columns pass."
  },
  {
    "objectID": "freshstart_0000.html#management",
    "href": "freshstart_0000.html#management",
    "title": "Process Data from 2021 into a consistent format.",
    "section": "Management",
    "text": "Management\n\nDate_Datetime\n\nmgmt = relocate_to_Imputation_Notes(df = mgmt, col = 'Date_Datetime', val_list= ['Before Planting'])\n\n\nmgmt = sanitize_col(\n    df = mgmt, \n    col = 'Date_Datetime', \n    simple_renames= {}, \n    split_renames= {'6/24/21 for all but plots in pass 2; 7/5/21 for pass 2' : [\n                        '6/24/21 for all but plots in pass 2', '7/5/21 for pass 2']})\n\n\n# make corrections too one-off to fix with a funciton. \nmask = ((mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2') & (mgmt.Pass != 2.))\nmgmt.loc[mask, 'Date_Datetime'] = '6/24/21'\n# since we split without specifiying pass we need to remove any rows that still have the search string.\n# and overwrite the df\nmask = (mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2')\nmgmt = mgmt.loc[~mask, :].copy()\n\nmask = ((mgmt.Date_Datetime == '7/5/21 for pass 2') & (mgmt.Pass == 2.))\nmgmt.loc[mask, 'Date_Datetime'] = '7/5/21'\nmask = (mgmt.Date_Datetime == '7/5/21 for pass 2')\nmgmt = mgmt.loc[~mask, :].copy()\n\n\n# convert types\nerr_list = find_unconvertable_datetimes(df_col=mgmt.Date_Datetime, pattern='%m/%d/%y', index=False)\nif err_list != []:\n    print(err_list)\nelse:\n    mgmt.Date_Datetime = pd.to_datetime(pd.Series(mgmt.Date_Datetime), format = '%m/%d/%y', errors='coerce')\n\n\n\nAmount_Per_Acre\n\n# mgmt.loc[find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = True), ]\n\n\nmgmt = sanitize_col(\n    df = mgmt, \n    col = 'Amount_Per_Acre', \n    simple_renames= {'170 lb (actual N)': '170 (N)'}, \n    split_renames= {'51.75, 40.7, 111.7 (N,P,K)': ['51.75 (N)', '40.7 (P)', '111.7 (K)'],\n                    '31-150-138': ['31 (N)', '150 (P)', '138 (K)'],\n                    '16 (N), 41 (P)': ['16 (N)', '41 (P)']})\n\n\nmgmt = safe_create_col(mgmt, \"Ingredient\")\nmask = mgmt.Ingredient.isna()\nmgmt.loc[mask, 'Ingredient'] = mgmt.loc[mask, 'Product']\n\n# assume each string is formated as 'val (key)'. `sanitize_col` should be used to enforce this.\nfor e in ['150 (P)', '36.6 (N)', '138 (K)', '111.7 (K)', '41 (P)', '16 (N)', '170 (N)', '35.7 (N)', '51.75 (N)', '31 (N)', '40.7 (P)']:\n    val = re.findall('^\\d+[.]*\\d*', e)[0]\n    key = re.findall('\\(.+\\)',      e)[0].replace('(', '').replace(')', '')\n    \n    mask = (mgmt['Amount_Per_Acre'] == e)\n    mgmt.loc[mask, 'Ingredient'] = key\n    mgmt.loc[mask, 'Amount_Per_Acre'] = val\n\n\n# convert types\nerr_list = find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = False)\nif err_list != []:\n    print(err_list)\nelse:\n    mgmt.Amount_Per_Acre = pd.to_numeric(mgmt.Amount_Per_Acre, errors='coerce')\n\n\n\nIngredient\nThis is to be the cleaned up version of the “Product” column\n\n# list(mgmt.loc[:, 'Ingredient'].drop_duplicates())\n\n\n\nSimple Columns\n\n# to bool\nmgmt['mgmt'] = mgmt['mgmt'].astype('bool')\n\n# to string\nfor e in [ee for ee in ['Application', 'Product', 'Ingredient', 'Unit', 'Imputation_Notes'] if ee in mgmt.columns]:\n    mgmt[e] = mgmt[e].astype('string')\n    \n\nmgmt.Year = year_string\nmgmt.Year = mgmt.Year.astype('string')\n\n\n\nCheck Success\n\ncheck_df_dtype_expectations(df = mgmt, dtype_dct = mgmt_col_dtypes)\n\n13/13 Columns pass.\n\n\n\n\n\n\n  \n    \n      \n      Column\n      dtype\n      Expected_dtype\n      Pass\n    \n  \n  \n    \n      0\n      Year\n      string\n      string\n      True\n    \n    \n      1\n      Experiment_Code\n      string\n      string\n      True\n    \n    \n      2\n      Range\n      string\n      string\n      True\n    \n    \n      3\n      Pass\n      string\n      string\n      True\n    \n    \n      4\n      Plot\n      string\n      string\n      True\n    \n    \n      5\n      Application\n      string\n      string\n      True\n    \n    \n      6\n      Product\n      string\n      string\n      True\n    \n    \n      7\n      Date_Datetime\n      datetime64[ns]\n      datetime64[ns]\n      True\n    \n    \n      8\n      Amount_Per_Acre\n      float64\n      float64\n      True\n    \n    \n      9\n      Unit\n      string\n      string\n      True\n    \n    \n      10\n      mgmt\n      bool\n      bool\n      True\n    \n    \n      11\n      Imputation_Notes\n      string\n      string\n      True\n    \n    \n      12\n      Ingredient\n      string\n      string\n      True"
  }
]