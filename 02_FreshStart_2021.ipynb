{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a688ca",
   "metadata": {},
   "source": [
    "# Process Data from 2021 into a consistent format. \n",
    "\n",
    "> This notebook is the first template for cleaning g2f data. Earlier years will be brought up to the standard naming and format which is based off of 2021's data release. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports ----\n",
    "import re\n",
    "import numpy as np # for np.nan\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "# import os   # for write_log, delete_logs\n",
    "import glob # for delete_all_logs\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import json # for saving a dict to txt with json.dumps\n",
    "\n",
    "import pickle\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03682dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2fd.internal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Settings ----\n",
    "# Helper functions,remove if no longer needed\n",
    "def prlst(lst): \n",
    "    \"This is just a helper function to ease formating lists of strings with each entryon a different line.\"\n",
    "    print('[')\n",
    "    for e in lst:\n",
    "        if e != lst[-1]:\n",
    "            print(\"'\"+e+\"', \")\n",
    "        else:\n",
    "            print(\"'\"+e+\"'\")\n",
    "    print(']')\n",
    "    \n",
    "def prlst2dct(lst): \n",
    "    \"This is just a helper function to ease formating lists of strings with each entryon a different line.\"\n",
    "    print('{')\n",
    "    for e in lst:\n",
    "        if e != lst[-1]:\n",
    "            print(\"'\"+e+\"': 'XXXXXXX', \")\n",
    "        else:\n",
    "            print(\"'\"+e+\"': 'XXXXXXX'\")\n",
    "    print('}')\n",
    "    \n",
    "def dash80(dash = '-'): return ''.join([dash for e in range(80)])\n",
    "\n",
    "\n",
    "# prlst([])\n",
    "# prlst2dct([])\n",
    "# dash80()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021\n",
    "year_string = '2021'\n",
    "\n",
    "meta_path = './data/raw/GenomesToFields_G2F_data_2021/z._2021_supplemental_info/g2f_2021_field_metadata.csv'\n",
    "phno_path = './data/raw/GenomesToFields_G2F_data_2021/a._2021_phenotypic_data/g2f_2021_phenotypic_clean_data.csv' # also contains 'g2f_2021_phenotypic_raw_data.csv' \n",
    "wthr_path = './data/raw/GenomesToFields_G2F_data_2021/b._2021_weather_data/g2f_2021_weather_cleaned.csv'\n",
    "soil_path = './data/raw/GenomesToFields_G2F_data_2021/c._2021_soil_data/g2f_2021_soil_data.csv'\n",
    "mgmt_path = './data/raw/GenomesToFields_G2F_data_2021/z._2021_supplemental_info/g2f_2021_agronomic_information.csv'\n",
    "\n",
    "\n",
    "meta = pd.read_csv(meta_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "phno = pd.read_csv(phno_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "wthr = pd.read_csv(wthr_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "soil = pd.read_csv(soil_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "mgmt = pd.read_csv(mgmt_path, encoding = \"ISO-8859-1\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dfc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Helper functions to match up the same data in different dataframes under different columns names\n",
    "\n",
    "# make a dictionary of column name : unique values\n",
    "def mk_uniq_val_dict(df1):\n",
    "    uniq_val_dict = {}\n",
    "    for df1_col in list(df1.columns):\n",
    "        uniq_val_dict.update({df1_col:set(df1[df1_col])})\n",
    "    return(uniq_val_dict)\n",
    "\n",
    "def pr_eq_list(lst1, lst2): return len(set(lst1) & set(lst2)) / len(set(lst1) | set(lst2))\n",
    "\n",
    "# take two dictionaries from `mk_uniq_val_dict` and a key to match, return a df of the n closest matches (based on set of column values)\n",
    "def mk_df_of_n_similar_cols(dct1, key_to_match, dct2, n = 1):\n",
    "    # key_to_match = 'Experiment_Code'        \n",
    "    lst = dct1[key_to_match]\n",
    "    # dct2 = dict1\n",
    "\n",
    "    keys = dct2.keys()\n",
    "    similarities = [pr_eq_list(lst, dct2[e]) for e in dct2.keys()]\n",
    "\n",
    "    similarityDf = pd.DataFrame(\n",
    "        zip(dct2.keys(), similarities), \n",
    "        columns=['Column2', 'PrMatch']\n",
    "    ).sort_values('PrMatch', ascending=False)\n",
    "    \n",
    "    output = similarityDf.head(n)\n",
    "    output.insert(0, column = 'Column1', value = key_to_match)\n",
    "    return(output)\n",
    "\n",
    "# take two dictionaries from `mk_uniq_val_dict` and find the best match for each key in dict1 in dict2. \n",
    "def mk_df_of_most_similar_cols(dict1, dict2):\n",
    "    return( pd.concat( [mk_df_of_n_similar_cols(dct1 = dict1, key_to_match = key, dct2 = dict2, n = 1) for key in dict1.keys() ]) )\n",
    "\n",
    "# Combine `mk_df_of_most_similar_cols` and `mk_uniq_val_dict` to find the closest matches between two columns in different dfs.\n",
    "# Should be helpful for finding columns with the same data but different names (e.g. Experiment vs Experiment_Code)\n",
    "def match_df_cols(df1, df2):\n",
    "    df1_cols = list(df1.columns)\n",
    "    df2_cols = list(df2.columns)\n",
    "\n",
    "    dict1 = mk_uniq_val_dict(df1)\n",
    "    dict2 = mk_uniq_val_dict(df2)\n",
    "    \n",
    "    return(mk_df_of_most_similar_cols(dict1, dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_df_cols(df1 = meta, df2 = soil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57afc255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba20000",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_name_dict = {\n",
    "# 'Experiment_Code': 'XXXXXXX', \n",
    "# 'Treatment': 'XXXXXXX', \n",
    "# 'City': 'XXXXXXX', \n",
    "# 'Farm': 'XXXXXXX', \n",
    "# 'Field': 'XXXXXXX', \n",
    "'Trial_ID (Assigned by collaborator for internal reference)': 'Trial_ID', \n",
    "'Soil_Taxonomic_ID and horizon description, if known': 'Soil_Taxonomic_ID', \n",
    "'Weather_Station_Serial_Number (Last four digits, e.g. m2700s#####)': 'Weather_Station_Serial_Number', \n",
    "'Weather_Station_Latitude (in decimal numbers NOT DMS)': 'Weather_Station_Latitude_Unit_Decimal', \n",
    "'Weather_Station_Longitude (in decimal numbers NOT DMS)': 'Weather_Station_Longitude_Unit_Decimal', \n",
    "'Date_weather_station_placed': 'Weather_Station_Placed_Unit_Datetime', \n",
    "'Date_weather_station_removed': 'Weather_Station_Removed_Unit_Datetime', \n",
    "'In-field weather station serial number': 'Weather_Station_In_Field_Serial_Number', \n",
    "'In-field_weather_station_latitude (in decimal)': 'Weather_Station_In_Field_Latitude_Unit_Decimal', \n",
    "'In-field_weather_station_longitude (in decimal)': 'Weather_Station_In_Field_Longitude_Unit_Decimal', \n",
    "# 'Previous_Crop': 'XXXXXXX', \n",
    "'Pre-plant_tillage_method(s)': 'Pre_Plant_Tillage', \n",
    "'In-season_tillage_method(s)': 'Post_Plant_Tillage', \n",
    "'Plot_length (center-alley to center-alley in feet)': 'Plot_Length_Unit_Feet', \n",
    "'Alley_length (in inches)': 'Alley_Length_Unit_Inches', \n",
    "'Row_spacing (in inches)': 'Row_Spacing_Unit_Inches', \n",
    "'Type_of_planter (fluted cone; belt cone; air planter)': 'Planter_Type', \n",
    "'Number_kernels_planted_per_plot (>200 seed/pack for cone planters)': 'Kernels_Per_Plot', \n",
    "# 'System_Determining_Moisture': 'XXXXXXX', \n",
    "# 'Pounds_Needed_Soil_Moisture': 'XXXXXXX', \n",
    "'Latitude_of_Field_Corner_#1 (lower left)': 'Field_Latitude_BL', \n",
    "'Longitude_of_Field_Corner_#1 (lower left)': 'Field_Longitude_BL', \n",
    "'Latitude_of_Field_Corner_#2 (lower right)': 'Field_Latitude_BR', \n",
    "'Longitude_of_Field_Corner_#2 (lower right)': 'Field_Longitude_BR', \n",
    "'Latitude_of_Field_Corner_#3 (upper right)': 'Field_Latitude_TR', \n",
    "'Longitude_of_Field_Corner_#3 (upper right)': 'Field_Longitude_TR', \n",
    "'Latitude_of_Field_Corner_#4 (upper left)': 'Field_Latitude_TL', \n",
    "'Longitude_of_Field_Corner_#4 (upper left)': 'Field_Longitude_TL', \n",
    "'Cardinal_Heading_Pass_1': 'Cardinal_Heading', \n",
    "'Local_Check_#1_Pedigree': 'Local_Check_Pedigree_1', \n",
    "'Local_Check_#1_Source': 'Local_Check_Source_1', \n",
    "'Local_Check_#2_Pedigree': 'Local_Check_Pedigree_2', \n",
    "'Local_Check_#2_Source': 'Local_Check_Source_2', \n",
    "'Local_Check_#3_Pedigree': 'Local_Check_Pedigree_3', \n",
    "'Local_Check_#3_Source': 'Local_Check_Source_3', \n",
    "'Local_Check_#4_Pedigree': 'Local_Check_Pedigree_4', \n",
    "'Local_Check_#4_Source': 'Local_Check_Source_4', \n",
    "'Local_Check_#5_Pedigree': 'Local_Check_Pedigree_5', \n",
    "'Local_Check_#5_Source': 'Local_Check_Source_5', \n",
    "'Issue/comment_#1': 'Comment_1', \n",
    "'Issue/comment_#2': 'Comment_2', \n",
    "'Issue/comment_#3': 'Comment_3', \n",
    "'Issue/comment_#4': 'Comment_4', \n",
    "'Issue/comment_#5': 'Comment_5', \n",
    "'Issue/comment_#6': 'Comment_6', \n",
    "'Issue/comment_#7': 'Comment_7', \n",
    "'Issue/comment_#8': 'Comment_8', \n",
    "'Issue/comment_#9': 'Comment_9', \n",
    "'Issue/comment_#10': 'Comment_70'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5588001",
   "metadata": {},
   "outputs": [],
   "source": [
    "phno_name_dict = {\n",
    "'Year': 'Year', \n",
    "'Field-Location': 'Experiment_Code', \n",
    "# 'State': 'XXXXXXX', \n",
    "# 'City': 'XXXXXXX', \n",
    "'Plot length (center-center in feet)': 'Plot_Length_Unit_Feet', \n",
    "'Plot area (ft2)': 'Plot_Area_Unit_Feet2', \n",
    "'Alley length (in inches)': 'Alley_Length_Unit_Inches', \n",
    "'Row spacing (in inches)': 'Row_Spacing_Unit_Inches', \n",
    "'Rows per plot': 'Rows_Per_Plot', \n",
    "'# Seed per plot': 'Seeds_Per_Plot', \n",
    "# 'Experiment': 'XXXXXXX', \n",
    "# 'Source': 'XXXXXXX', \n",
    "# 'Pedigree': 'XXXXXXX', \n",
    "# 'Family': 'XXXXXXX', \n",
    "# 'Tester': 'XXXXXXX', \n",
    "# 'Replicate': 'XXXXXXX', \n",
    "# 'Block': 'XXXXXXX', \n",
    "# 'Plot': 'XXXXXXX', \n",
    "# 'Plot_ID': 'XXXXXXX', \n",
    "# 'Range': 'XXXXXXX', \n",
    "# 'Pass': 'XXXXXXX', \n",
    "'Date Plot Planted [MM/DD/YY]': 'Planted_Unit_Datetime', \n",
    "'Date Plot Harvested [MM/DD/YY]': 'Harvested_Unit_Datetime', \n",
    "'Anthesis [MM/DD/YY]': 'Anthesis_Unit_Datetime', \n",
    "'Silking [MM/DD/YY]': 'Silking_Unit_Datetime', \n",
    "'Anthesis [days]': 'Anthesis_Unit_Days', \n",
    "'Silking [days]': 'Silking_Unit_Days', \n",
    "'Plant Height [cm]': 'Plant_Height_Unit_cm', \n",
    "'Ear Height [cm]': 'Ear_Height_Unit_cm', \n",
    "'Stand Count [# of plants]': 'Stand_Count_Unit_Number', \n",
    "'Root Lodging [# of plants]': 'Root_Lodging_Unit_Number', \n",
    "'Stalk Lodging [# of plants]': 'Stalk_Lodging_Unit_Number', \n",
    "'Grain Moisture [%]': 'Grain_Moisture_Unit_Percent', \n",
    "'Test Weight [lbs]': 'Test_Weight_Unit_lbs', \n",
    "'Plot Weight [lbs]': 'Plot_Weight_Unit_lbs', \n",
    "'Grain Yield (bu/A)': 'Grain_Yield_Unit_bu_Per_A', \n",
    "\"Plot Discarded [enter 'yes' or blank]\": 'Discarded', \n",
    "'Comments': 'Phenotype_Comments', \n",
    "# 'Filler': 'XXXXXXX', \n",
    "'Snap [# of plants]': 'Snap_Unit_Number'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_name_dict = {\n",
    "# 'Grower': 'XXXXXXX', \n",
    "'Location': 'Experiment_Code', \n",
    "'Date Received': 'Recieved_Date_Unit_Datetime', \n",
    "'Date Reported': 'Processed_Date_Unit_Datetime', \n",
    "'E Depth': 'Depth_Unit_UNK', \n",
    "'1:1 Soil pH': 'Soil_1_to_1_Unit_pH', \n",
    "'WDRF Buffer pH': 'WDRF_Buffer_Unit_pH', \n",
    "'1:1 S Salts mmho/cm': 'Soluable_Salts_Unit_mmho_Per_cm', \n",
    "'Texture No': 'Texture_Number', \n",
    "'Organic Matter LOI %': 'Organic_Matter_Unit_Percent', \n",
    "'Nitrate-N ppm N': 'Nitrates_Unit_ppm', \n",
    "'lbs N/A': 'N_per_Acre_Unit_lbs', \n",
    "'Potassium ppm K': 'K_Unit_ppm', \n",
    "'Sulfate-S ppm S': 'Sulfate_Unit_ppm', \n",
    "'Calcium ppm Ca': 'Ca_Unit_ppm', \n",
    "'Magnesium ppm Mg': 'Mg_Unit_ppm', \n",
    "'Sodium ppm Na': 'Na_Unit_ppm', \n",
    "'CEC/Sum of Cations me/100g': 'Cation_Exchange_Capacity', \n",
    "'%H Sat': 'H_Sat_Unit_Percent', \n",
    "'%K Sat': 'K_Sat_Unit_Percent', \n",
    "'%Ca Sat': 'Ca_Sat_Unit_Percent', \n",
    "'%Mg Sat': 'Mg_Sat_Unit_Percent', \n",
    "'%Na Sat': 'Na_Sat_Unit_Percent', \n",
    "'Mehlich P-III ppm P': 'Mehlich_PIII_P_Unit_ppm', \n",
    "'% Sand': 'Sand_Unit_Percent', \n",
    "'% Silt': 'Silt_Unit_Percent', \n",
    "'% Clay': 'Clay_Unit_Percent', \n",
    "# 'Texture': 'XXXXXXX', \n",
    "'Comments': 'Soil_Comments'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1583e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wthr_name_dict = {\n",
    "'Field Location': 'Experiment_Code', \n",
    "'Station ID': 'Weather_Station_ID', \n",
    "'NWS Network': 'NWS_Network', \n",
    "'NWS Station': 'NWS_Station', \n",
    "'Date_key': 'Datetime', \n",
    "# 'Month': 'XXXXXXX', \n",
    "# 'Day': 'XXXXXXX', \n",
    "# 'Year': 'XXXXXXX', \n",
    "# 'Time': 'XXXXXXX', \n",
    "'Temperature [C]': 'Temperature_Unit_C', \n",
    "'Dew Point [C]': 'Dew_Point_Unit_C', \n",
    "'Relative Humidity [%]': 'Relative_Humidity_Unit_Percent', \n",
    "'Solar Radiation [W/m2]': 'Solar_Radiation_Unit_W_per_m2', \n",
    "'Rainfall [mm]': 'Rainfall_Unit_mm', \n",
    "'Wind Speed [m/s]': 'Wind_Speed_Unit_m_per_s', \n",
    "'Wind Direction [degrees]': 'Wind_Direction_Unit_Degrees', \n",
    "'Wind Gust [m/s]': 'Wind_Gust_Unit_m_per_s', \n",
    "'Soil Temperature [C]': 'Soil_Temperature_Unit_C', \n",
    "'Soil Moisture [%VWC]': 'Soil_Moisture_Unit_Percent_VWC', \n",
    "'Soil EC [mS/cm]': 'Soil_EC_Unit_mS_per_cm', \n",
    "'UV Light [uM/m2s]': 'UV_Light_Unit_uM_per_m2s', \n",
    "'PAR [uM/m2s]': 'PAR_Unit_uM_per_m2s'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901376a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt_name_dict = {\n",
    "'Location': 'Experiment_Code', \n",
    "'Application_or_treatment': 'Application', \n",
    "'Product_or_nutrient_applied': 'Product', \n",
    "'Date_of_application': 'Date_Datetime', \n",
    "'Quantity_per_acre': 'Amount_Per_Acre', \n",
    "'Application_unit': 'Unit'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_df_cols(df1 = meta, df2 = soil)\n",
    "# set(meta.Experiment_Code), set(soil.Location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ab577",
   "metadata": {},
   "source": [
    "# Rename\n",
    "**Naming rules:**\n",
    "- One dict for each input df\n",
    "- Comment out anything that shouldn't be changed\n",
    "- Upper_Upper_Unit_\\$unit\n",
    "- Upper_$number\n",
    "- No special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta = meta.rename(columns=meta_name_dict)\n",
    "phno = phno.rename(columns=phno_name_dict)\n",
    "soil = soil.rename(columns=soil_name_dict)\n",
    "wthr = wthr.rename(columns=wthr_name_dict)\n",
    "mgmt = mgmt.rename(columns=mgmt_name_dict)\n",
    "\n",
    "# add indicator columns to help with debugging merge\n",
    "meta['meta'] = True\n",
    "phno['phno'] = True\n",
    "soil['soil'] = True\n",
    "wthr['wthr'] = True\n",
    "mgmt['mgmt'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[e.shape for e in [meta, phno, soil, wthr, mgmt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16113d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# check Experiment_Code columns for any unexpected columns\n",
    "def find_unrecognized_experiments(column, return_all_exps = False):\n",
    "    known_exps = ['COH1', 'DEH1', 'GAH1', 'GAH2', 'GEH1', 'IAH1', 'IAH2', 'IAH3', 'IAH4', 'ILH1', 'INH1', 'MIH1', 'MNH1', 'NCH1', 'NEH1', 'NEH2', 'NEH3', 'NYH2', 'NYH3', 'NYS1', 'SCH1', 'TXH1', 'TXH2', 'TXH3', 'WIH1', 'WIH2', 'WIH3']\n",
    "    if return_all_exps:\n",
    "        known_exps.sort()\n",
    "        return(known_exps)\n",
    "    else:\n",
    "        unknown_exps = [str(e) for e in list(set(column)) if e not in known_exps]\n",
    "        unknown_exps.sort()\n",
    "        return(unknown_exps)\n",
    "\n",
    "# find_unrecognized_experiments(soil.Experiment_Code, print_all_exps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068b7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b3a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# sanitize Experiment Codes\n",
    "\n",
    "def sanitize_Experiment_Codes(df, simple_renames= {}, split_renames= {}):\n",
    "    # simple renames\n",
    "    for e in simple_renames.keys():\n",
    "        mask = (df.Experiment_Code == e)\n",
    "        df.loc[mask, 'Experiment_Code'] = simple_renames[e]\n",
    "\n",
    "    # splits\n",
    "    # pull out the relevant multiname rows, copy, rename, append\n",
    "    for e in split_renames.keys():\n",
    "        mask = (df.Experiment_Code == e)\n",
    "        temp = df.loc[mask, :] \n",
    "\n",
    "        df = df.loc[~mask, :]\n",
    "        for e2 in split_renames[e]:\n",
    "            temp2 = temp.copy()\n",
    "            temp2['Experiment_Code'] = e2\n",
    "            df = df.merge(temp2, how = 'outer')\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1589ac0",
   "metadata": {},
   "source": [
    "# Sanatize ID columns as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soil = sanitize_Experiment_Codes(\n",
    "    df = soil, \n",
    "    simple_renames = {\n",
    "        'W1H1': 'WIH1', \n",
    "        'W1H2': 'WIH2', \n",
    "        'W1H3': 'WIH3'\n",
    "    }, \n",
    "    split_renames = {\n",
    "        'NEH2_NEH3': ['NEH2', 'NEH3']\n",
    "    })\n",
    "\n",
    "wthr = sanitize_Experiment_Codes(\n",
    "    df = wthr, \n",
    "    simple_renames = {\n",
    "    }, \n",
    "    split_renames = {\n",
    "        'NEH2_NEH3': ['NEH2', 'NEH3'],\n",
    "        'NYH3_NYS1': ['NYS1', 'NYH3'],\n",
    "        'TXH1_TXH3': ['TXH1', 'TXH3']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm everything's okay\n",
    "print(\n",
    "  'meta', find_unrecognized_experiments(meta.Experiment_Code, return_all_exps=False), \n",
    "'\\nphno', find_unrecognized_experiments(phno.Experiment_Code, return_all_exps=False),\n",
    "'\\nsoil', find_unrecognized_experiments(soil.Experiment_Code, return_all_exps=False),\n",
    "'\\nwthr', find_unrecognized_experiments(wthr.Experiment_Code, return_all_exps=False),\n",
    "'\\nmgmt', find_unrecognized_experiments(mgmt.Experiment_Code, return_all_exps=False),\n",
    "'\\nall ', find_unrecognized_experiments([], return_all_exps=True)\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Independent variables\n",
    "# ## Constant in season\n",
    "\n",
    "\n",
    "# ## Variable over season\n",
    "\n",
    "\n",
    "# # Dependent variables\n",
    "# # phno\n",
    "# # Key indexing variables\n",
    "# 'Year', 'Experiment_Code', 'Range', 'Pass', 'Plot', \n",
    "# # Location\n",
    "# 'State', 'City', 'Block', 'Plot_ID', 'Replicate',\n",
    "# # Treatment/mgmt info\n",
    "# 'Experiment', \n",
    "# 'Plot_Length_Unit_Feet', 'Plot_Area_Unit_Feet2', 'Alley_Length_Unit_Inches',\n",
    "# 'Row_Spacing_Unit_Inches', 'Rows_Per_Plot', 'Seeds_Per_Plot',\n",
    "# # Genetic info\n",
    "# 'Source', 'Pedigree', 'Family', 'Tester', \n",
    "# 'Filler',\n",
    "# # Plot info\n",
    "# 'Discarded',\n",
    "# 'Comments',  \n",
    "# # Response variables\n",
    "# 'Planted_Unit_Datetime',\n",
    "# 'Harvested_Unit_Datetime', \n",
    "# 'Anthesis_Unit_Datetime', 'Silking_Unit_Datetime',\n",
    "# 'Anthesis_Unit_Days', 'Silking_Unit_Days', 'Plant_Height_Unit_cm',\n",
    "# 'Ear_Height_Unit_cm', 'Stand_Count_Unit_Number',\n",
    "# 'Root_Lodging_Unit_Number', 'Stalk_Lodging_Unit_Number',\n",
    "# 'Grain_Moisture_Unit_Percent', 'Test_Weight_Unit_lbs',\n",
    "# 'Plot_Weight_Unit_lbs', 'Grain_Yield_Unit_bu_Per_A', \n",
    "# 'Snap_Unit_Number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c37165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find minimum cols needed to index all rows\n",
    "# df = phno\n",
    "# id_cols = ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot',]\n",
    "# candidate_cols = ['State', 'City',\n",
    "#                  'Experiment', 'Source', 'Pedigree', 'Family', 'Tester', 'Replicate',\n",
    "#                   'Block',  'Plot_ID']\n",
    "# target = df.shape[0]\n",
    "\n",
    "# output = pd.DataFrame(zip(\n",
    "#     candidate_cols,\n",
    "#     [df.loc[:, id_cols+[e]].drop_duplicates().shape[0] for e in candidate_cols]\n",
    "#    ), columns=['Additional_ID', 'Uniq_Vals'])\n",
    "\n",
    "# output.assign(At_Target=lambda x:x.Uniq_Vals == target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94761eb",
   "metadata": {},
   "source": [
    "# Rearrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb762bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate static and dynamic values\n",
    "sval = phno.merge(soil, how = 'outer')\n",
    "sval = sval.merge(meta, how = 'outer') # This introduces 3 sites that have no data\n",
    "# sval.shape # used to confirm nrow = #20574 + 3\n",
    "\n",
    "# these tables are different enought we'll keep them separate\n",
    "# mgmt\n",
    "# unfortunately we need multiples because at least one field treats different passes differently\n",
    "mgmt = phno.loc[:, ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot', 'phno']\n",
    "               ].drop_duplicates().merge(mgmt, how = 'outer')\n",
    "# confirm there are no rows in mgmt that are not in phno\n",
    "temp = mgmt.loc[(~mgmt.phno & mgmt.mgmt), :]\n",
    "if 0 != temp.shape[0]:\n",
    "    print(temp)\n",
    "else:\n",
    "    mgmt = mgmt.loc[mgmt.mgmt.notna(), :].drop(columns = 'phno')\n",
    "\n",
    "\n",
    "# wthr\n",
    "# There's only ever one weather station so we have to worry about imputation but not duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set each id col to a string\n",
    "for i in ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot']:\n",
    "    sval[i] = sval[i].astype('string')\n",
    "    mgmt[i]  =  mgmt[i].astype('string')\n",
    "    \n",
    "    if i not in ['Range', 'Pass', 'Plot']:\n",
    "        wthr[i]  =  wthr[i].astype('string')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a94f13",
   "metadata": {},
   "source": [
    "# Sanitize Non-ID columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc8720",
   "metadata": {},
   "source": [
    "## Sanitization functions\n",
    "\n",
    "The pattern to use is:\n",
    " 1. Alter the dataframe\n",
    " 1. Test the dataframe against expectations\n",
    " \n",
    "The main tasks that need to be completed are:\n",
    " 1. Identify values that can't be converted to the expected data type. The \"find_unconvertable_\" family of functions should be used. \n",
    "     1. `find_unconvertable_datetimes`\n",
    "     \n",
    " 1. For simple renaming (e.g. misspellings) or splitting non-tidy data into two rows (\"entry1-entry2\" -> \"entry1\", \"entry2\") use `sanitize_col` \n",
    " 1. Move values that are ambigous but pertain to data imputation to \"Imputation_Notes\" using `relocate_to_Imputation_Notes`\n",
    " 1. If new columns need to be added (e.g. mgmt.Ingredient for parsed components of Product (e.g. elements) ) this should be accomplished with `safe_create_col`.\n",
    " 1. Any one off changes should be accomplised manually. \n",
    " 1. Confirm columns match the expected types with `check_df_dtype_expectations`, and report mismatches. \n",
    "\n",
    "\n",
    "These steps should be completed for each dataframe in turn to minimize the cognitive load of the reader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Make versions of `find_unconvertable_datetimes` for other datatype\n",
    "# make a function to find the unexpected entries so it's easy to write the santization code\n",
    "\n",
    "# in a column, report all the values causing errors OR an index of these values\n",
    "def find_unconvertable_datetimes(df_col, pattern = '%m/%d/%y', index = False):\n",
    "    datetime_errors = pd.to_datetime(pd.Series(df_col), format = pattern, errors='coerce').isna()\n",
    "    if index == True:\n",
    "        return(datetime_errors)\n",
    "    else:\n",
    "        # This is a interesting trick. Python's nan is not equal to itself.\n",
    "        # missing values can't become datetimes so nan is returned if there's a missing value\n",
    "        # This list comprehension removes nan (which is otherwise stubborn to remove) because nan != nan\n",
    "        return([e for e in list(set(df_col[datetime_errors])) if e == e]) \n",
    "\n",
    "    \n",
    "def find_unconvertable_numerics(df_col, index = False):\n",
    "    numeric_errors = pd.to_numeric(pd.Series(df_col), errors='coerce').isna()\n",
    "    if index == True:\n",
    "        return(numeric_errors)\n",
    "    else:\n",
    "        # This is a interesting trick. Python's nan is not equal to itself.\n",
    "        # missing values can't become datetimes so nan is returned if there's a missing value\n",
    "        # This list comprehension removes nan (which is otherwise stubborn to remove) because nan != nan\n",
    "        return([e for e in list(set(df_col[numeric_errors])) if e == e]) \n",
    "\n",
    "    \n",
    "# generalized version of `sanitize_Experiment_Codes`\n",
    "def sanitize_col(df, col, simple_renames= {}, split_renames= {}):\n",
    "    # simple renames\n",
    "    for e in simple_renames.keys():\n",
    "        mask = (df[col] == e)\n",
    "        df.loc[mask, col] = simple_renames[e]\n",
    "\n",
    "    # splits\n",
    "    # pull out the relevant multiname rows, copy, rename, append\n",
    "    for e in split_renames.keys():\n",
    "        mask = (df[col] == e)\n",
    "        temp = df.loc[mask, :] \n",
    "\n",
    "        df = df.loc[~mask, :]\n",
    "        for e2 in split_renames[e]:\n",
    "            temp2 = temp.copy()\n",
    "            temp2[col] = e2\n",
    "            df = df.merge(temp2, how = 'outer')\n",
    "\n",
    "    return(df)\n",
    "\n",
    "\n",
    "# If the Imputation_Notes column doesnt exist, create it. So long as it wouldn't overwrite any imputation notes move each specified value and replace it with nan.\n",
    "def relocate_to_Imputation_Notes(df, col, val_list):\n",
    "    if not 'Imputation_Notes' in df.columns:\n",
    "        df.loc[:, 'Imputation_Notes'] = np.nan\n",
    "\n",
    "    for relocate in val_list:\n",
    "        mask = (df.loc[:, col] == relocate)\n",
    "        mask_Impute_Full = ((df.loc[:, 'Imputation_Notes'] == '') | (df.loc[:, 'Imputation_Notes'].isna()))\n",
    "        # check if this contains anyting\n",
    "        overwrite_danger = df.loc[(mask & ~mask_Impute_Full), 'Imputation_Notes']\n",
    "        if overwrite_danger.shape[0] > 0:\n",
    "            print(\"Warning! The following values will be overwritten. Skipping relocation.\")\n",
    "            print(overwrite_danger)\n",
    "        else:\n",
    "            df.loc[(mask), 'Imputation_Notes'] = df.loc[(mask), col]\n",
    "            df.loc[(mask), col] = np.nan\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# helper function so we can ask for a new column don't have to worry about overwritting a if it already exists \n",
    "def safe_create_col(df, col_name):\n",
    "    if not col_name in df.columns:\n",
    "        df.loc[:, col_name] = np.nan\n",
    "    return(df)\n",
    "\n",
    "# little helper function to make this easier. Make all the columns in a list into dtype string.\n",
    "# require the column to exist to make this safe.\n",
    "# to make things even easier, use a list comprehension to pull out the keys in the *_col_dtype dict \n",
    "# that have value of 'string'!\n",
    "def cols_astype_string(df, col_list):\n",
    "    for e in [ee for ee in col_list if ee in df.columns]:\n",
    "        df[e] = df[e].astype('string')\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# Ignore columns that don't exist in the dataframe even if they're specified in the dict\n",
    "# For testing that sanitization was successful\n",
    "# a function to check the type of each column \n",
    "# shouldn't _change_ anything, just report what I need to fix\n",
    "def check_df_dtype_expectations(df, dtype_dct):\n",
    "    found = pd.DataFrame(zip(\n",
    "        df.columns,\n",
    "        [str(df[e].dtype) for e in df.columns]\n",
    "    ), columns=['Column', 'dtype'])\n",
    "\n",
    "\n",
    "    expected = pd.DataFrame(zip(dtype_dct.keys(), dtype_dct.values()),\n",
    "                 columns=['Column', 'Expected_dtype']\n",
    "                )\n",
    "    mask = [True if e in df.columns else False for e in expected.Column]\n",
    "    expected = expected.loc[mask, ]\n",
    "    \n",
    "    out = found.merge(expected, how = 'outer')\n",
    "    out = out.assign(Pass = out.dtype == out.Expected_dtype)\n",
    "\n",
    "    print(str(sum(out.Pass))+'/'+str(len(out.Pass))+' Columns pass.')\n",
    "    return(out)\n",
    "\n",
    "# each df should get individual treatment with these steps. Probably most readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cad031",
   "metadata": {},
   "source": [
    "## Sanitization: Column data type expectations\n",
    "Note: to handle missing values some columns that would otherwise be ints are floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc39964",
   "metadata": {},
   "outputs": [],
   "source": [
    "sval_col_dtypes = {\n",
    "    'Year': 'string', \n",
    "    'Experiment_Code': 'string', \n",
    "    'State': 'string', \n",
    "    'City': 'string', \n",
    "    'Plot_Length_Unit_Feet': 'float64', \n",
    "    'Plot_Area_Unit_Feet2': 'float64', \n",
    "    'Alley_Length_Unit_Inches': 'float64', \n",
    "    'Row_Spacing_Unit_Inches': 'float64', \n",
    "    'Rows_Per_Plot': 'float64', \n",
    "    'Seeds_Per_Plot': 'float64', \n",
    "    'Experiment': 'string', \n",
    "    'Source': 'string', \n",
    "    'Pedigree': 'string', \n",
    "    'Family': 'string', \n",
    "    'Tester': 'string', \n",
    "    'Replicate': 'string', \n",
    "    'Block': 'string', \n",
    "    'Plot': 'string', \n",
    "    'Plot_ID': 'string', \n",
    "    'Range': 'string', \n",
    "    'Pass': 'string', \n",
    "    'Planted_Unit_Datetime': 'datetime64[ns]', \n",
    "    'Harvested_Unit_Datetime': 'datetime64[ns]', \n",
    "    'Anthesis_Unit_Datetime': 'datetime64[ns]', \n",
    "    'Silking_Unit_Datetime': 'datetime64[ns]', \n",
    "    'Anthesis_Unit_Days': 'float64', \n",
    "    'Silking_Unit_Days': 'float64', \n",
    "    'Plant_Height_Unit_cm': 'float64', \n",
    "    'Ear_Height_Unit_cm': 'float64', \n",
    "    'Stand_Count_Unit_Number': 'float64', \n",
    "    'Root_Lodging_Unit_Number': 'float64', \n",
    "    'Stalk_Lodging_Unit_Number': 'float64', \n",
    "    'Grain_Moisture_Unit_Percent': 'float64', \n",
    "    'Test_Weight_Unit_lbs': 'float64', \n",
    "    'Plot_Weight_Unit_lbs': 'float64', \n",
    "    'Grain_Yield_Unit_bu_Per_A': 'float64', \n",
    "    'Discarded': 'bool', \n",
    "    'Phenotype_Comments': 'string', \n",
    "    'Filler': 'string', \n",
    "    'Snap_Unit_Number': 'float64', \n",
    "'phno': 'bool', \n",
    "    'Grower': 'string', \n",
    "    'Recieved_Date_Unit_Datetime': 'datetime64[ns]', \n",
    "    'Processed_Date_Unit_Datetime': 'datetime64[ns]', \n",
    "    'Depth_Unit_UNK': 'float64', \n",
    "    'Soil_1_to_1_Unit_pH': 'float64', \n",
    "    'WDRF_Buffer_Unit_pH': 'float64', \n",
    "    'Soluable_Salts_Unit_mmho_Per_cm': 'float64', \n",
    "    'Texture_Number': 'float64', \n",
    "    'Organic_Matter_Unit_Percent': 'float64', \n",
    "    'Nitrates_Unit_ppm': 'float64', \n",
    "    'N_per_Acre_Unit_lbs': 'float64', \n",
    "    'K_Unit_ppm': 'float64', \n",
    "    'Sulfate_Unit_ppm': 'float64', \n",
    "    'Ca_Unit_ppm': 'float64', \n",
    "    'Mg_Unit_ppm': 'float64', \n",
    "    'Na_Unit_ppm': 'float64', \n",
    "    'Cation_Exchange_Capacity': 'float64', \n",
    "    'H_Sat_Unit_Percent': 'float64', \n",
    "    'K_Sat_Unit_Percent': 'float64', \n",
    "    'Ca_Sat_Unit_Percent': 'float64', \n",
    "    'Mg_Sat_Unit_Percent': 'float64', \n",
    "    'Na_Sat_Unit_Percent': 'float64', \n",
    "    'Mehlich_PIII_P_Unit_ppm': 'float64', \n",
    "    'Sand_Unit_Percent': 'float64', \n",
    "    'Silt_Unit_Percent': 'float64', \n",
    "    'Clay_Unit_Percent': 'float64', \n",
    "    'Texture': 'string', \n",
    "    'Soil_Comments': 'string', \n",
    "'soil': 'bool', \n",
    "    'Treatment': 'string', \n",
    "    'Farm': 'string', \n",
    "    'Field': 'string', \n",
    "    'Trial_ID': 'string', \n",
    "    'Soil_Taxonomic_ID': 'string', \n",
    "    'Weather_Station_Serial_Number': 'string', \n",
    "    'Weather_Station_Latitude_Unit_Decimal': 'float64', \n",
    "    'Weather_Station_Longitude_Unit_Decimal': 'float64', \n",
    "    'Weather_Station_Placed_Unit_Datetime': 'datetime64[ns]', \n",
    "    'Weather_Station_Removed_Unit_Datetime': 'datetime64[ns]', \n",
    "    'Weather_Station_In_Field_Serial_Number': 'string', \n",
    "    'Weather_Station_In_Field_Latitude_Unit_Decimal': 'float64', \n",
    "    'Weather_Station_In_Field_Longitude_Unit_Decimal': 'float64', \n",
    "    'Previous_Crop': 'string', \n",
    "    'Pre_Plant_Tillage': 'string', \n",
    "    'Post_Plant_Tillage': 'string', \n",
    "    'Planter_Type': 'string', \n",
    "    'Kernels_Per_Plot': 'float64', \n",
    "    'System_Determining_Moisture': 'string', \n",
    "    'Pounds_Needed_Soil_Moisture': 'float64', \n",
    "    'Field_Latitude_BL': 'float64', \n",
    "    'Field_Longitude_BL': 'float64', \n",
    "    'Field_Latitude_BR': 'float64', \n",
    "    'Field_Longitude_BR': 'float64', \n",
    "    'Field_Latitude_TR': 'float64', \n",
    "    'Field_Longitude_TR': 'float64', \n",
    "    'Field_Latitude_TL': 'float64', \n",
    "    'Field_Longitude_TL': 'float64', \n",
    "    'Cardinal_Heading': 'float64', \n",
    "    'Local_Check_Pedigree_1': 'string', \n",
    "    'Local_Check_Source_1': 'string', \n",
    "    'Local_Check_Pedigree_2': 'string', \n",
    "    'Local_Check_Source_2': 'string', \n",
    "    'Local_Check_Pedigree_3': 'string', \n",
    "    'Local_Check_Source_3': 'string', \n",
    "    'Local_Check_Pedigree_4': 'string', \n",
    "    'Local_Check_Source_4': 'string', \n",
    "    'Local_Check_Pedigree_5': 'string', \n",
    "    'Local_Check_Source_5': 'string', \n",
    "    'Comment_1': 'string', \n",
    "    'Comment_2': 'string', \n",
    "    'Comment_3': 'string', \n",
    "    'Comment_4': 'string', \n",
    "    'Comment_5': 'string', \n",
    "    'Comment_6': 'string', \n",
    "    'Comment_7': 'string', \n",
    "    'Comment_8': 'string', \n",
    "    'Comment_9': 'string', \n",
    "    'Comment_70': 'string', \n",
    "'meta': 'bool',\n",
    "    'Imputation_Notes': 'string'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wthr_col_dtypes = {\n",
    "    'Experiment_Code': 'string', \n",
    "    'Weather_Station_ID': 'string', \n",
    "    'NWS_Network': 'string', \n",
    "    'NWS_Station': 'string', \n",
    "    'Datetime': 'datetime64[ns]', \n",
    "    'Month': 'string', \n",
    "    'Day': 'string', \n",
    "    'Year': 'string', \n",
    "    'Time': 'string', \n",
    "    'Temperature_Unit_C': 'float64', \n",
    "    'Dew_Point_Unit_C': 'float64', \n",
    "    'Relative_Humidity_Unit_Percent': 'float64', \n",
    "    'Solar_Radiation_Unit_W_per_m2': 'float64', \n",
    "    'Rainfall_Unit_mm': 'float64', \n",
    "    'Wind_Speed_Unit_m_per_s': 'float64', \n",
    "    'Wind_Direction_Unit_Degrees': 'float64', \n",
    "    'Wind_Gust_Unit_m_per_s': 'float64', \n",
    "    'Soil_Temperature_Unit_C': 'float64', \n",
    "    'Soil_Moisture_Unit_Percent_VWC': 'float64', \n",
    "    'Soil_EC_Unit_mS_per_cm': 'float64', \n",
    "    'UV_Light_Unit_uM_per_m2s': 'float64', \n",
    "    'PAR_Unit_uM_per_m2s': 'float64', \n",
    "'wthr': 'bool',\n",
    "    'Imputation_Notes': 'string'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ef35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt_col_dtypes = {\n",
    "    'Year': 'string',   \n",
    "    'Experiment_Code': 'string', \n",
    "    'Range': 'string',\n",
    "    'Pass': 'string',\n",
    "    'Plot': 'string',\n",
    "    'Application': 'string', \n",
    "    'Product': 'string', \n",
    "    'Date_Datetime': 'datetime64[ns]', \n",
    "    'Amount_Per_Acre': 'float64', \n",
    "    'Unit': 'string', \n",
    "'mgmt': 'bool',\n",
    "    'Imputation_Notes': 'string',\n",
    "    'Ingredient': 'string'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972cfb9f",
   "metadata": {},
   "source": [
    "# Sanitization: Alter entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccee17b",
   "metadata": {},
   "source": [
    "## Static values (within season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe5bcc",
   "metadata": {},
   "source": [
    "### Datetime containing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e33b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date cols into datetime. Lean on pd.to_datetime() to infer the format, assume that each site uses the same format.\n",
    "\n",
    "for e in ['Planted_Unit_Datetime', \n",
    "    'Harvested_Unit_Datetime', \n",
    "    'Anthesis_Unit_Datetime', \n",
    "    'Silking_Unit_Datetime', \n",
    "    'Recieved_Date_Unit_Datetime', \n",
    "    'Processed_Date_Unit_Datetime', \n",
    "    'Weather_Station_Placed_Unit_Datetime', \n",
    "    'Weather_Station_Removed_Unit_Datetime'\n",
    "    ]:\n",
    "# find_unconvertable_datetimes(df_col=sval[e], pattern='%Y-%m-%d %H:%M', index=False)\n",
    "\n",
    "    sval['Datetime_Temp'] = pd.to_datetime(np.nan)\n",
    "\n",
    "    for code in list(sval.Experiment_Code.drop_duplicates()):\n",
    "    # code = list(sval.Experiment_Code.drop_duplicates())[0]\n",
    "        sval.loc[sval.Experiment_Code == code, 'Datetime_Temp'\n",
    "                 ] = pd.to_datetime(sval.loc[sval.Experiment_Code == code, e])\n",
    "\n",
    "    sval.loc[:, e] = sval.loc[:, 'Datetime_Temp'] \n",
    "\n",
    "sval = sval.drop(columns = 'Datetime_Temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62420a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> floats\n",
    "\n",
    "# [find_unconvertable_numerics(df_col = sval[e], index = False) for e in [\n",
    "#     'Alley_Length_Unit_Inches',\n",
    "# 'Row_Spacing_Unit_Inches',\n",
    "# 'Pounds_Needed_Soil_Moisture'\n",
    "# ]]\n",
    "\n",
    "sval = sanitize_col(\n",
    "    df = sval, \n",
    "    col = 'Pounds_Needed_Soil_Moisture', \n",
    "    simple_renames= {'3 to 4':'3.5'}, \n",
    "    split_renames= {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert types\n",
    "for e in ['Alley_Length_Unit_Inches', 'Row_Spacing_Unit_Inches', 'Pounds_Needed_Soil_Moisture',\n",
    "         'Anthesis_Unit_Days', 'Silking_Unit_Days', 'Kernels_Per_Plot']:\n",
    "    err_list = find_unconvertable_numerics(df_col = sval[e], index = False)\n",
    "    if err_list != []:\n",
    "        print(e)\n",
    "        print(err_list)\n",
    "    else:\n",
    "        sval[e] = sval[e].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bool\n",
    "sval = sanitize_col(\n",
    "    df = sval, \n",
    "    col = 'Discarded', \n",
    "    simple_renames= {\n",
    "        'Yes':'True',\n",
    "        'yes':'True'}, \n",
    "    split_renames= {})\n",
    "\n",
    "# set missing to false\n",
    "sval.loc[sval.Discarded.isna(), 'Discarded'] = 'False'\n",
    "sval.Discarded = sval.Discarded.map({'True': True, 'False': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acff56d",
   "metadata": {},
   "source": [
    "### Simple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b742d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bool\n",
    "sval['phno'] = sval['phno'].astype('bool')\n",
    "sval['soil'] = sval['soil'].astype('bool')\n",
    "sval['meta'] = sval['meta'].astype('bool')\n",
    "\n",
    "# to string\n",
    "sval = cols_astype_string(\n",
    "    df = sval, \n",
    "    col_list = [key for key in sval_col_dtypes.keys() if sval_col_dtypes[key] == 'string'])\n",
    "\n",
    "sval.Year = year_string\n",
    "sval.Year = sval.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0e476",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec742610",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = check_df_dtype_expectations(df = sval, dtype_dct = sval_col_dtypes)\n",
    "\n",
    "if sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n",
    "    pass\n",
    "else:\n",
    "    print(checkpoint)\n",
    "    print()\n",
    "\n",
    "# checkpoint.loc[~checkpoint.Pass, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54014602",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be674d",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6659168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of writing regexes to figure out the mose likely format for each datetime, we assume each experiment will be consistent withing that experiment\n",
    "# and let pd figure it out.\n",
    "# wthr['Datetime_Temp'] = pd.to_datetime(np.nan)\n",
    "\n",
    "# for code in list(wthr.loc[:, 'Experiment_Code'].drop_duplicates()):\n",
    "#     wthr.loc[wthr.Experiment_Code == code, 'Datetime_Temp'] = pd.to_datetime(wthr.loc[wthr.Experiment_Code == code, 'Datetime'], errors='coerce')\n",
    "\n",
    "\n",
    "# ... or we use the fields in the df to make a consistent format\n",
    "wthr = cols_astype_string(\n",
    "    df = wthr, \n",
    "    col_list = ['Year', 'Month', 'Day', 'Time'])\n",
    "\n",
    "wthr = sanitize_col(\n",
    "    df = wthr,\n",
    "    col = 'Time', \n",
    "    simple_renames= {'24:00:00': '00:00:00'}, # this could be day + 24 h instead of a miscoded day + 0 h\n",
    "    split_renames= {})\n",
    "\n",
    "wthr['Datetime_Temp'] = wthr['Year']+'-'+wthr['Month']+'-'+wthr['Day']+' '+wthr['Time']\n",
    "\n",
    "# convert types\n",
    "err_list = find_unconvertable_datetimes(df_col=wthr['Datetime_Temp'], pattern='%Y-%m-%d %H:%M', index=False)\n",
    "if err_list != []:\n",
    "    print(err_list)\n",
    "else:\n",
    "    wthr.Datetime_Temp = pd.to_datetime(pd.Series(wthr.Datetime_Temp), errors='coerce')\n",
    "    wthr.Datetime = wthr.Datetime_Temp\n",
    "    wthr = wthr.drop(columns= 'Datetime_Temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee7200",
   "metadata": {},
   "source": [
    "### Simple Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed990b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to string\n",
    "wthr = cols_astype_string(\n",
    "    df = wthr, \n",
    "    col_list = [key for key in wthr_col_dtypes.keys() if wthr_col_dtypes[key] == 'string'])\n",
    "\n",
    "wthr.Year = year_string\n",
    "wthr.Year = wthr.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a16c36",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fd6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = check_df_dtype_expectations(df = wthr, dtype_dct = wthr_col_dtypes)\n",
    "\n",
    "if sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n",
    "    pass\n",
    "else:\n",
    "    print(checkpoint.loc[~checkpoint.Pass, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958033e",
   "metadata": {},
   "source": [
    "## Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf1a42",
   "metadata": {},
   "source": [
    "### Date_Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt = relocate_to_Imputation_Notes(df = mgmt, col = 'Date_Datetime', val_list= ['Before Planting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt = sanitize_col(\n",
    "    df = mgmt, \n",
    "    col = 'Date_Datetime', \n",
    "    simple_renames= {}, \n",
    "    split_renames= {'6/24/21 for all but plots in pass 2; 7/5/21 for pass 2' : [\n",
    "                        '6/24/21 for all but plots in pass 2', '7/5/21 for pass 2']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make corrections too one-off to fix with a funciton. \n",
    "mask = ((mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2') & (mgmt.Pass != 2.))\n",
    "mgmt.loc[mask, 'Date_Datetime'] = '6/24/21'\n",
    "# since we split without specifiying pass we need to remove any rows that still have the search string.\n",
    "# and overwrite the df\n",
    "mask = (mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2')\n",
    "mgmt = mgmt.loc[~mask, :].copy()\n",
    "\n",
    "mask = ((mgmt.Date_Datetime == '7/5/21 for pass 2') & (mgmt.Pass == 2.))\n",
    "mgmt.loc[mask, 'Date_Datetime'] = '7/5/21'\n",
    "mask = (mgmt.Date_Datetime == '7/5/21 for pass 2')\n",
    "mgmt = mgmt.loc[~mask, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b612643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert types\n",
    "err_list = find_unconvertable_datetimes(df_col=mgmt.Date_Datetime, pattern='%m/%d/%y', index=False)\n",
    "if err_list != []:\n",
    "    print(err_list)\n",
    "else:\n",
    "    mgmt.Date_Datetime = pd.to_datetime(pd.Series(mgmt.Date_Datetime), format = '%m/%d/%y', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e0f42",
   "metadata": {},
   "source": [
    "### Amount_Per_Acre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt.loc[find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = True), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt = sanitize_col(\n",
    "    df = mgmt, \n",
    "    col = 'Amount_Per_Acre', \n",
    "    simple_renames= {'170 lb (actual N)': '170 (N)'}, \n",
    "    split_renames= {'51.75, 40.7, 111.7 (N,P,K)': ['51.75 (N)', '40.7 (P)', '111.7 (K)'],\n",
    "                    '31-150-138': ['31 (N)', '150 (P)', '138 (K)'],\n",
    "                    '16 (N), 41 (P)': ['16 (N)', '41 (P)']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ead32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt = safe_create_col(mgmt, \"Ingredient\")\n",
    "mask = mgmt.Ingredient.isna()\n",
    "mgmt.loc[mask, 'Ingredient'] = mgmt.loc[mask, 'Product']\n",
    "\n",
    "# assume each string is formated as 'val (key)'. `sanitize_col` should be used to enforce this.\n",
    "for e in ['150 (P)', '36.6 (N)', '138 (K)', '111.7 (K)', '41 (P)', '16 (N)', '170 (N)', '35.7 (N)', '51.75 (N)', '31 (N)', '40.7 (P)']:\n",
    "    val = re.findall('^\\d+[.]*\\d*', e)[0]\n",
    "    key = re.findall('\\(.+\\)',      e)[0].replace('(', '').replace(')', '')\n",
    "    \n",
    "    mask = (mgmt['Amount_Per_Acre'] == e)\n",
    "    mgmt.loc[mask, 'Ingredient'] = key\n",
    "    mgmt.loc[mask, 'Amount_Per_Acre'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc11303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert types\n",
    "err_list = find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = False)\n",
    "if err_list != []:\n",
    "    print(err_list)\n",
    "else:\n",
    "    mgmt.Amount_Per_Acre = pd.to_numeric(mgmt.Amount_Per_Acre, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57973e80",
   "metadata": {},
   "source": [
    "### Ingredient\n",
    "This is to be the cleaned up version of the \"Product\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66140777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(mgmt.loc[:, 'Ingredient'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c3e99",
   "metadata": {},
   "source": [
    "### Simple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb60d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bool\n",
    "mgmt['mgmt'] = mgmt['mgmt'].astype('bool')\n",
    "\n",
    "# to string\n",
    "for e in [ee for ee in ['Application', 'Product', 'Ingredient', 'Unit', 'Imputation_Notes'] if ee in mgmt.columns]:\n",
    "    mgmt[e] = mgmt[e].astype('string')\n",
    "    \n",
    "\n",
    "mgmt.Year = year_string\n",
    "mgmt.Year = mgmt.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2160e",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1578ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df_dtype_expectations(df = mgmt, dtype_dct = mgmt_col_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109ee8b",
   "metadata": {},
   "source": [
    "# Publish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_out_pkl(obj, path = './temp.pickle'):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_out_pkl(obj = sval, path = './data/interim/'+year_string+'sval.pickle')\n",
    "write_out_pkl(obj = wthr, path = './data/interim/'+year_string+'wthr.pickle')\n",
    "write_out_pkl(obj = mgmt, path = './data/interim/'+year_string+'mgmt.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
