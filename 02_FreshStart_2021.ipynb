{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d1c95f",
   "metadata": {},
   "source": [
    "# Process Data from 2021 into a consistent format. \n",
    "\n",
    "> This notebook is the first template for cleaning g2f data. Earlier years will be brought up to the standard naming and format which is based off of 2021's data release. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports ----\n",
    "import re\n",
    "import numpy as np # for np.nan\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "# import os   # for write_log, delete_logs\n",
    "import glob # for delete_all_logs\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import json # for saving a dict to txt with json.dumps\n",
    "\n",
    "import pickle\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2fd.internal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b420d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Settings ----\n",
    "# Helper functions,remove if no longer needed\n",
    "def prlst(lst): \n",
    "    \"This is just a helper function to ease formating lists of strings with each entryon a different line.\"\n",
    "    print('[')\n",
    "    for e in lst:\n",
    "        if e != lst[-1]:\n",
    "            print(\"'\"+e+\"', \")\n",
    "        else:\n",
    "            print(\"'\"+e+\"'\")\n",
    "    print(']')\n",
    "    \n",
    "def prlst2dct(lst): \n",
    "    \"This is just a helper function to ease formating lists of strings with each entryon a different line.\"\n",
    "    print('{')\n",
    "    for e in lst:\n",
    "        if e != lst[-1]:\n",
    "            print(\"'\"+e+\"': 'XXXXXXX', \")\n",
    "        else:\n",
    "            print(\"'\"+e+\"': 'XXXXXXX'\")\n",
    "    print('}')\n",
    "    \n",
    "def dash80(dash = '-'): return ''.join([dash for e in range(80)])\n",
    "\n",
    "\n",
    "# prlst([])\n",
    "# prlst2dct([])\n",
    "# dash80()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021\n",
    "year_string = '2021'\n",
    "\n",
    "meta_path = './data/raw/GenomesToFields_G2F_data_2021/z._2021_supplemental_info/g2f_2021_field_metadata.csv'\n",
    "phno_path = './data/raw/GenomesToFields_G2F_data_2021/a._2021_phenotypic_data/g2f_2021_phenotypic_clean_data.csv' # also contains 'g2f_2021_phenotypic_raw_data.csv' \n",
    "wthr_path = './data/raw/GenomesToFields_G2F_data_2021/b._2021_weather_data/g2f_2021_weather_cleaned.csv'\n",
    "soil_path = './data/raw/GenomesToFields_G2F_data_2021/c._2021_soil_data/g2f_2021_soil_data.csv'\n",
    "mgmt_path = './data/raw/GenomesToFields_G2F_data_2021/z._2021_supplemental_info/g2f_2021_agronomic_information.csv'\n",
    "\n",
    "\n",
    "meta = pd.read_csv(meta_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "phno = pd.read_csv(phno_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "wthr = pd.read_csv(wthr_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "soil = pd.read_csv(soil_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "mgmt = pd.read_csv(mgmt_path, encoding = \"ISO-8859-1\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dfc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Helper functions to match up the same data in different dataframes under different columns names\n",
    "\n",
    "# make a dictionary of column name : unique values\n",
    "def mk_uniq_val_dict(df1):\n",
    "    uniq_val_dict = {}\n",
    "    for df1_col in list(df1.columns):\n",
    "        uniq_val_dict.update({df1_col:set(df1[df1_col])})\n",
    "    return(uniq_val_dict)\n",
    "\n",
    "def pr_eq_list(lst1, lst2): return len(set(lst1) & set(lst2)) / len(set(lst1) | set(lst2))\n",
    "\n",
    "# take two dictionaries from `mk_uniq_val_dict` and a key to match, return a df of the n closest matches (based on set of column values)\n",
    "def mk_df_of_n_similar_cols(dct1, key_to_match, dct2, n = 1):\n",
    "    # key_to_match = 'Experiment_Code'        \n",
    "    lst = dct1[key_to_match]\n",
    "    # dct2 = dict1\n",
    "\n",
    "    keys = dct2.keys()\n",
    "    similarities = [pr_eq_list(lst, dct2[e]) for e in dct2.keys()]\n",
    "\n",
    "    similarityDf = pd.DataFrame(\n",
    "        zip(dct2.keys(), similarities), \n",
    "        columns=['Column2', 'PrMatch']\n",
    "    ).sort_values('PrMatch', ascending=False)\n",
    "    \n",
    "    output = similarityDf.head(n)\n",
    "    output.insert(0, column = 'Column1', value = key_to_match)\n",
    "    return(output)\n",
    "\n",
    "# take two dictionaries from `mk_uniq_val_dict` and find the best match for each key in dict1 in dict2. \n",
    "def mk_df_of_most_similar_cols(dict1, dict2):\n",
    "    return( pd.concat( [mk_df_of_n_similar_cols(dct1 = dict1, key_to_match = key, dct2 = dict2, n = 1) for key in dict1.keys() ]) )\n",
    "\n",
    "# Combine `mk_df_of_most_similar_cols` and `mk_uniq_val_dict` to find the closest matches between two columns in different dfs.\n",
    "# Should be helpful for finding columns with the same data but different names (e.g. Experiment vs Experiment_Code)\n",
    "def match_df_cols(df1, df2):\n",
    "    df1_cols = list(df1.columns)\n",
    "    df2_cols = list(df2.columns)\n",
    "\n",
    "    dict1 = mk_uniq_val_dict(df1)\n",
    "    dict2 = mk_uniq_val_dict(df2)\n",
    "    \n",
    "    return(mk_df_of_most_similar_cols(dict1, dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_df_cols(df1 = meta, df2 = soil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd0d63",
   "metadata": {},
   "source": [
    "## mk_name_dict -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba20000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def mk_name_dict(name # table meta, phno, soil, wthr, or mgmt\n",
    "                ):\n",
    "    'Easily share dictionaries for renaming columns across scripts.'  \n",
    "\n",
    "    meta_name_dict = {\n",
    "                              'Experiment_Code': 'Experiment_Code', # Unchanged \n",
    "                                    'Treatment': 'Treatment',       # Unchanged \n",
    "                                         'City': 'City',            # Unchanged \n",
    "                                         'Farm': 'Farm',            # Unchanged \n",
    "                                        'Field': 'Field',           # Unchanged \n",
    " 'Trial_ID (Assigned by collaborator for internal reference)': 'Trial_ID', \n",
    " 'Soil_Taxonomic_ID and horizon description, if known': 'Soil_Taxonomic_ID', \n",
    " 'Weather_Station_Serial_Number (Last four digits, e.g. m2700s#####)': 'Weather_Station_Serial_Number', \n",
    " 'Weather_Station_Latitude (in decimal numbers NOT DMS)': 'Weather_Station_Latitude_Unit_Decimal', \n",
    " 'Weather_Station_Longitude (in decimal numbers NOT DMS)': 'Weather_Station_Longitude_Unit_Decimal', \n",
    "                  'Date_weather_station_placed': 'Weather_Station_Placed_Unit_Datetime', \n",
    "                 'Date_weather_station_removed': 'Weather_Station_Removed_Unit_Datetime', \n",
    "       'In-field weather station serial number': 'Weather_Station_In_Field_Serial_Number', \n",
    " 'In-field_weather_station_latitude (in decimal)': 'Weather_Station_In_Field_Latitude_Unit_Decimal', \n",
    " 'In-field_weather_station_longitude (in decimal)': 'Weather_Station_In_Field_Longitude_Unit_Decimal', \n",
    "                                'Previous_Crop': 'Previous_Crop',   # Unchanged \n",
    "                  'Pre-plant_tillage_method(s)': 'Pre_Plant_Tillage', \n",
    "                  'In-season_tillage_method(s)': 'Post_Plant_Tillage', \n",
    " 'Plot_length (center-alley to center-alley in feet)': 'Plot_Length_Unit_Feet', \n",
    "                     'Alley_length (in inches)': 'Alley_Length_Unit_Inches', \n",
    "                      'Row_spacing (in inches)': 'Row_Spacing_Unit_Inches', \n",
    " 'Type_of_planter (fluted cone; belt cone; air planter)': 'Planter_Type', \n",
    " 'Number_kernels_planted_per_plot (>200 seed/pack for cone planters)': 'Kernels_Per_Plot', \n",
    "                  'System_Determining_Moisture': 'System_Determining_Moisture', # Unchanged \n",
    "                  'Pounds_Needed_Soil_Moisture': 'Pounds_Needed_Soil_Moisture', # Unchanged \n",
    "     'Latitude_of_Field_Corner_#1 (lower left)': 'Field_Latitude_BL', \n",
    "    'Longitude_of_Field_Corner_#1 (lower left)': 'Field_Longitude_BL', \n",
    "    'Latitude_of_Field_Corner_#2 (lower right)': 'Field_Latitude_BR', \n",
    "   'Longitude_of_Field_Corner_#2 (lower right)': 'Field_Longitude_BR', \n",
    "    'Latitude_of_Field_Corner_#3 (upper right)': 'Field_Latitude_TR', \n",
    "   'Longitude_of_Field_Corner_#3 (upper right)': 'Field_Longitude_TR', \n",
    "     'Latitude_of_Field_Corner_#4 (upper left)': 'Field_Latitude_TL', \n",
    "    'Longitude_of_Field_Corner_#4 (upper left)': 'Field_Longitude_TL', \n",
    "                      'Cardinal_Heading_Pass_1': 'Cardinal_Heading', \n",
    "                      'Local_Check_#1_Pedigree': 'Local_Check_Pedigree_1', \n",
    "                        'Local_Check_#1_Source': 'Local_Check_Source_1', \n",
    "                      'Local_Check_#2_Pedigree': 'Local_Check_Pedigree_2', \n",
    "                        'Local_Check_#2_Source': 'Local_Check_Source_2', \n",
    "                      'Local_Check_#3_Pedigree': 'Local_Check_Pedigree_3', \n",
    "                        'Local_Check_#3_Source': 'Local_Check_Source_3', \n",
    "                      'Local_Check_#4_Pedigree': 'Local_Check_Pedigree_4', \n",
    "                        'Local_Check_#4_Source': 'Local_Check_Source_4', \n",
    "                      'Local_Check_#5_Pedigree': 'Local_Check_Pedigree_5', \n",
    "                        'Local_Check_#5_Source': 'Local_Check_Source_5', \n",
    "                             'Issue/comment_#1': 'Comment_1', \n",
    "                             'Issue/comment_#2': 'Comment_2', \n",
    "                             'Issue/comment_#3': 'Comment_3', \n",
    "                             'Issue/comment_#4': 'Comment_4', \n",
    "                             'Issue/comment_#5': 'Comment_5', \n",
    "                             'Issue/comment_#6': 'Comment_6', \n",
    "                             'Issue/comment_#7': 'Comment_7', \n",
    "                             'Issue/comment_#8': 'Comment_8', \n",
    "                             'Issue/comment_#9': 'Comment_9', \n",
    "                            'Issue/comment_#10': 'Comment_70',\n",
    "                                               # 2018\n",
    " 'Weather_Station_Serial_Number (Last four digits, e.g.\\xa0m2700s#####)': 'Weather_Station_Serial_Number',\n",
    "                                               # 2015\n",
    "                                   'Experiment': 'Experiment_Code', \n",
    "                                         'Type': 'Type', \n",
    "                                        'WS_SN': 'Weather_Station_In_Field_Serial_Number', \n",
    "                                       'WS Lat': 'Weather_Station_In_Field_Latitude_Unit_Decimal', \n",
    "                                       'WS Lon': 'Weather_Station_In_Field_Longitude_Unit_Decimal', \n",
    "                                       'DateIn': 'Weather_Station_Placed_Unit_Datetime', \n",
    "                                      'DateOut': 'Weather_Station_Removed_Unit_Datetime', \n",
    "                                'Previous Crop': 'Previous_Crop', \n",
    "                                      'Tillage': 'Tillage', \n",
    "                                      'PlotLen': 'Plot_Length_Unit_Feet', \n",
    "                                     'AlleyLen': 'Alley_Length_Unit_Inches', \n",
    "                                        'RowSp': 'Row_Spacing_Unit_Inches', \n",
    "                                  'PlanterType': 'Planter_Type', \n",
    "                               'KernelsPerPlot': 'Kernels_Per_Plot', \n",
    "                               'Moisture Meter': 'System_Determining_Moisture', \n",
    "                                'LBS for  test': 'Test_Weight_Unit_lbs', \n",
    "                                  'corner1 lat': 'Field_Latitude_BL', \n",
    "                                  'corner1 lon': 'Field_Longitude_BL', \n",
    "                                  'corner 2lat': 'Field_Latitude_BR', \n",
    "                                  'corner2 lon': 'Field_Longitude_BR', \n",
    "                                  'corner3 lat': 'Field_Latitude_TR', \n",
    "                                  'corner3 lon': 'Field_Longitude_TR', \n",
    "                                  'corner4 lat': 'Field_Latitude_TL', \n",
    "                                  'corner4 lon': 'Field_Longitude_TL', \n",
    "                                     'Cardinal': 'Cardinal_Heading', \n",
    "                        'date of soil sampling': 'Recieved_Date_Unit_Datetime', \n",
    "                                'preplant herb': 'Pre_Plant_Herbicide', \n",
    "                               'postplant herb': 'Post_Plant_Herbicide', \n",
    "                                      'total N': 'N_Unit_lbs_per_A', \n",
    "                                      'total P': 'P_Unit_lbs_per_A', \n",
    "                                      'total K': 'K_Unit_lbs_per_A', \n",
    "                                 'fert dates 1': 'Fertilizer_Application_Datetime_1', \n",
    "                                 'fert dates 2': 'Fertilizer_Application_Datetime_2', \n",
    "                                 'fert dates 3': 'Fertilizer_Application_Datetime_3', \n",
    "                                 'fert dates 4': 'Fertilizer_Application_Datetime_4', \n",
    "                                 'fert dates 5': 'Fertilizer_Application_Datetime_5', \n",
    "                                 'fert dates 6': 'Fertilizer_Application_Datetime_6', \n",
    "                                 'fert dates 7': 'Fertilizer_Application_Datetime_7', \n",
    "                                 'fert dates 8': 'Fertilizer_Application_Datetime_8', \n",
    "                                 'Type of Fert': 'Fertilizer_Product', \n",
    "                                  'Insecticide': 'Insecticide',\n",
    "                                               # 2014  \n",
    "                                'Location name': 'Experiment_Code', \n",
    "                                         'long': 'Field_Longitude_BL', \n",
    "                                          'lat': 'Field_Latitude_BL', \n",
    "       'Plot length (center to center in feet)': 'Plot_Length_Unit_Feet', \n",
    "                        'Alley length (inches)': 'Alley_Length_Unit_Inches', \n",
    "                         'Row spacing (inches)': 'Row_Spacing_Unit_Inches', \n",
    "                       'Number kernels planted': 'Kernels_Per_Plot', \n",
    "                                 'Planter type': 'Planter_Type', \n",
    "                                'Previous crop': 'Previous_Crop', \n",
    "                         'Pre-plant herbicides': 'Pre_Plant_Herbicide', \n",
    "                        'Post-plant herbicides': 'Post_Plant_Herbicide', \n",
    "                               'Tillage method': 'Tillage', \n",
    "                               'Soil test type': 'Soil_Test', \n",
    "                                 'Soil texture': 'Texture', \n",
    "                                      'Soil pH': 'Soil_1_to_1_Unit_pH', \n",
    "                               'Total nitrogen': 'N_Unit_lbs_per_A', \n",
    "                             'Total phosphorus': 'P_Unit_lbs_per_A', \n",
    "                              'Total potassium': 'K_Unit_lbs_per_A', \n",
    "                'Nutrient application schedule': 'Nutrient_Schedule', \n",
    "                                   'Irrigated?': 'Irrigation_Applied', \n",
    "          'Weather station includes irrigation': 'Weather_Station_Documents_Irrigation', \n",
    "                         'Fertigation schedule': 'Fertigation_Schedule', \n",
    "                          'Irrigation schedule': 'Irrigation_Schedule', \n",
    "                                  'Local check': 'Local_Check', \n",
    "                                 'Harvest date': 'Harvested_Unit_Datetime', \n",
    "                                'Planting date': 'Planted_Unit_Datetime', \n",
    "                                  'Inbred reps': 'Inbred_Reps', \n",
    "                                 'Inbred plots': 'Inbred_Plots', \n",
    "                                'Collaborators': 'Collaborator', \n",
    "                               'Data file name': 'State', \n",
    "                                       'traits': 'Traits_Measured', \n",
    "                                       'folder': 'State', \n",
    "                           'Metadata file name': 'Metadata_File', \n",
    "                          'additional metadata': 'Additional_Metadata', \n",
    "                'Weather Station serial number': 'Weather_Station_In_Field_Serial_Number', \n",
    "                               'Weather-folder': 'Weather_Directory'\n",
    "    }\n",
    "    \n",
    "    phno_name_dict = {\n",
    "                              'Year': 'Year', \n",
    "                    'Field-Location': 'Experiment_Code', \n",
    "                             'State': 'State',      # Unchanged \n",
    "                              'City': 'City',       # Unchanged \n",
    " 'Plot length (center-center in feet)': 'Plot_Length_Unit_Feet', \n",
    "                   'Plot area (ft2)': 'Plot_Area_Unit_Feet2', \n",
    "          'Alley length (in inches)': 'Alley_Length_Unit_Inches', \n",
    "           'Row spacing (in inches)': 'Row_Spacing_Unit_Inches', \n",
    "                     'Rows per plot': 'Rows_Per_Plot', \n",
    "                   '# Seed per plot': 'Seeds_Per_Plot', \n",
    "                        'Experiment': 'Experiment', # Unchanged \n",
    "                            'Source': 'Source',     # Unchanged \n",
    "                          'Pedigree': 'Pedigree',   # Unchanged \n",
    "                            'Family': 'Family',     # Unchanged \n",
    "                            'Tester': 'Tester',     # Unchanged \n",
    "                         'Replicate': 'Replicate',  # Unchanged \n",
    "                             'Block': 'Block',      # Unchanged \n",
    "                              'Plot': 'Plot',       # Unchanged \n",
    "                           'Plot_ID': 'Plot_ID',    # Unchanged \n",
    "                             'Range': 'Range',      # Unchanged \n",
    "                              'Pass': 'Pass',       # Unchanged \n",
    "      'Date Plot Planted [MM/DD/YY]': 'Planted_Unit_Datetime', \n",
    "    'Date Plot Harvested [MM/DD/YY]': 'Harvested_Unit_Datetime', \n",
    "               'Anthesis [MM/DD/YY]': 'Anthesis_Unit_Datetime', \n",
    "                'Silking [MM/DD/YY]': 'Silking_Unit_Datetime', \n",
    "                   'Anthesis [days]': 'Anthesis_Unit_Days', \n",
    "                    'Silking [days]': 'Silking_Unit_Days', \n",
    "                 'Plant Height [cm]': 'Plant_Height_Unit_cm', \n",
    "                   'Ear Height [cm]': 'Ear_Height_Unit_cm', \n",
    "         'Stand Count [# of plants]': 'Stand_Count_Unit_Number', \n",
    "        'Root Lodging [# of plants]': 'Root_Lodging_Unit_Number', \n",
    "       'Stalk Lodging [# of plants]': 'Stalk_Lodging_Unit_Number', \n",
    "                'Grain Moisture [%]': 'Grain_Moisture_Unit_Percent', \n",
    "                 'Test Weight [lbs]': 'Test_Weight_Unit_lbs', \n",
    "                 'Plot Weight [lbs]': 'Plot_Weight_Unit_lbs', \n",
    "                'Grain Yield (bu/A)': 'Grain_Yield_Unit_bu_Per_A', \n",
    "  \"Plot Discarded [enter 'yes' or blank]\": 'Discarded', \n",
    "                          'Comments': 'Phenotype_Comments', \n",
    "                            'Filler': 'Filler',     # Unchanged \n",
    "                'Snap [# of plants]': 'Snap_Unit_Number',\n",
    "                                    # 2019\n",
    "                   'Kernels/Packet' : 'Kernels_Per_Packet',\n",
    " \"Filler [enter 'filler' or blank]\" : 'Filler',\n",
    "                    'Possible subs' : 'Possible_Subs',\n",
    "                   'Confirmed subs' : 'Confirmed_Subs',\n",
    "  'Single Plant Biomass in July(g)' : 'Biomass_July_Unit_g',\n",
    " 'Single Plant Biomass in August(g)' : 'Biomass_Aug_Unit_g',\n",
    "       'RootPullingForce(kgf)_July' : 'Root_Pulling_Force_July_Unit_kgf',\n",
    "     'RootPullingForce(kgf)_August' : 'Root_Pulling_Force_Aug_Unit_kgf',\n",
    "                                    # 2018\n",
    "                             'RecId': 'Drop_Record_Index', \n",
    "                      'Tester/Group': 'Tester', \n",
    "             'Local Check (Yes, No)': 'Local_Check', \n",
    "                 'Plot Length Field': 'Plot_Length_Unit_Feet', \n",
    "                      'Alley Length': 'Alley_Length_Unit_Inches', \n",
    "                       'Row Spacing': 'Row_Spacing_Unit_Inches', \n",
    "                         'Plot Area': 'Plot_Area_Unit_Feet2', \n",
    "                         'Rows/Plot': 'Rows_Per_Plot', \n",
    "                       'Packet/Plot': 'Packets_Per_Plot', \n",
    "                            '# Seed': 'Seeds_Per_Plot', \n",
    "                      'Date Planted': 'Planted_Unit_Datetime', \n",
    "                    'Date Harvested': 'Harvested_Unit_Datetime', \n",
    "                   'Anthesis [date]': 'Anthesis_Unit_Datetime', \n",
    "                    'Silking [date]': 'Silking_Unit_Datetime', \n",
    "                 'Pollen DAP [days]': 'Anthesis_Unit_Days', \n",
    "                   'Silk DAP [days]': 'Silking_Unit_Days', \n",
    "              'Stand Count [plants]': 'Stand_Count_Unit_Number', \n",
    "                         'Stand [%]': 'Stand_Count_Unit_Percent', \n",
    "             'Root Lodging [plants]': 'Root_Lodging_Unit_Number', \n",
    "            'Stalk Lodging [plants]': 'Stalk_Lodging_Unit_Number', \n",
    "              'Test Weight [lbs/bu]': 'Test_Weight_Unit_lbs', \n",
    "                'Grain Yield [bu/A]': 'Grain_Yield_Unit_bu_Per_A', \n",
    " 'Plot Discarded [enter \"Yes\" or \"blank\"]': 'Discarded', \n",
    " 'Filler [enter \"filler\" or \"blank\"]': 'Filler', \n",
    " '[add additional measurements here]': 'Additional_Metics',\n",
    "                                    # 2017\n",
    "                           'ï»¿Year': 'Year', \n",
    "      'LOCAL_CHECK (Yes, No[Blank])': 'Local_Check', \n",
    " 'Plot Discarded [enter \"yes\" or \"blank\"]': 'Discarded',\n",
    "                                    # 2015\n",
    " 'Plot Length Field (center to center in feet)': 'Plot_Length_Unit_Feet', \n",
    "               'Alley Length (feet)': 'Alley_Length_Unit_Feet', \n",
    "              'Row Spacing (inches)': 'Row_Spacing_Unit_Inches'\n",
    "                                    # 2014\n",
    "    }\n",
    "\n",
    "    soil_name_dict = {\n",
    "                        'Grower': 'Grower',  # Unchanged \n",
    "                      'Location': 'Experiment_Code', \n",
    "                 'Date Received': 'Recieved_Date_Unit_Datetime', \n",
    "                 'Date Reported': 'Processed_Date_Unit_Datetime', \n",
    "                       'E Depth': 'Depth_Unit_UNK', \n",
    "                   '1:1 Soil pH': 'Soil_1_to_1_Unit_pH', \n",
    "                'WDRF Buffer pH': 'WDRF_Buffer_Unit_pH', \n",
    "           '1:1 S Salts mmho/cm': 'Soluable_Salts_Unit_mmho_Per_cm', \n",
    "                    'Texture No': 'Texture_Number', \n",
    "          'Organic Matter LOI %': 'Organic_Matter_Unit_Percent', \n",
    "               'Nitrate-N ppm N': 'Nitrates_Unit_ppm', \n",
    "                       'lbs N/A': 'N_per_Acre_Unit_lbs', \n",
    "               'Potassium ppm K': 'K_Unit_ppm', \n",
    "               'Sulfate-S ppm S': 'Sulfate_Unit_ppm', \n",
    "                'Calcium ppm Ca': 'Ca_Unit_ppm', \n",
    "              'Magnesium ppm Mg': 'Mg_Unit_ppm', \n",
    "                 'Sodium ppm Na': 'Na_Unit_ppm', \n",
    "    'CEC/Sum of Cations me/100g': 'Cation_Exchange_Capacity', \n",
    "                        '%H Sat': 'H_Sat_Unit_Percent', \n",
    "                        '%K Sat': 'K_Sat_Unit_Percent', \n",
    "                       '%Ca Sat': 'Ca_Sat_Unit_Percent', \n",
    "                       '%Mg Sat': 'Mg_Sat_Unit_Percent', \n",
    "                       '%Na Sat': 'Na_Sat_Unit_Percent', \n",
    "           'Mehlich P-III ppm P': 'Mehlich_PIII_P_Unit_ppm', \n",
    "                        '% Sand': 'Sand_Unit_Percent', \n",
    "                        '% Silt': 'Silt_Unit_Percent', \n",
    "                        '% Clay': 'Clay_Unit_Percent', \n",
    "                       'Texture': 'Texture', # Unchanged \n",
    "                      'Comments': 'Soil_Comments',\n",
    "                                # 2018\n",
    "                      'Field ID': 'Experiment_Code', \n",
    "                 'Date Recieved': 'Recieved_Date_Unit_Datetime',\n",
    "                                # 2016\n",
    "                   'Sample Type': 'Sample_Type', \n",
    "                   'Zinc ppm Zn': 'Zn_Unit_ppm', \n",
    "                   'Iron ppm Fe': 'Fe_Unit_ppm', \n",
    "              'Manganese ppm Mn': 'Mn_Unit_ppm', \n",
    "                 'Copper ppm Cu': 'Cu_Unit_ppm', \n",
    "                   'Boron ppm B': 'B_Unit_ppm',\n",
    "                                # 2015\n",
    "                         'LabID': 'Lab_ID', \n",
    "                     'LabSmplID': 'Lab_ID_Number', \n",
    "                      'SmplDate': 'Recieved_Date_Unit_Datetime', \n",
    "                      'CoopName': 'Cooperator', \n",
    "                    'Experiment': 'Experiment_Code', \n",
    "                     'PlowDepth': 'Depth_Unit_UNK', \n",
    "                            'PH': 'Soil_1_to_1_Unit_pH', \n",
    "                           'BpH': 'Sikora_Buffer_Unit_pH', \n",
    "                            'OM': 'Organic_Matter_Unit_Percent', \n",
    "                             'P': 'P_Unit_ppm', \n",
    "                             'K': 'K_Unit_ppm'  \n",
    "                                # 2014\n",
    "    }\n",
    "\n",
    "    wthr_name_dict = {\n",
    "              'Field Location': 'Experiment_Code', \n",
    "                  'Station ID': 'Weather_Station_ID', \n",
    "                 'NWS Network': 'NWS_Network', \n",
    "                 'NWS Station': 'NWS_Station', \n",
    "                    'Date_key': 'Datetime', \n",
    "                       'Month': 'Month', # Unchanged \n",
    "                         'Day': 'Day',   # Unchanged \n",
    "                        'Year': 'Year',  # Unchanged \n",
    "                        'Time': 'Time',  # Unchanged \n",
    "             'Temperature [C]': 'Temperature_Unit_C', \n",
    "               'Dew Point [C]': 'Dew_Point_Unit_C', \n",
    "       'Relative Humidity [%]': 'Relative_Humidity_Unit_Percent', \n",
    "      'Solar Radiation [W/m2]': 'Solar_Radiation_Unit_W_per_m2', \n",
    "               'Rainfall [mm]': 'Rainfall_Unit_mm', \n",
    "            'Wind Speed [m/s]': 'Wind_Speed_Unit_m_per_s', \n",
    "    'Wind Direction [degrees]': 'Wind_Direction_Unit_Degrees', \n",
    "             'Wind Gust [m/s]': 'Wind_Gust_Unit_m_per_s', \n",
    "        'Soil Temperature [C]': 'Soil_Temperature_Unit_C', \n",
    "        'Soil Moisture [%VWC]': 'Soil_Moisture_Unit_Percent_VWC', \n",
    "             'Soil EC [mS/cm]': 'Soil_EC_Unit_mS_per_cm', \n",
    "           'UV Light [uM/m2s]': 'UV_Light_Unit_uM_per_m2s', \n",
    "                'PAR [uM/m2s]': 'PAR_Unit_uM_per_m2s',\n",
    "                              # 2020\n",
    "                   'CO2 [ppm]': 'CO2_Unit_ppm', \n",
    "                              # 2018\n",
    "            'ï»¿Record Number': 'Drop_Record_Index', \n",
    "               'UVL (uM/m^2s)': 'UV_Light_Unit_uM_per_m2s', \n",
    "        'Photoperiod [ hours]': 'Photoperiod_Unit_Hours', \n",
    "              'Column Altered': 'Data_Cleaned', \n",
    "        'Altered Column Names': 'Fields_Cleaned', \n",
    "             'Cleaning Method': 'Cleaning_Method', \n",
    "                     'Comment': 'Weather_Comments',\n",
    "                              # 2017\n",
    "         'Photoperiod [hours]': 'Photoperiod_Unit_Hours',\n",
    "                              # 2016\n",
    "                 'Time[Local]': 'Time', \n",
    "                'Rainfall[mm]': 'Rainfall_Unit_mm', \n",
    "          'Photoperiod[hours]': 'Photoperiod_Unit_Hours',\n",
    "                              # 2015\n",
    "               'Record Number': 'Drop_Record_Index', \n",
    "               'Experiment(s)': 'Experiment_Code', \n",
    "                 'Day of Year': 'Day_Of_Year', \n",
    "                'Time [Local]': 'Time', \n",
    "              'Datetime [UTC]': 'Datetime', \n",
    "           'Soil Moisture [%]': 'Soil_Moisture_Percent',\n",
    "                              # 2014\n",
    "                 'Day [Local]': 'Day', \n",
    "               'Month [Local]': 'Month', \n",
    "                'Year [Local]': 'Year', \n",
    "         'Day of Year [Local]': 'Day_Of_Year'\n",
    "    }\n",
    "\n",
    "    mgmt_name_dict = {\n",
    "                       'Location': 'Experiment_Code', \n",
    "       'Application_or_treatment': 'Application', \n",
    "    'Product_or_nutrient_applied': 'Product', \n",
    "            'Date_of_application': 'Date_Datetime', \n",
    "              'Quantity_per_acre': 'Amount_Per_Acre', \n",
    "               'Application_unit': 'Unit',\n",
    "                                 # 2016\n",
    "                   'Record Order': 'Drop_Record_Index', \n",
    "                'Experiment Code': 'Experiment_Code', \n",
    "                      'Record ID': 'Drop_Record_Index2', \n",
    "       'Application or Treatment': 'Application', \n",
    "       'Product/Nutrient Applied': 'Product', \n",
    "            'Date of Application': 'Date_Datetime', \n",
    "              'Quantity per acre': 'Amount_Per_Acre', \n",
    " 'Application unit\\n(lbs, in, oz per acre)': 'Unit',\n",
    "                                 # 2015\n",
    " 'Irrigation/Fertigation (yes/no)': 'Irrigation_Applied', \n",
    " 'Weather station documents irrigation? (yes/no)': 'Weather_Station_Documents_Irrigation', \n",
    "              'Nutrients Applied': 'Nutrients_Applied', \n",
    "                          'Notes': 'Management_Comments'\n",
    "                                 # 2014\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if name == 'meta':\n",
    "        return meta_name_dict\n",
    "    elif name == 'phno':\n",
    "        return phno_name_dict\n",
    "    elif name == 'soil':\n",
    "        return soil_name_dict\n",
    "    elif name == 'wthr':\n",
    "        return wthr_name_dict\n",
    "    elif name == 'mgmt':\n",
    "        return mgmt_name_dict\n",
    "    else:\n",
    "        print('Requested name is not defined')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dicts for column renaming\n",
    "meta_name_dict = mk_name_dict(name = 'meta')\n",
    "phno_name_dict = mk_name_dict(name = 'phno')\n",
    "soil_name_dict = mk_name_dict(name = 'soil')\n",
    "wthr_name_dict = mk_name_dict(name = 'wthr')\n",
    "mgmt_name_dict = mk_name_dict(name = 'mgmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_df_cols(df1 = meta, df2 = soil)\n",
    "# set(meta.Experiment_Code), set(soil.Location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ab577",
   "metadata": {},
   "source": [
    "# Rename\n",
    "**Naming rules:**\n",
    "- One dict for each input df\n",
    "- Comment out anything that shouldn't be changed\n",
    "- Upper_Upper_Unit_\\$unit\n",
    "- Upper_$number\n",
    "- No special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# check if there are columns that need to be aded to the naming dictionaries:\n",
    "def find_unrecognized_columns(df, dct): \n",
    "    keys_and_vals = list(dct.keys())\n",
    "    keys_and_vals.extend(list(dct.values()))\n",
    "    keys_and_vals\n",
    "    return([e for e in df.columns if e not in keys_and_vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d26449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [], [], [], [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(find_unrecognized_columns(df = meta, dct = meta_name_dict),\n",
    "find_unrecognized_columns(df = phno, dct = phno_name_dict),\n",
    "find_unrecognized_columns(df = soil, dct = soil_name_dict),\n",
    "find_unrecognized_columns(df = wthr, dct = wthr_name_dict),\n",
    "find_unrecognized_columns(df = mgmt, dct = mgmt_name_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta = meta.rename(columns=meta_name_dict)\n",
    "phno = phno.rename(columns=phno_name_dict)\n",
    "soil = soil.rename(columns=soil_name_dict)\n",
    "wthr = wthr.rename(columns=wthr_name_dict)\n",
    "mgmt = mgmt.rename(columns=mgmt_name_dict)\n",
    "\n",
    "# add indicator columns to help with debugging merge\n",
    "meta['meta'] = True\n",
    "phno['phno'] = True\n",
    "soil['soil'] = True\n",
    "wthr['wthr'] = True\n",
    "mgmt['mgmt'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b4fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27, 55), (20574, 41), (22, 30), (172618, 23), (196, 7)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.shape for e in [meta, phno, soil, wthr, mgmt]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b5e10",
   "metadata": {},
   "source": [
    "## list_known_experiments -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def list_known_experiments():\n",
    "    'Provides a list of the experiments expected for use in `find_unrecognized_experiments`'\n",
    "    known_exps = [\n",
    "        'COH1', 'DEH1', 'GAH1', 'GAH2', 'GEH1', 'IAH1', 'IAH2', 'IAH3', 'IAH4', 'ILH1', 'INH1', \n",
    "        'MIH1', 'MNH1', 'NCH1', 'NEH1', 'NEH2', 'NEH3', 'NYH2', 'NYH3', 'NYS1', 'SCH1', 'TXH1', \n",
    "        'TXH2', 'TXH3', 'WIH1', 'WIH2', 'WIH3',\n",
    "        'MOH1', 'OHH1', # 2020\n",
    "        'NYH1', 'ONH2', 'TXH4', # 2019\n",
    "        'GEH2', 'IAH2 ', 'IAH3 ', 'IAH4 ', 'MOH1 ', 'NYH1', 'W1H1', 'W1H2',\n",
    "        # 2018\n",
    "        'ARH1', 'ARH2', 'KSH1', 'KSH2', 'KSH3', 'MOH1-Rep1', 'MOH1-Rep2', 'ONH1', 'TXH1-Dry', 'TXH1-Early', 'TXH1-Late',\n",
    "        # 2017\n",
    "        'ILH2', 'NEH4', 'NYH4',\n",
    "        # 2016\n",
    "        'AZH1'\n",
    "                 ]\n",
    "    return(known_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16113d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# check Experiment_Code columns for any unexpected columns\n",
    "def find_unrecognized_experiments(column, \n",
    "                                  known_exps = list_known_experiments(), # Either a list of Experiment_Code s or a list of all provided by the default\n",
    "                                  return_all_exps = False):\n",
    "#     known_exps = ['COH1', 'DEH1', 'GAH1', 'GAH2', 'GEH1', 'IAH1', 'IAH2', 'IAH3', 'IAH4', 'ILH1', 'INH1', 'MIH1', 'MNH1', 'NCH1', 'NEH1', 'NEH2', 'NEH3', 'NYH2', 'NYH3', 'NYS1', 'SCH1', 'TXH1', 'TXH2', 'TXH3', 'WIH1', 'WIH2', 'WIH3']\n",
    "    if return_all_exps:\n",
    "        known_exps.sort()\n",
    "        return(known_exps)\n",
    "    else:\n",
    "        unknown_exps = [str(e) for e in list(set(column)) if e not in known_exps]\n",
    "        unknown_exps.sort()\n",
    "        return(unknown_exps)\n",
    "\n",
    "# find_unrecognized_experiments(soil.Experiment_Code, print_all_exps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b3a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# sanitize Experiment Codes\n",
    "\n",
    "def sanitize_Experiment_Codes(df, simple_renames= {}, split_renames= {}):\n",
    "    # simple renames\n",
    "    for e in simple_renames.keys():\n",
    "        mask = (df.Experiment_Code == e)\n",
    "        df.loc[mask, 'Experiment_Code'] = simple_renames[e]\n",
    "\n",
    "    # splits\n",
    "    # pull out the relevant multiname rows, copy, rename, append\n",
    "    for e in split_renames.keys():\n",
    "        mask = (df.Experiment_Code == e)\n",
    "        temp = df.loc[mask, :] \n",
    "\n",
    "        df = df.loc[~mask, :]\n",
    "        for e2 in split_renames[e]:\n",
    "            temp2 = temp.copy()\n",
    "            temp2['Experiment_Code'] = e2\n",
    "            df = df.merge(temp2, how = 'outer')\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1589ac0",
   "metadata": {},
   "source": [
    "# Sanatize ID columns as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soil = sanitize_Experiment_Codes(\n",
    "    df = soil, \n",
    "    simple_renames = {\n",
    "        'W1H1': 'WIH1', \n",
    "        'W1H2': 'WIH2', \n",
    "        'W1H3': 'WIH3'\n",
    "    }, \n",
    "    split_renames = {\n",
    "        'NEH2_NEH3': ['NEH2', 'NEH3']\n",
    "    })\n",
    "\n",
    "wthr = sanitize_Experiment_Codes(\n",
    "    df = wthr, \n",
    "    simple_renames = {\n",
    "    }, \n",
    "    split_renames = {\n",
    "        'NEH2_NEH3': ['NEH2', 'NEH3'],\n",
    "        'NYH3_NYS1': ['NYS1', 'NYH3'],\n",
    "        'TXH1_TXH3': ['TXH1', 'TXH3']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta [] \n",
      "phno [] \n",
      "soil [] \n",
      "wthr [] \n",
      "mgmt [] \n",
      "all  ['COH1', 'DEH1', 'GAH1', 'GAH2', 'GEH1', 'IAH1', 'IAH2', 'IAH3', 'IAH4', 'ILH1', 'INH1', 'MIH1', 'MNH1', 'MOH1', 'NCH1', 'NEH1', 'NEH2', 'NEH3', 'NYH2', 'NYH3', 'NYS1', 'OHH1', 'SCH1', 'TXH1', 'TXH2', 'TXH3', 'WIH1', 'WIH2', 'WIH3']\n"
     ]
    }
   ],
   "source": [
    "# confirm everything's okay\n",
    "print(\n",
    "  'meta', find_unrecognized_experiments(meta.Experiment_Code, return_all_exps=False), \n",
    "'\\nphno', find_unrecognized_experiments(phno.Experiment_Code, return_all_exps=False),\n",
    "'\\nsoil', find_unrecognized_experiments(soil.Experiment_Code, return_all_exps=False),\n",
    "'\\nwthr', find_unrecognized_experiments(wthr.Experiment_Code, return_all_exps=False),\n",
    "'\\nmgmt', find_unrecognized_experiments(mgmt.Experiment_Code, return_all_exps=False),\n",
    "'\\nall ', find_unrecognized_experiments([], return_all_exps=True)\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c37165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find minimum cols needed to index all rows\n",
    "# df = phno\n",
    "# id_cols = ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot',]\n",
    "# candidate_cols = ['State', 'City',\n",
    "#                  'Experiment', 'Source', 'Pedigree', 'Family', 'Tester', 'Replicate',\n",
    "#                   'Block',  'Plot_ID']\n",
    "# target = df.shape[0]\n",
    "\n",
    "# output = pd.DataFrame(zip(\n",
    "#     candidate_cols,\n",
    "#     [df.loc[:, id_cols+[e]].drop_duplicates().shape[0] for e in candidate_cols]\n",
    "#    ), columns=['Additional_ID', 'Uniq_Vals'])\n",
    "\n",
    "# output.assign(At_Target=lambda x:x.Uniq_Vals == target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94761eb",
   "metadata": {},
   "source": [
    "# Rearrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb762bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate static and dynamic values\n",
    "sval = phno.merge(soil, how = 'outer')\n",
    "sval = sval.merge(meta, how = 'outer') # This introduces 3 sites that have no data\n",
    "# sval.shape # used to confirm nrow = #20574 + 3\n",
    "\n",
    "# these tables are different enought we'll keep them separate\n",
    "# mgmt\n",
    "# unfortunately we need multiples because at least one field treats different passes differently\n",
    "mgmt = phno.loc[:, ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot', 'phno']\n",
    "               ].drop_duplicates().merge(mgmt, how = 'outer')\n",
    "# confirm there are no rows in mgmt that are not in phno\n",
    "temp = mgmt.loc[(~mgmt.phno & mgmt.mgmt), :]\n",
    "if 0 != temp.shape[0]:\n",
    "    print(temp)\n",
    "else:\n",
    "    mgmt = mgmt.loc[mgmt.mgmt.notna(), :].drop(columns = 'phno')\n",
    "\n",
    "\n",
    "# wthr\n",
    "# There's only ever one weather station so we have to worry about imputation but not duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set each id col to a string\n",
    "for i in ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot']:\n",
    "    sval[i] = sval[i].astype('string')\n",
    "    mgmt[i]  =  mgmt[i].astype('string')\n",
    "    \n",
    "    if i not in ['Range', 'Pass', 'Plot']:\n",
    "        wthr[i]  =  wthr[i].astype('string')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a94f13",
   "metadata": {},
   "source": [
    "# Sanitize Non-ID columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc8720",
   "metadata": {},
   "source": [
    "## Sanitization functions\n",
    "\n",
    "The pattern to use is:\n",
    " 1. Alter the dataframe\n",
    " 1. Test the dataframe against expectations\n",
    " \n",
    "The main tasks that need to be completed are:\n",
    " 1. Identify values that can't be converted to the expected data type. The \"find_unconvertable_\" family of functions should be used. \n",
    "     1. `find_unconvertable_datetimes`\n",
    "     \n",
    " 1. For simple renaming (e.g. misspellings) or splitting non-tidy data into two rows (\"entry1-entry2\" -> \"entry1\", \"entry2\") use `sanitize_col` \n",
    " 1. Move values that are ambigous but pertain to data imputation to \"Imputation_Notes\" using `relocate_to_Imputation_Notes`\n",
    " 1. If new columns need to be added (e.g. mgmt.Ingredient for parsed components of Product (e.g. elements) ) this should be accomplished with `safe_create_col`.\n",
    " 1. Any one off changes should be accomplised manually. \n",
    " 1. Confirm columns match the expected types with `check_df_dtype_expectations`, and report mismatches. \n",
    "\n",
    "\n",
    "These steps should be completed for each dataframe in turn to minimize the cognitive load of the reader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28410f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "# Make versions of `find_unconvertable_datetimes` for other datatype\n",
    "# make a function to find the unexpected entries so it's easy to write the santization code\n",
    "# in a column, report all the values causing errors OR an index of these values\n",
    "def find_unconvertable_datetimes(df_col, pattern = '%m/%d/%y', index = False):\n",
    "    datetime_errors = pd.to_datetime(pd.Series(df_col), format = pattern, errors='coerce').isna()\n",
    "    if index == True:\n",
    "        return(datetime_errors)\n",
    "    else:\n",
    "        # This is a interesting trick. Python's nan is not equal to itself.\n",
    "        # missing values can't become datetimes so nan is returned if there's a missing value\n",
    "        # This list comprehension removes nan (which is otherwise stubborn to remove) because nan != nan\n",
    "        return([e for e in list(set(df_col[datetime_errors])) if e == e]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "def find_unconvertable_numerics(df_col, # Dataframe column (e.g. df['example']) to be used.\n",
    "                                index = False # Return an index of unconveratbles or a list of unique values\n",
    "                               ):\n",
    "    \"Find the values or positions of values that cannot be converted to a numeric.\"\n",
    "    numeric_errors = pd.to_numeric(pd.Series(df_col), errors='coerce').isna()\n",
    "    if index == True:\n",
    "        return(numeric_errors) # a\n",
    "    else:\n",
    "        # This is a interesting trick. Python's nan is not equal to itself.\n",
    "        # missing values can't become datetimes so nan is returned if there's a missing value\n",
    "        # This list comprehension removes nan (which is otherwise stubborn to remove) because nan != nan\n",
    "        return([e for e in list(set(df_col[numeric_errors])) if e == e]) # b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# generalized version of `sanitize_Experiment_Codes`\n",
    "def sanitize_col(df, col, simple_renames= {}, split_renames= {}):\n",
    "    # simple renames\n",
    "    for e in simple_renames.keys():\n",
    "        mask = (df[col] == e)\n",
    "        df.loc[mask, col] = simple_renames[e]\n",
    "\n",
    "    # splits\n",
    "    # pull out the relevant multiname rows, copy, rename, append\n",
    "    for e in split_renames.keys():\n",
    "        mask = (df[col] == e)\n",
    "        temp = df.loc[mask, :] \n",
    "\n",
    "        df = df.loc[~mask, :]\n",
    "        for e2 in split_renames[e]:\n",
    "            temp2 = temp.copy()\n",
    "            temp2[col] = e2\n",
    "            df = df.merge(temp2, how = 'outer')\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff81af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "# If the Imputation_Notes column doesnt exist, create it. So long as it wouldn't overwrite any imputation notes move each specified value and replace it with nan.\n",
    "def relocate_to_Imputation_Notes(df, col, val_list):\n",
    "    if not 'Imputation_Notes' in df.columns:\n",
    "        df.loc[:, 'Imputation_Notes'] = np.nan\n",
    "\n",
    "    for relocate in val_list:\n",
    "        mask = (df.loc[:, col] == relocate)\n",
    "        mask_Impute_Full = ((df.loc[:, 'Imputation_Notes'] == '') | (df.loc[:, 'Imputation_Notes'].isna()))\n",
    "        # check if this contains anyting\n",
    "        overwrite_danger = df.loc[(mask & ~mask_Impute_Full), 'Imputation_Notes']\n",
    "        if overwrite_danger.shape[0] > 0:\n",
    "            print(\"Warning! The following values will be overwritten. Skipping relocation.\")\n",
    "            print(overwrite_danger)\n",
    "        else:\n",
    "            df.loc[(mask), 'Imputation_Notes'] = df.loc[(mask), col]\n",
    "            df.loc[(mask), col] = np.nan\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6af9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# helper function so we can ask for a new column don't have to worry about overwritting a if it already exists \n",
    "def safe_create_col(df, col_name):\n",
    "    if not col_name in df.columns:\n",
    "        df.loc[:, col_name] = np.nan\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# little helper function to make this easier. Make all the columns in a list into dtype string.\n",
    "# require the column to exist to make this safe.\n",
    "# to make things even easier, use a list comprehension to pull out the keys in the *_col_dtype dict \n",
    "# that have value of 'string'!\n",
    "def cols_astype_string(df, col_list):\n",
    "    for e in [ee for ee in col_list if ee in df.columns]:\n",
    "        df[e] = df[e].astype('string')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "# Ignore columns that don't exist in the dataframe even if they're specified in the dict\n",
    "# For testing that sanitization was successful\n",
    "# a function to check the type of each column \n",
    "# shouldn't _change_ anything, just report what I need to fix\n",
    "def check_df_dtype_expectations(df, dtype_dct):\n",
    "    found = pd.DataFrame(zip(\n",
    "        df.columns,\n",
    "        [str(df[e].dtype) for e in df.columns]\n",
    "    ), columns=['Column', 'dtype'])\n",
    "\n",
    "\n",
    "    expected = pd.DataFrame(zip(dtype_dct.keys(), dtype_dct.values()),\n",
    "                 columns=['Column', 'Expected_dtype']\n",
    "                )\n",
    "    mask = [True if e in df.columns else False for e in expected.Column]\n",
    "    expected = expected.loc[mask, ]\n",
    "    \n",
    "    out = found.merge(expected, how = 'outer')\n",
    "    out = out.assign(Pass = out.dtype == out.Expected_dtype)\n",
    "\n",
    "    print(str(sum(out.Pass))+'/'+str(len(out.Pass))+' Columns pass.')\n",
    "    return(out)\n",
    "\n",
    "# each df should get individual treatment with these steps. Probably most readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cad031",
   "metadata": {},
   "source": [
    "## Sanitization: Column data type expectations\n",
    "Note: to handle missing values some columns that would otherwise be ints are floats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f27a3",
   "metadata": {},
   "source": [
    "### mk_dtype_dict -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def mk_dtype_dict(name # table sval, wthr, or mgmt\n",
    "                ):\n",
    "    'Easily share dictionaries of expected datatypes of the columns across scripts.'\n",
    "    sval_col_dtypes = {\n",
    "        'Year': 'string', \n",
    "        'Experiment_Code': 'string', \n",
    "        'State': 'string', \n",
    "        'City': 'string', \n",
    "        'Plot_Length_Unit_Feet': 'float64', \n",
    "        'Plot_Area_Unit_Feet2': 'float64', \n",
    "        'Alley_Length_Unit_Inches': 'float64', \n",
    "        'Row_Spacing_Unit_Inches': 'float64', \n",
    "        'Rows_Per_Plot': 'float64', \n",
    "        'Seeds_Per_Plot': 'float64', \n",
    "        'Experiment': 'string', \n",
    "        'Source': 'string', \n",
    "        'Pedigree': 'string', \n",
    "        'Family': 'string', \n",
    "        'Tester': 'string', \n",
    "        'Replicate': 'string', \n",
    "        'Block': 'string', \n",
    "        'Plot': 'string', \n",
    "        'Plot_ID': 'string', \n",
    "        'Range': 'string', \n",
    "        'Pass': 'string', \n",
    "        'Planted_Unit_Datetime': 'datetime64[ns]', \n",
    "        'Harvested_Unit_Datetime': 'datetime64[ns]', \n",
    "        'Anthesis_Unit_Datetime': 'datetime64[ns]', \n",
    "        'Silking_Unit_Datetime': 'datetime64[ns]', \n",
    "        'Anthesis_Unit_Days': 'float64', \n",
    "        'Silking_Unit_Days': 'float64', \n",
    "        'Plant_Height_Unit_cm': 'float64', \n",
    "        'Ear_Height_Unit_cm': 'float64', \n",
    "        'Stand_Count_Unit_Number': 'float64', \n",
    "        'Root_Lodging_Unit_Number': 'float64', \n",
    "        'Stalk_Lodging_Unit_Number': 'float64', \n",
    "        'Grain_Moisture_Unit_Percent': 'float64', \n",
    "        'Test_Weight_Unit_lbs': 'float64', \n",
    "        'Plot_Weight_Unit_lbs': 'float64', \n",
    "        'Grain_Yield_Unit_bu_Per_A': 'float64', \n",
    "        'Discarded': 'bool', \n",
    "        'Phenotype_Comments': 'string', \n",
    "        'Filler': 'string', \n",
    "        'Snap_Unit_Number': 'float64', \n",
    "    'phno': 'bool', \n",
    "        'Grower': 'string', \n",
    "        'Recieved_Date_Unit_Datetime': 'datetime64[ns]', \n",
    "        'Processed_Date_Unit_Datetime': 'datetime64[ns]', \n",
    "        'Depth_Unit_UNK': 'float64', \n",
    "        'Soil_1_to_1_Unit_pH': 'float64', \n",
    "        'WDRF_Buffer_Unit_pH': 'float64', \n",
    "        'Soluable_Salts_Unit_mmho_Per_cm': 'float64', \n",
    "        'Texture_Number': 'float64', \n",
    "        'Organic_Matter_Unit_Percent': 'float64', \n",
    "        'Nitrates_Unit_ppm': 'float64', \n",
    "        'N_per_Acre_Unit_lbs': 'float64', \n",
    "        'K_Unit_ppm': 'float64', \n",
    "        'Sulfate_Unit_ppm': 'float64', \n",
    "        'Ca_Unit_ppm': 'float64', \n",
    "        'Mg_Unit_ppm': 'float64', \n",
    "        'Na_Unit_ppm': 'float64', \n",
    "        'Cation_Exchange_Capacity': 'float64', \n",
    "        'H_Sat_Unit_Percent': 'float64', \n",
    "        'K_Sat_Unit_Percent': 'float64', \n",
    "        'Ca_Sat_Unit_Percent': 'float64', \n",
    "        'Mg_Sat_Unit_Percent': 'float64', \n",
    "        'Na_Sat_Unit_Percent': 'float64', \n",
    "        'Mehlich_PIII_P_Unit_ppm': 'float64', \n",
    "        'Sand_Unit_Percent': 'float64', \n",
    "        'Silt_Unit_Percent': 'float64', \n",
    "        'Clay_Unit_Percent': 'float64', \n",
    "        'Texture': 'string', \n",
    "        'Soil_Comments': 'string', \n",
    "    'soil': 'bool', \n",
    "        'Treatment': 'string', \n",
    "        'Farm': 'string', \n",
    "        'Field': 'string', \n",
    "        'Trial_ID': 'string', \n",
    "        'Soil_Taxonomic_ID': 'string', \n",
    "        'Weather_Station_Serial_Number': 'string', \n",
    "        'Weather_Station_Latitude_Unit_Decimal': 'float64', \n",
    "        'Weather_Station_Longitude_Unit_Decimal': 'float64', \n",
    "        'Weather_Station_Placed_Unit_Datetime': 'datetime64[ns]', \n",
    "        'Weather_Station_Removed_Unit_Datetime': 'datetime64[ns]', \n",
    "        'Weather_Station_In_Field_Serial_Number': 'string', \n",
    "        'Weather_Station_In_Field_Latitude_Unit_Decimal': 'float64', \n",
    "        'Weather_Station_In_Field_Longitude_Unit_Decimal': 'float64', \n",
    "        'Previous_Crop': 'string', \n",
    "        'Pre_Plant_Tillage': 'string', \n",
    "        'Post_Plant_Tillage': 'string', \n",
    "        'Planter_Type': 'string', \n",
    "        'Kernels_Per_Plot': 'float64', \n",
    "        'System_Determining_Moisture': 'string', \n",
    "        'Pounds_Needed_Soil_Moisture': 'float64', \n",
    "        'Field_Latitude_BL': 'float64', \n",
    "        'Field_Longitude_BL': 'float64', \n",
    "        'Field_Latitude_BR': 'float64', \n",
    "        'Field_Longitude_BR': 'float64', \n",
    "        'Field_Latitude_TR': 'float64', \n",
    "        'Field_Longitude_TR': 'float64', \n",
    "        'Field_Latitude_TL': 'float64', \n",
    "        'Field_Longitude_TL': 'float64', \n",
    "        'Cardinal_Heading': 'float64', \n",
    "        'Local_Check_Pedigree_1': 'string', \n",
    "        'Local_Check_Source_1': 'string', \n",
    "        'Local_Check_Pedigree_2': 'string', \n",
    "        'Local_Check_Source_2': 'string', \n",
    "        'Local_Check_Pedigree_3': 'string', \n",
    "        'Local_Check_Source_3': 'string', \n",
    "        'Local_Check_Pedigree_4': 'string', \n",
    "        'Local_Check_Source_4': 'string', \n",
    "        'Local_Check_Pedigree_5': 'string', \n",
    "        'Local_Check_Source_5': 'string', \n",
    "        'Comment_1': 'string', \n",
    "        'Comment_2': 'string', \n",
    "        'Comment_3': 'string', \n",
    "        'Comment_4': 'string', \n",
    "        'Comment_5': 'string', \n",
    "        'Comment_6': 'string', \n",
    "        'Comment_7': 'string', \n",
    "        'Comment_8': 'string', \n",
    "        'Comment_9': 'string', \n",
    "        'Comment_70': 'string', \n",
    "    'meta': 'bool',\n",
    "        'Imputation_Notes': 'string',\n",
    "         # 2019\n",
    "        'Possible_Subs': 'string',\n",
    "        'Confirmed_Subs': 'string',\n",
    "        'Kernels_Per_Packet': 'float64',\n",
    "        'Biomass_July_Unit_g': 'float64',\n",
    "        'Biomass_Aug_Unit_g': 'float64',\n",
    "        'Root_Pulling_Force_July_Unit_kgf': 'float64',\n",
    "        'Root_Pulling_Force_Aug_Unit_kgf': 'float64',\n",
    "         # 2018 \n",
    "        'Local_Check': 'string',\n",
    "        'Packets_Per_Plot': 'float64',\n",
    "        'Stand_Count_Unit_Percent': 'float64',\n",
    "        # 2016\n",
    "        'Sample_Type': 'string',\n",
    "        'Zn_Unit_ppm': 'float64',\n",
    "        'Fe_Unit_ppm': 'float64',\n",
    "        'Mn_Unit_ppm': 'float64',\n",
    "        'Cu_Unit_ppm': 'float64',\n",
    "        'B_Unit_ppm': 'float64'\n",
    "        \n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    wthr_col_dtypes = {\n",
    "        'Experiment_Code': 'string', \n",
    "        'Weather_Station_ID': 'string', \n",
    "        'NWS_Network': 'string', \n",
    "        'NWS_Station': 'string', \n",
    "        'Datetime': 'datetime64[ns]', \n",
    "        'Month': 'string', \n",
    "        'Day': 'string', \n",
    "        'Year': 'string', \n",
    "        'Time': 'string', \n",
    "        'Temperature_Unit_C': 'float64', \n",
    "        'Dew_Point_Unit_C': 'float64', \n",
    "        'Relative_Humidity_Unit_Percent': 'float64', \n",
    "        'Solar_Radiation_Unit_W_per_m2': 'float64', \n",
    "        'Rainfall_Unit_mm': 'float64', \n",
    "        'Wind_Speed_Unit_m_per_s': 'float64', \n",
    "        'Wind_Direction_Unit_Degrees': 'float64', \n",
    "        'Wind_Gust_Unit_m_per_s': 'float64', \n",
    "        'Soil_Temperature_Unit_C': 'float64', \n",
    "        'Soil_Moisture_Unit_Percent_VWC': 'float64', \n",
    "        'Soil_EC_Unit_mS_per_cm': 'float64', \n",
    "        'UV_Light_Unit_uM_per_m2s': 'float64', \n",
    "        'PAR_Unit_uM_per_m2s': 'float64', \n",
    "    'wthr': 'bool',\n",
    "        'Imputation_Notes': 'string',\n",
    "        'CO2_Unit_ppm': 'float64',# 2020\n",
    "        # 2018\n",
    "        'Photoperiod_Unit_Hours': 'float64',\n",
    "        'Data_Cleaned': 'bool',\n",
    "        'Fields_Cleaned': 'string',\n",
    "        'Cleaning_Method': 'string',\n",
    "        'Weather_Comments': 'string'\n",
    "    }\n",
    "\n",
    "    mgmt_col_dtypes = {\n",
    "        'Year': 'string',   \n",
    "        'Experiment_Code': 'string', \n",
    "        'Range': 'string',\n",
    "        'Pass': 'string',\n",
    "        'Plot': 'string',\n",
    "        'Application': 'string', \n",
    "        'Product': 'string', \n",
    "        'Date_Datetime': 'datetime64[ns]', \n",
    "        'Amount_Per_Acre': 'float64', \n",
    "        'Unit': 'string', \n",
    "    'mgmt': 'bool',\n",
    "        'Imputation_Notes': 'string',\n",
    "        'Ingredient': 'string',\n",
    "        # 2016\n",
    "        'Replicate': 'string'\n",
    "    }\n",
    "   \n",
    "    if name == 'sval':\n",
    "        return sval_col_dtypes\n",
    "    elif name == 'wthr':\n",
    "        return wthr_col_dtypes\n",
    "    elif name == 'mgmt':\n",
    "        return mgmt_col_dtypes\n",
    "    else:\n",
    "        print('Requested name is not defined')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723aaeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sval_col_dtypes = mk_dtype_dict(name = 'sval')\n",
    "wthr_col_dtypes = mk_dtype_dict(name = 'wthr')\n",
    "mgmt_col_dtypes = mk_dtype_dict(name = 'mgmt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972cfb9f",
   "metadata": {},
   "source": [
    "# Sanitization: Alter entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccee17b",
   "metadata": {},
   "source": [
    "## Static values (within season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351db7b",
   "metadata": {},
   "source": [
    "### Datetime containing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date cols into datetime. Lean on pd.to_datetime() to infer the format, assume that each site uses the same format.\n",
    "\n",
    "for e in ['Planted_Unit_Datetime', \n",
    "    'Harvested_Unit_Datetime', \n",
    "    'Anthesis_Unit_Datetime', \n",
    "    'Silking_Unit_Datetime', \n",
    "    'Recieved_Date_Unit_Datetime', \n",
    "    'Processed_Date_Unit_Datetime', \n",
    "    'Weather_Station_Placed_Unit_Datetime', \n",
    "    'Weather_Station_Removed_Unit_Datetime'\n",
    "    ]:\n",
    "# find_unconvertable_datetimes(df_col=sval[e], pattern='%Y-%m-%d %H:%M', index=False)\n",
    "\n",
    "    sval['Datetime_Temp'] = pd.to_datetime(np.nan)\n",
    "\n",
    "    for code in list(sval.Experiment_Code.drop_duplicates()):\n",
    "    # code = list(sval.Experiment_Code.drop_duplicates())[0]\n",
    "        sval.loc[sval.Experiment_Code == code, 'Datetime_Temp'\n",
    "                 ] = pd.to_datetime(sval.loc[sval.Experiment_Code == code, e])\n",
    "\n",
    "    sval.loc[:, e] = sval.loc[:, 'Datetime_Temp'] \n",
    "\n",
    "sval = sval.drop(columns = 'Datetime_Temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> floats\n",
    "\n",
    "# [find_unconvertable_numerics(df_col = sval[e], index = False) for e in [\n",
    "#     'Alley_Length_Unit_Inches',\n",
    "# 'Row_Spacing_Unit_Inches',\n",
    "# 'Pounds_Needed_Soil_Moisture'\n",
    "# ]]\n",
    "\n",
    "sval = sanitize_col(\n",
    "    df = sval, \n",
    "    col = 'Pounds_Needed_Soil_Moisture', \n",
    "    simple_renames= {'3 to 4':'3.5'}, \n",
    "    split_renames= {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd19e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert types\n",
    "for e in ['Alley_Length_Unit_Inches', 'Row_Spacing_Unit_Inches', 'Pounds_Needed_Soil_Moisture',\n",
    "         'Anthesis_Unit_Days', 'Silking_Unit_Days', 'Kernels_Per_Plot']:\n",
    "    err_list = find_unconvertable_numerics(df_col = sval[e], index = False)\n",
    "    if err_list != []:\n",
    "        print(e)\n",
    "        print(err_list)\n",
    "    else:\n",
    "        sval[e] = sval[e].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97517d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bool\n",
    "sval = sanitize_col(\n",
    "    df = sval, \n",
    "    col = 'Discarded', \n",
    "    simple_renames= {\n",
    "        'Yes':'True',\n",
    "        'yes':'True'}, \n",
    "    split_renames= {})\n",
    "\n",
    "# set missing to false\n",
    "sval.loc[sval.Discarded.isna(), 'Discarded'] = 'False'\n",
    "sval.Discarded = sval.Discarded.map({'True': True, 'False': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e21fb",
   "metadata": {},
   "source": [
    "### Simple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bool\n",
    "sval['phno'] = sval['phno'].astype('bool')\n",
    "sval['soil'] = sval['soil'].astype('bool')\n",
    "sval['meta'] = sval['meta'].astype('bool')\n",
    "\n",
    "# to string\n",
    "sval = cols_astype_string(\n",
    "    df = sval, \n",
    "    col_list = [key for key in sval_col_dtypes.keys() if sval_col_dtypes[key] == 'string'])\n",
    "\n",
    "sval.Year = year_string\n",
    "sval.Year = sval.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16487c66",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec742610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 Columns pass.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = check_df_dtype_expectations(df = sval, dtype_dct = sval_col_dtypes)\n",
    "\n",
    "if sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n",
    "    pass\n",
    "else:\n",
    "    print(checkpoint.loc[~checkpoint.Pass, ]) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54014602",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3311ef6",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dacfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of writing regexes to figure out the mose likely format for each datetime, we assume each experiment will be consistent withing that experiment\n",
    "# and let pd figure it out.\n",
    "# wthr['Datetime_Temp'] = pd.to_datetime(np.nan)\n",
    "\n",
    "# for code in list(wthr.loc[:, 'Experiment_Code'].drop_duplicates()):\n",
    "#     wthr.loc[wthr.Experiment_Code == code, 'Datetime_Temp'] = pd.to_datetime(wthr.loc[wthr.Experiment_Code == code, 'Datetime'], errors='coerce')\n",
    "\n",
    "\n",
    "# ... or we use the fields in the df to make a consistent format\n",
    "wthr = cols_astype_string(\n",
    "    df = wthr, \n",
    "    col_list = ['Year', 'Month', 'Day', 'Time'])\n",
    "\n",
    "wthr = sanitize_col(\n",
    "    df = wthr,\n",
    "    col = 'Time', \n",
    "    simple_renames= {'24:00:00': '00:00:00'}, # this could be day + 24 h instead of a miscoded day + 0 h\n",
    "    split_renames= {})\n",
    "\n",
    "wthr['Datetime_Temp'] = wthr['Year']+'-'+wthr['Month']+'-'+wthr['Day']+' '+wthr['Time']\n",
    "\n",
    "# convert types\n",
    "err_list = find_unconvertable_datetimes(df_col=wthr['Datetime_Temp'], pattern='%Y-%m-%d %H:%M', index=False)\n",
    "if err_list != []:\n",
    "    print(err_list)\n",
    "else:\n",
    "    wthr.Datetime_Temp = pd.to_datetime(pd.Series(wthr.Datetime_Temp), errors='coerce')\n",
    "    wthr.Datetime = wthr.Datetime_Temp\n",
    "    wthr = wthr.drop(columns= 'Datetime_Temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd61efb",
   "metadata": {},
   "source": [
    "### Simple Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed990b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to string\n",
    "wthr = cols_astype_string(\n",
    "    df = wthr, \n",
    "    col_list = [key for key in wthr_col_dtypes.keys() if wthr_col_dtypes[key] == 'string'])\n",
    "\n",
    "wthr.Year = year_string\n",
    "wthr.Year = wthr.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3252022d",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fd6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 Columns pass.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = check_df_dtype_expectations(df = wthr, dtype_dct = wthr_col_dtypes)\n",
    "\n",
    "if sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n",
    "    pass\n",
    "else:\n",
    "    print(checkpoint.loc[~checkpoint.Pass, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958033e",
   "metadata": {},
   "source": [
    "## Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf1a42",
   "metadata": {},
   "source": [
    "### Date_Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt = relocate_to_Imputation_Notes(df = mgmt, col = 'Date_Datetime', val_list= ['Before Planting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt = sanitize_col(\n",
    "    df = mgmt, \n",
    "    col = 'Date_Datetime', \n",
    "    simple_renames= {}, \n",
    "    split_renames= {'6/24/21 for all but plots in pass 2; 7/5/21 for pass 2' : [\n",
    "                        '6/24/21 for all but plots in pass 2', '7/5/21 for pass 2']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make corrections too one-off to fix with a funciton. \n",
    "mask = ((mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2') & (mgmt.Pass != 2.))\n",
    "mgmt.loc[mask, 'Date_Datetime'] = '6/24/21'\n",
    "# since we split without specifiying pass we need to remove any rows that still have the search string.\n",
    "# and overwrite the df\n",
    "mask = (mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2')\n",
    "mgmt = mgmt.loc[~mask, :].copy()\n",
    "\n",
    "mask = ((mgmt.Date_Datetime == '7/5/21 for pass 2') & (mgmt.Pass == 2.))\n",
    "mgmt.loc[mask, 'Date_Datetime'] = '7/5/21'\n",
    "mask = (mgmt.Date_Datetime == '7/5/21 for pass 2')\n",
    "mgmt = mgmt.loc[~mask, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b612643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert types\n",
    "err_list = find_unconvertable_datetimes(df_col=mgmt.Date_Datetime, pattern='%m/%d/%y', index=False)\n",
    "if err_list != []:\n",
    "    print(err_list)\n",
    "else:\n",
    "    mgmt.Date_Datetime = pd.to_datetime(pd.Series(mgmt.Date_Datetime), format = '%m/%d/%y', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e0f42",
   "metadata": {},
   "source": [
    "### Amount_Per_Acre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt.loc[find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = True), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt = sanitize_col(\n",
    "    df = mgmt, \n",
    "    col = 'Amount_Per_Acre', \n",
    "    simple_renames= {'170 lb (actual N)': '170 (N)'}, \n",
    "    split_renames= {'51.75, 40.7, 111.7 (N,P,K)': ['51.75 (N)', '40.7 (P)', '111.7 (K)'],\n",
    "                    '31-150-138': ['31 (N)', '150 (P)', '138 (K)'],\n",
    "                    '16 (N), 41 (P)': ['16 (N)', '41 (P)']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ead32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt = safe_create_col(mgmt, \"Ingredient\")\n",
    "mask = mgmt.Ingredient.isna()\n",
    "mgmt.loc[mask, 'Ingredient'] = mgmt.loc[mask, 'Product']\n",
    "\n",
    "# assume each string is formated as 'val (key)'. `sanitize_col` should be used to enforce this.\n",
    "for e in ['150 (P)', '36.6 (N)', '138 (K)', '111.7 (K)', '41 (P)', '16 (N)', '170 (N)', '35.7 (N)', '51.75 (N)', '31 (N)', '40.7 (P)']:\n",
    "    val = re.findall('^\\d+[.]*\\d*', e)[0]\n",
    "    key = re.findall('\\(.+\\)',      e)[0].replace('(', '').replace(')', '')\n",
    "    \n",
    "    mask = (mgmt['Amount_Per_Acre'] == e)\n",
    "    mgmt.loc[mask, 'Ingredient'] = key\n",
    "    mgmt.loc[mask, 'Amount_Per_Acre'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc11303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert types\n",
    "err_list = find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = False)\n",
    "if err_list != []:\n",
    "    print(err_list)\n",
    "else:\n",
    "    mgmt.Amount_Per_Acre = pd.to_numeric(mgmt.Amount_Per_Acre, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57973e80",
   "metadata": {},
   "source": [
    "### Ingredient\n",
    "This is to be the cleaned up version of the \"Product\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66140777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(mgmt.loc[:, 'Ingredient'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c3e99",
   "metadata": {},
   "source": [
    "### Simple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb60d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bool\n",
    "mgmt['mgmt'] = mgmt['mgmt'].astype('bool')\n",
    "\n",
    "# to string\n",
    "for e in [ee for ee in ['Application', 'Product', 'Ingredient', 'Unit', 'Imputation_Notes'] if ee in mgmt.columns]:\n",
    "    mgmt[e] = mgmt[e].astype('string')\n",
    "    \n",
    "\n",
    "mgmt.Year = year_string\n",
    "mgmt.Year = mgmt.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2160e",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1578ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 Columns pass.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>dtype</th>\n",
       "      <th>Expected_dtype</th>\n",
       "      <th>Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Year</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Experiment_Code</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Range</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pass</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Plot</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Application</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Product</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Date_Datetime</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Amount_Per_Acre</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Unit</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mgmt</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Imputation_Notes</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ingredient</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Column           dtype  Expected_dtype  Pass\n",
       "0               Year          string          string  True\n",
       "1    Experiment_Code          string          string  True\n",
       "2              Range          string          string  True\n",
       "3               Pass          string          string  True\n",
       "4               Plot          string          string  True\n",
       "5        Application          string          string  True\n",
       "6            Product          string          string  True\n",
       "7      Date_Datetime  datetime64[ns]  datetime64[ns]  True\n",
       "8    Amount_Per_Acre         float64         float64  True\n",
       "9               Unit          string          string  True\n",
       "10              mgmt            bool            bool  True\n",
       "11  Imputation_Notes          string          string  True\n",
       "12        Ingredient          string          string  True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_df_dtype_expectations(df = mgmt, dtype_dct = mgmt_col_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109ee8b",
   "metadata": {},
   "source": [
    "# Publish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccde6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pickle\n",
    "def write_out_pkl(obj, path = './temp.pickle'):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_out_pkl(obj = sval, path = './data/interim/'+year_string+'sval.pickle')\n",
    "write_out_pkl(obj = wthr, path = './data/interim/'+year_string+'wthr.pickle')\n",
    "write_out_pkl(obj = mgmt, path = './data/interim/'+year_string+'mgmt.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
