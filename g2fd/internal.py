# AUTOGENERATED! DO NOT EDIT! File to edit: ../02_FreshStart_2021.ipynb.

# %% auto 0
__all__ = ['prlst', 'prlst2dct', 'dash80', 'mk_uniq_val_dict', 'pr_eq_list', 'mk_df_of_n_similar_cols',
           'mk_df_of_most_similar_cols', 'match_df_cols', 'find_unrecognized_experiments', 'sanitize_Experiment_Codes',
           'find_unconvertable_datetimes', 'find_unconvertable_numerics', 'sanitize_col',
           'relocate_to_Imputation_Notes', 'safe_create_col', 'cols_astype_string', 'check_df_dtype_expectations',
           'write_out_pkl']

# %% ../02_FreshStart_2021.ipynb 4
# Settings ----
# Helper functions,remove if no longer needed
def prlst(lst): 
    "This is just a helper function to ease formating lists of strings with each entryon a different line."
    print('[')
    for e in lst:
        if e != lst[-1]:
            print("'"+e+"', ")
        else:
            print("'"+e+"'")
    print(']')
    
def prlst2dct(lst): 
    "This is just a helper function to ease formating lists of strings with each entryon a different line."
    print('{')
    for e in lst:
        if e != lst[-1]:
            print("'"+e+"': 'XXXXXXX', ")
        else:
            print("'"+e+"': 'XXXXXXX'")
    print('}')
    
def dash80(dash = '-'): return ''.join([dash for e in range(80)])


# prlst([])
# prlst2dct([])
# dash80()

# %% ../02_FreshStart_2021.ipynb 7
# Helper functions to match up the same data in different dataframes under different columns names

# make a dictionary of column name : unique values
def mk_uniq_val_dict(df1):
    uniq_val_dict = {}
    for df1_col in list(df1.columns):
        uniq_val_dict.update({df1_col:set(df1[df1_col])})
    return(uniq_val_dict)

def pr_eq_list(lst1, lst2): return len(set(lst1) & set(lst2)) / len(set(lst1) | set(lst2))

# take two dictionaries from `mk_uniq_val_dict` and a key to match, return a df of the n closest matches (based on set of column values)
def mk_df_of_n_similar_cols(dct1, key_to_match, dct2, n = 1):
    # key_to_match = 'Experiment_Code'        
    lst = dct1[key_to_match]
    # dct2 = dict1

    keys = dct2.keys()
    similarities = [pr_eq_list(lst, dct2[e]) for e in dct2.keys()]

    similarityDf = pd.DataFrame(
        zip(dct2.keys(), similarities), 
        columns=['Column2', 'PrMatch']
    ).sort_values('PrMatch', ascending=False)
    
    output = similarityDf.head(n)
    output.insert(0, column = 'Column1', value = key_to_match)
    return(output)

# take two dictionaries from `mk_uniq_val_dict` and find the best match for each key in dict1 in dict2. 
def mk_df_of_most_similar_cols(dict1, dict2):
    return( pd.concat( [mk_df_of_n_similar_cols(dct1 = dict1, key_to_match = key, dct2 = dict2, n = 1) for key in dict1.keys() ]) )

# Combine `mk_df_of_most_similar_cols` and `mk_uniq_val_dict` to find the closest matches between two columns in different dfs.
# Should be helpful for finding columns with the same data but different names (e.g. Experiment vs Experiment_Code)
def match_df_cols(df1, df2):
    df1_cols = list(df1.columns)
    df2_cols = list(df2.columns)

    dict1 = mk_uniq_val_dict(df1)
    dict2 = mk_uniq_val_dict(df2)
    
    return(mk_df_of_most_similar_cols(dict1, dict2))

# %% ../02_FreshStart_2021.ipynb 19
# check Experiment_Code columns for any unexpected columns
def find_unrecognized_experiments(column, return_all_exps = False):
    known_exps = ['COH1', 'DEH1', 'GAH1', 'GAH2', 'GEH1', 'IAH1', 'IAH2', 'IAH3', 'IAH4', 'ILH1', 'INH1', 'MIH1', 'MNH1', 'NCH1', 'NEH1', 'NEH2', 'NEH3', 'NYH2', 'NYH3', 'NYS1', 'SCH1', 'TXH1', 'TXH2', 'TXH3', 'WIH1', 'WIH2', 'WIH3']
    if return_all_exps:
        known_exps.sort()
        return(known_exps)
    else:
        unknown_exps = [str(e) for e in list(set(column)) if e not in known_exps]
        unknown_exps.sort()
        return(unknown_exps)

# find_unrecognized_experiments(soil.Experiment_Code, print_all_exps=True)


# %% ../02_FreshStart_2021.ipynb 21
# sanitize Experiment Codes

def sanitize_Experiment_Codes(df, simple_renames= {}, split_renames= {}):
    # simple renames
    for e in simple_renames.keys():
        mask = (df.Experiment_Code == e)
        df.loc[mask, 'Experiment_Code'] = simple_renames[e]

    # splits
    # pull out the relevant multiname rows, copy, rename, append
    for e in split_renames.keys():
        mask = (df.Experiment_Code == e)
        temp = df.loc[mask, :] 

        df = df.loc[~mask, :]
        for e2 in split_renames[e]:
            temp2 = temp.copy()
            temp2['Experiment_Code'] = e2
            df = df.merge(temp2, how = 'outer')

    return(df)

# %% ../02_FreshStart_2021.ipynb 32
# Make versions of `find_unconvertable_datetimes` for other datatype
# make a function to find the unexpected entries so it's easy to write the santization code

# in a column, report all the values causing errors OR an index of these values
def find_unconvertable_datetimes(df_col, pattern = '%m/%d/%y', index = False):
    datetime_errors = pd.to_datetime(pd.Series(df_col), format = pattern, errors='coerce').isna()
    if index == True:
        return(datetime_errors)
    else:
        # This is a interesting trick. Python's nan is not equal to itself.
        # missing values can't become datetimes so nan is returned if there's a missing value
        # This list comprehension removes nan (which is otherwise stubborn to remove) because nan != nan
        return([e for e in list(set(df_col[datetime_errors])) if e == e]) 

    
def find_unconvertable_numerics(df_col, index = False):
    numeric_errors = pd.to_numeric(pd.Series(df_col), errors='coerce').isna()
    if index == True:
        return(numeric_errors)
    else:
        # This is a interesting trick. Python's nan is not equal to itself.
        # missing values can't become datetimes so nan is returned if there's a missing value
        # This list comprehension removes nan (which is otherwise stubborn to remove) because nan != nan
        return([e for e in list(set(df_col[numeric_errors])) if e == e]) 

    
# generalized version of `sanitize_Experiment_Codes`
def sanitize_col(df, col, simple_renames= {}, split_renames= {}):
    # simple renames
    for e in simple_renames.keys():
        mask = (df[col] == e)
        df.loc[mask, col] = simple_renames[e]

    # splits
    # pull out the relevant multiname rows, copy, rename, append
    for e in split_renames.keys():
        mask = (df[col] == e)
        temp = df.loc[mask, :] 

        df = df.loc[~mask, :]
        for e2 in split_renames[e]:
            temp2 = temp.copy()
            temp2[col] = e2
            df = df.merge(temp2, how = 'outer')

    return(df)


# If the Imputation_Notes column doesnt exist, create it. So long as it wouldn't overwrite any imputation notes move each specified value and replace it with nan.
def relocate_to_Imputation_Notes(df, col, val_list):
    if not 'Imputation_Notes' in df.columns:
        df.loc[:, 'Imputation_Notes'] = np.nan

    for relocate in val_list:
        mask = (df.loc[:, col] == relocate)
        mask_Impute_Full = ((df.loc[:, 'Imputation_Notes'] == '') | (df.loc[:, 'Imputation_Notes'].isna()))
        # check if this contains anyting
        overwrite_danger = df.loc[(mask & ~mask_Impute_Full), 'Imputation_Notes']
        if overwrite_danger.shape[0] > 0:
            print("Warning! The following values will be overwritten. Skipping relocation.")
            print(overwrite_danger)
        else:
            df.loc[(mask), 'Imputation_Notes'] = df.loc[(mask), col]
            df.loc[(mask), col] = np.nan
    return(df)



# helper function so we can ask for a new column don't have to worry about overwritting a if it already exists 
def safe_create_col(df, col_name):
    if not col_name in df.columns:
        df.loc[:, col_name] = np.nan
    return(df)

# little helper function to make this easier. Make all the columns in a list into dtype string.
# require the column to exist to make this safe.
# to make things even easier, use a list comprehension to pull out the keys in the *_col_dtype dict 
# that have value of 'string'!
def cols_astype_string(df, col_list):
    for e in [ee for ee in col_list if ee in df.columns]:
        df[e] = df[e].astype('string')
    return(df)



# Ignore columns that don't exist in the dataframe even if they're specified in the dict
# For testing that sanitization was successful
# a function to check the type of each column 
# shouldn't _change_ anything, just report what I need to fix
def check_df_dtype_expectations(df, dtype_dct):
    found = pd.DataFrame(zip(
        df.columns,
        [str(df[e].dtype) for e in df.columns]
    ), columns=['Column', 'dtype'])


    expected = pd.DataFrame(zip(dtype_dct.keys(), dtype_dct.values()),
                 columns=['Column', 'Expected_dtype']
                )
    mask = [True if e in df.columns else False for e in expected.Column]
    expected = expected.loc[mask, ]
    
    out = found.merge(expected, how = 'outer')
    out = out.assign(Pass = out.dtype == out.Expected_dtype)

    print(str(sum(out.Pass))+'/'+str(len(out.Pass))+' Columns pass.')
    return(out)

# each df should get individual treatment with these steps. Probably most readable

# %% ../02_FreshStart_2021.ipynb 73
def write_out_pkl(obj, path = './temp.pickle'):
    with open(path, 'wb') as handle:
        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)   
