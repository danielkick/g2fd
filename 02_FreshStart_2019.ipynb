{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065a08d0",
   "metadata": {},
   "source": [
    "# Process Data from 2019 into a consistent format. \n",
    "\n",
    "> This notebook brings the 2019 into alignment with the desired format with respect to field name, type, and grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports ----\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2fd.internal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c88f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019\n",
    "year_string = '2019'\n",
    "\n",
    "meta_path = './data/raw/GenomesToFields_data_2019/z._2019_supplemental_info/g2f_2019_field_metadata.csv'\n",
    "phno_path = './data/raw/GenomesToFields_data_2019/a._2019_phenotypic_data/g2f_2019_phenotypic_clean_data.csv'\n",
    "wthr_path = './data/raw/GenomesToFields_data_2019/b._2019_weather_data/2019_weather_cleaned.csv'\n",
    "soil_path = './data/raw/GenomesToFields_data_2019/c._2019_soil_data/g2f_2019_soil_data.csv'\n",
    "mgmt_path = './data/raw/GenomesToFields_data_2019/z._2019_supplemental_info/g2f_2019_agronomic_information.csv'\n",
    "\n",
    "meta = pd.read_csv(meta_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "phno = pd.read_csv(phno_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "wthr = pd.read_csv(wthr_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "soil = pd.read_csv(soil_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "mgmt = pd.read_csv(mgmt_path, encoding = \"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d31cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dicts for column renaming\n",
    "meta_name_dict = mk_name_dict(name = 'meta')\n",
    "phno_name_dict = mk_name_dict(name = 'phno')\n",
    "soil_name_dict = mk_name_dict(name = 'soil')\n",
    "wthr_name_dict = mk_name_dict(name = 'wthr')\n",
    "mgmt_name_dict = mk_name_dict(name = 'mgmt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ab577",
   "metadata": {},
   "source": [
    "# Rename\n",
    "**Naming rules:**\n",
    "- One dict for each input df\n",
    "- Comment out anything that shouldn't be changed\n",
    "- Upper_Upper_Unit_\\$unit\n",
    "- Upper_$number\n",
    "- No special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b0a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [], [], [], [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(find_unrecognized_columns(df = meta, dct = meta_name_dict),\n",
    "find_unrecognized_columns(df = phno, dct = phno_name_dict),\n",
    "find_unrecognized_columns(df = soil, dct = soil_name_dict),\n",
    "find_unrecognized_columns(df = wthr, dct = wthr_name_dict),\n",
    "find_unrecognized_columns(df = mgmt, dct = mgmt_name_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.rename(columns=meta_name_dict)\n",
    "phno = phno.rename(columns=phno_name_dict)\n",
    "soil = soil.rename(columns=soil_name_dict)\n",
    "wthr = wthr.rename(columns=wthr_name_dict)\n",
    "mgmt = mgmt.rename(columns=mgmt_name_dict)\n",
    "\n",
    "# add indicator columns to help with debugging merge\n",
    "meta['meta'] = True\n",
    "phno['phno'] = True\n",
    "soil['soil'] = True\n",
    "wthr['wthr'] = True\n",
    "mgmt['mgmt'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b4fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(29, 55), (20952, 45), (25, 30), (232437, 24), (92, 7)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.shape for e in [meta, phno, soil, wthr, mgmt]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1589ac0",
   "metadata": {},
   "source": [
    "# Sanatize ID columns as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soil = sanitize_Experiment_Codes(\n",
    "#     df = soil, \n",
    "#     simple_renames = {\n",
    "#         'W1H1': 'WIH1', \n",
    "#         'W1H2': 'WIH2', \n",
    "#         'W1H3': 'WIH3'\n",
    "#     }, \n",
    "#     split_renames = {\n",
    "#         'NEH2_NEH3': ['NEH2', 'NEH3']\n",
    "#     })\n",
    "\n",
    "# wthr = sanitize_Experiment_Codes(\n",
    "#     df = wthr, \n",
    "#     simple_renames = {\n",
    "#     }, \n",
    "#     split_renames = {\n",
    "#         'NEH2_NEH3': ['NEH2', 'NEH3'],\n",
    "#         'NYH3_NYS1': ['NYS1', 'NYH3'],\n",
    "#         'TXH1_TXH3': ['TXH1', 'TXH3']\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta [] \n",
      "phno [] \n",
      "soil [] \n",
      "wthr [] \n",
      "mgmt [] \n",
      "all  ['COH1', 'DEH1', 'GAH1', 'GAH2', 'GEH1', 'GEH2', 'IAH1', 'IAH2', 'IAH2 ', 'IAH3', 'IAH3 ', 'IAH4', 'IAH4 ', 'ILH1', 'INH1', 'MIH1', 'MNH1', 'MOH1', 'MOH1 ', 'NCH1', 'NEH1', 'NEH2', 'NEH3', 'NYH1', 'NYH1', 'NYH2', 'NYH3', 'NYS1', 'OHH1', 'ONH2', 'SCH1', 'TXH1', 'TXH2', 'TXH3', 'TXH4', 'W1H1', 'W1H2', 'WIH1', 'WIH2', 'WIH3']\n"
     ]
    }
   ],
   "source": [
    "# confirm everything's okay\n",
    "print(\n",
    "  'meta', find_unrecognized_experiments(meta.Experiment_Code, return_all_exps=False), \n",
    "'\\nphno', find_unrecognized_experiments(phno.Experiment_Code, return_all_exps=False),\n",
    "'\\nsoil', find_unrecognized_experiments(soil.Experiment_Code, return_all_exps=False),\n",
    "'\\nwthr', find_unrecognized_experiments(wthr.Experiment_Code, return_all_exps=False),\n",
    "'\\nmgmt', find_unrecognized_experiments(mgmt.Experiment_Code, return_all_exps=False),\n",
    "'\\nall ', find_unrecognized_experiments([], return_all_exps=True)\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c37165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find minimum cols needed to index all rows\n",
    "# df = phno\n",
    "# id_cols = ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot',]\n",
    "# candidate_cols = ['State', 'City',\n",
    "#                  'Experiment', 'Source', 'Pedigree', 'Family', 'Tester', 'Replicate',\n",
    "#                   'Block',  'Plot_ID']\n",
    "# target = df.shape[0]\n",
    "\n",
    "# output = pd.DataFrame(zip(\n",
    "#     candidate_cols,\n",
    "#     [df.loc[:, id_cols+[e]].drop_duplicates().shape[0] for e in candidate_cols]\n",
    "#    ), columns=['Additional_ID', 'Uniq_Vals'])\n",
    "\n",
    "# output.assign(At_Target=lambda x:x.Uniq_Vals == target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94761eb",
   "metadata": {},
   "source": [
    "# Rearrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb762bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate static and dynamic values\n",
    "sval = phno.merge(soil, how = 'outer')\n",
    "sval = sval.merge(meta, how = 'outer') # This introduces 3 sites that have no data\n",
    "# sval.shape # used to confirm nrow = #20574 + 3\n",
    "\n",
    "# these tables are different enought we'll keep them separate\n",
    "# mgmt\n",
    "# unfortunately we need multiples because at least one field treats different passes differently\n",
    "mgmt = phno.loc[:, ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot', 'phno']\n",
    "               ].drop_duplicates().merge(mgmt, how = 'outer')\n",
    "# confirm there are no rows in mgmt that are not in phno\n",
    "temp = mgmt.loc[(~mgmt.phno & mgmt.mgmt), :]\n",
    "if 0 != temp.shape[0]:\n",
    "    print(temp)\n",
    "else:\n",
    "    mgmt = mgmt.loc[mgmt.mgmt.notna(), :].drop(columns = 'phno')\n",
    "\n",
    "\n",
    "# wthr\n",
    "# There's only ever one weather station so we have to worry about imputation but not duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set each id col to a string\n",
    "for i in ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot']:\n",
    "    sval[i] = sval[i].astype('string')\n",
    "    mgmt[i]  =  mgmt[i].astype('string')\n",
    "    \n",
    "    if i not in ['Range', 'Pass', 'Plot']:\n",
    "        wthr[i]  =  wthr[i].astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a94f13",
   "metadata": {},
   "source": [
    "# Sanitize Non-ID columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc8720",
   "metadata": {},
   "source": [
    "## Sanitization functions\n",
    "\n",
    "The pattern to use is:\n",
    " 1. Alter the dataframe\n",
    " 1. Test the dataframe against expectations\n",
    " \n",
    "The main tasks that need to be completed are:\n",
    " 1. Identify values that can't be converted to the expected data type. The \"find_unconvertable_\" family of functions should be used. \n",
    "     1. `find_unconvertable_datetimes`\n",
    "     \n",
    " 1. For simple renaming (e.g. misspellings) or splitting non-tidy data into two rows (\"entry1-entry2\" -> \"entry1\", \"entry2\") use `sanitize_col` \n",
    " 1. Move values that are ambigous but pertain to data imputation to \"Imputation_Notes\" using `relocate_to_Imputation_Notes`\n",
    " 1. If new columns need to be added (e.g. mgmt.Ingredient for parsed components of Product (e.g. elements) ) this should be accomplished with `safe_create_col`.\n",
    " 1. Any one off changes should be accomplised manually. \n",
    " 1. Confirm columns match the expected types with `check_df_dtype_expectations`, and report mismatches. \n",
    "\n",
    "\n",
    "These steps should be completed for each dataframe in turn to minimize the cognitive load of the reader. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cad031",
   "metadata": {},
   "source": [
    "## Sanitization: Column data type expectations\n",
    "Note: to handle missing values some columns that would otherwise be ints are floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sval_col_dtypes = mk_dtype_dict(name = 'sval')\n",
    "wthr_col_dtypes = mk_dtype_dict(name = 'wthr')\n",
    "mgmt_col_dtypes = mk_dtype_dict(name = 'mgmt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972cfb9f",
   "metadata": {},
   "source": [
    "# Sanitization: Alter entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccee17b",
   "metadata": {},
   "source": [
    "## Static values (within season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab9954",
   "metadata": {},
   "source": [
    "### Datetime containing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eef93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['43681',\n",
       " '7/2/2019',\n",
       " '43685',\n",
       " '7/11/2019',\n",
       " '7/5/2019',\n",
       " '7/14/2019',\n",
       " '7/15/2019',\n",
       " '7/7/2019',\n",
       " '7/13/2019',\n",
       " '7/12/2019',\n",
       " '7/8/2019',\n",
       " '7/3/2019',\n",
       " '7/1/2019',\n",
       " '7/10/2019',\n",
       " '43684',\n",
       " '7/6/2019',\n",
       " '7/4/2019',\n",
       " '43686',\n",
       " '6/30/2019',\n",
       " '7/9/2019']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['43681', \n",
    "'43682', \n",
    "'43684', \n",
    "'43683']\n",
    "\n",
    "sval.loc[sval.Anthesis_Unit_Datetime == ['43681', \n",
    "'43682', \n",
    "'43684', \n",
    "'43683'][0] , :]\n",
    "find_unconvertable_datetimes(df_col = sval.Silking_Unit_Datetime, pattern='%m/%d/%y', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sval = sanitize_col(\n",
    "    df = sval, \n",
    "    col = 'Anthesis_Unit_Datetime', \n",
    "    simple_renames= {'43681': '8/4/2019', \n",
    "                    '43682': '8/5/2019', \n",
    "                    '43684': '8/7/2019', \n",
    "                    '43683': '8/6/2019'}, \n",
    "    split_renames= {})\n",
    "\n",
    "sval = sanitize_col(\n",
    "    df = sval, \n",
    "    col = 'Silking_Unit_Datetime', \n",
    "    simple_renames= {'43681': '8/4/2019',\n",
    "                    '43686': '8/9/2019',\n",
    "                    '43684': '8/7/2019',\n",
    "                    '43685': '8/8/2019',}, \n",
    "    split_renames= {})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2d163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planted_Unit_Datetime\n",
      "Harvested_Unit_Datetime\n",
      "Anthesis_Unit_Datetime\n",
      "Silking_Unit_Datetime\n",
      "Recieved_Date_Unit_Datetime\n",
      "Processed_Date_Unit_Datetime\n",
      "Weather_Station_Placed_Unit_Datetime\n",
      "Weather_Station_Removed_Unit_Datetime\n"
     ]
    }
   ],
   "source": [
    "# convert the date cols into datetime. Lean on pd.to_datetime() to infer the format, assume that each site uses the same format.\n",
    "\n",
    "for e in ['Planted_Unit_Datetime', \n",
    "          'Harvested_Unit_Datetime', \n",
    "          'Anthesis_Unit_Datetime', \n",
    "          'Silking_Unit_Datetime', \n",
    "          'Recieved_Date_Unit_Datetime', \n",
    "          'Processed_Date_Unit_Datetime', \n",
    "          'Weather_Station_Placed_Unit_Datetime', \n",
    "          'Weather_Station_Removed_Unit_Datetime'\n",
    "    ]:\n",
    "# find_unconvertable_datetimes(df_col=sval[e], pattern='%Y-%m-%d %H:%M', index=False)\n",
    "    print(e)\n",
    "    sval['Datetime_Temp'] = pd.to_datetime(np.nan)\n",
    "\n",
    "    for code in list(sval.Experiment_Code.drop_duplicates()):\n",
    "    # code = list(sval.Experiment_Code.drop_duplicates())[0]\n",
    "        sval.loc[sval.Experiment_Code == code, 'Datetime_Temp'\n",
    "                 ] = pd.to_datetime(sval.loc[sval.Experiment_Code == code, e])\n",
    "\n",
    "    sval.loc[:, e] = sval.loc[:, 'Datetime_Temp'] \n",
    "\n",
    "sval = sval.drop(columns = 'Datetime_Temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa07a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> floats\n",
    "\n",
    "sval = sanitize_col(\n",
    "    df = sval, \n",
    "    col = 'Pounds_Needed_Soil_Moisture', \n",
    "    simple_renames= {'3 to 4':'3.5',\n",
    "                     '56 lb/bu and adjusted to 15.5% moisture': '56'}, \n",
    "    split_renames= {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert types\n",
    "for e in ['Alley_Length_Unit_Inches', 'Row_Spacing_Unit_Inches', 'Pounds_Needed_Soil_Moisture',\n",
    "         'Anthesis_Unit_Days', 'Silking_Unit_Days', 'Kernels_Per_Plot']:\n",
    "    err_list = find_unconvertable_numerics(df_col = sval[e], index = False)\n",
    "    if err_list != []:\n",
    "        print(e)\n",
    "        print(err_list)\n",
    "    else:\n",
    "        sval[e] = sval[e].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c067fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bool\n",
    "sval = sanitize_col(\n",
    "    df = sval, \n",
    "    col = 'Discarded', \n",
    "    simple_renames= {\n",
    "        'Yes':'True',\n",
    "        'yes':'True'}, \n",
    "    split_renames= {})\n",
    "\n",
    "# set missing to false\n",
    "sval.loc[sval.Discarded.isna(), 'Discarded'] = 'False'\n",
    "sval.Discarded = sval.Discarded.map({'True': True, 'False': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a16f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6e4df11",
   "metadata": {},
   "source": [
    "### Simple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bool\n",
    "sval['phno'] = sval['phno'].astype('bool')\n",
    "sval['soil'] = sval['soil'].astype('bool')\n",
    "sval['meta'] = sval['meta'].astype('bool')\n",
    "\n",
    "# to string\n",
    "sval = cols_astype_string(\n",
    "    df = sval, \n",
    "    col_list = [key for key in sval_col_dtypes.keys() if sval_col_dtypes[key] == 'string'])\n",
    "\n",
    "sval.Year = year_string\n",
    "sval.Year = sval.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b1261",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec742610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 Columns pass.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = check_df_dtype_expectations(df = sval, dtype_dct = sval_col_dtypes)\n",
    "\n",
    "if sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n",
    "    pass\n",
    "else:\n",
    "    print(checkpoint.loc[~checkpoint.Pass, ]) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54014602",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31085f",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa1a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wthr.loc[:, ['Year',\n",
    "# 'Month',\n",
    "# 'Day',\n",
    "# 'Time']].info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instead of writing regexes to figure out the mose likely format for each datetime, we assume each experiment will be consistent withing that experiment\n",
    "# # and let pd figure it out.\n",
    "# # wthr['Datetime_Temp'] = pd.to_datetime(np.nan)\n",
    "\n",
    "# # for code in list(wthr.loc[:, 'Experiment_Code'].drop_duplicates()):\n",
    "# #     wthr.loc[wthr.Experiment_Code == code, 'Datetime_Temp'] = pd.to_datetime(wthr.loc[wthr.Experiment_Code == code, 'Datetime'], errors='coerce')\n",
    "\n",
    "\n",
    "# # ... or we use the fields in the df to make a consistent format\n",
    "# wthr = cols_astype_string(\n",
    "#     df = wthr, \n",
    "#     col_list = ['Year', 'Month', 'Day', 'Time'])\n",
    "\n",
    "# wthr = sanitize_col(\n",
    "#     df = wthr,\n",
    "#     col = 'Time', \n",
    "#     simple_renames= {'24:00:00': '00:00:00'}, # this could be day + 24 h instead of a miscoded day + 0 h\n",
    "#     split_renames= {})\n",
    "\n",
    "wthr.Month = wthr.Month.astype('string')\n",
    "wthr.Day = wthr.Day.astype('string')\n",
    "\n",
    "wthr['Datetime_Temp'] = wthr['Year']+'-'+wthr['Month']+'-'+wthr['Day']+' '+wthr['Time']\n",
    "\n",
    "# convert types\n",
    "err_list = find_unconvertable_datetimes(df_col=wthr['Datetime_Temp'], pattern='%Y-%m-%d %H:%M', index=False)\n",
    "if err_list != []:\n",
    "    print(err_list)\n",
    "else:\n",
    "    wthr.Datetime_Temp = pd.to_datetime(pd.Series(wthr.Datetime_Temp), errors='coerce')\n",
    "    wthr.Datetime = wthr.Datetime_Temp\n",
    "    wthr = wthr.drop(columns= 'Datetime_Temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b1197",
   "metadata": {},
   "source": [
    "### Wind_Gust_Unit_m_per_s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b566a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wthr = sanitize_col(\n",
    "    df = wthr,\n",
    "    col = 'Wind_Gust_Unit_m_per_s', \n",
    "    simple_renames= {'#REF!': np.nan}, # this could be day + 24 h instead of a miscoded day + 0 h\n",
    "    split_renames= {})\n",
    "\n",
    "# convert types\n",
    "# err_list = find_unconvertable_numerics(wthr.Wind_Gust_Unit_m_per_s)\n",
    "# if err_list != []:\n",
    "#     print(err_list)\n",
    "# else:\n",
    "wthr.Wind_Gust_Unit_m_per_s = wthr.Wind_Gust_Unit_m_per_s.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84197ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42ccd74b",
   "metadata": {},
   "source": [
    "### Simple Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed990b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to string\n",
    "wthr = cols_astype_string(\n",
    "    df = wthr, \n",
    "    col_list = [key for key in wthr_col_dtypes.keys() if wthr_col_dtypes[key] == 'string'])\n",
    "\n",
    "wthr.Year = year_string\n",
    "wthr.Year = wthr.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ad550",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fd6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 Columns pass.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = check_df_dtype_expectations(df = wthr, dtype_dct = wthr_col_dtypes)\n",
    "\n",
    "if sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n",
    "    pass\n",
    "else:\n",
    "    print(checkpoint.loc[~checkpoint.Pass, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958033e",
   "metadata": {},
   "source": [
    "## Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf1a42",
   "metadata": {},
   "source": [
    "### Date_Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt = relocate_to_Imputation_Notes(df = mgmt, col = 'Date_Datetime', val_list= ['Before Planting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt = sanitize_col(\n",
    "#     df = mgmt, \n",
    "#     col = 'Date_Datetime', \n",
    "#     simple_renames= {}, \n",
    "#     split_renames= {'6/24/21 for all but plots in pass 2; 7/5/21 for pass 2' : [\n",
    "#                         '6/24/21 for all but plots in pass 2', '7/5/21 for pass 2']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make corrections too one-off to fix with a funciton. \n",
    "# mask = ((mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2') & (mgmt.Pass != 2.))\n",
    "# mgmt.loc[mask, 'Date_Datetime'] = '6/24/21'\n",
    "# # since we split without specifiying pass we need to remove any rows that still have the search string.\n",
    "# # and overwrite the df\n",
    "# mask = (mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2')\n",
    "# mgmt = mgmt.loc[~mask, :].copy()\n",
    "\n",
    "# mask = ((mgmt.Date_Datetime == '7/5/21 for pass 2') & (mgmt.Pass == 2.))\n",
    "# mgmt.loc[mask, 'Date_Datetime'] = '7/5/21'\n",
    "# mask = (mgmt.Date_Datetime == '7/5/21 for pass 2')\n",
    "# mgmt = mgmt.loc[~mask, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e96ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe34144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incompatable values are of the form:\n",
    "# 64        Thursday, May 9, 2019\n",
    "# 65        Monday, June 10, 2019\n",
    "# 66        Thursday, May 9, 2019\n",
    "# so we rename each before transformation\n",
    "err = find_unconvertable_datetimes(df_col=mgmt.Date_Datetime, pattern='%m/%d/%y', index=False)\n",
    "mgmt.Date_Datetime = mgmt.Date_Datetime.map( dict(zip(err, pd.to_datetime(err) )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b612643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert types\n",
    "err_list = find_unconvertable_datetimes(df_col=mgmt.Date_Datetime, pattern='%m/%d/%y', index=False)\n",
    "if err_list != []:\n",
    "    print(err_list)\n",
    "else:\n",
    "    mgmt.Date_Datetime = pd.to_datetime(pd.Series(mgmt.Date_Datetime), format = '%m/%d/%y', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e0f42",
   "metadata": {},
   "source": [
    "### Amount_Per_Acre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt.loc[find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = True), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt = sanitize_col(\n",
    "    df = mgmt, \n",
    "    col = 'Amount_Per_Acre', \n",
    "    simple_renames= {'100 of N': '100 (N)'}, \n",
    "    split_renames= {'30-26-0-6S': ['30 (N)', '26 (P)', '6 (S)'],\n",
    "                    '21/48': ['21 (Outlook)', '48 (Infantry)'],\n",
    "                    '3/32/2.5%/1%': ['3 (Calisto)', '32 (Atrazine)', '0.025 (AMS)', '0.01 (COC)']\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ead32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmt = safe_create_col(mgmt, \"Ingredient\")\n",
    "mask = mgmt.Ingredient.isna()\n",
    "mgmt.loc[mask, 'Ingredient'] = mgmt.loc[mask, 'Product']\n",
    "\n",
    "# assume each string is formated as 'val (key)'. `sanitize_col` should be used to enforce this.\n",
    "for e in ['26 (P)', '32 (Atrazine)', '0.01 (COC)', '48 (Infantry)', '3 (Calisto)', '100 (N)', '0.025 (AMS)', '6 (S)', '30 (N)', '21 (Outlook)']:\n",
    "    val = re.findall('^\\d+[.]*\\d*', e)[0]\n",
    "    key = re.findall('\\(.+\\)',      e)[0].replace('(', '').replace(')', '')\n",
    "    \n",
    "    mask = (mgmt['Amount_Per_Acre'] == e)\n",
    "    mgmt.loc[mask, 'Ingredient'] = key\n",
    "    mgmt.loc[mask, 'Amount_Per_Acre'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc11303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert types\n",
    "err_list = find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = False)\n",
    "if err_list != []:\n",
    "    print(err_list)\n",
    "else:\n",
    "    mgmt.Amount_Per_Acre = pd.to_numeric(mgmt.Amount_Per_Acre, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57973e80",
   "metadata": {},
   "source": [
    "### Ingredient\n",
    "This is to be the cleaned up version of the \"Product\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66140777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(mgmt.loc[:, 'Ingredient'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c3e99",
   "metadata": {},
   "source": [
    "### Simple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb60d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bool\n",
    "mgmt['mgmt'] = mgmt['mgmt'].astype('bool')\n",
    "\n",
    "# to string\n",
    "for e in [ee for ee in ['Application', 'Product', 'Ingredient', 'Unit', 'Imputation_Notes'] if ee in mgmt.columns]:\n",
    "    mgmt[e] = mgmt[e].astype('string')\n",
    "    \n",
    "\n",
    "mgmt.Year = year_string\n",
    "mgmt.Year = mgmt.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2160e",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1578ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 Columns pass.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>dtype</th>\n",
       "      <th>Expected_dtype</th>\n",
       "      <th>Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Year</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Experiment_Code</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Range</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pass</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Plot</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Application</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Product</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Date_Datetime</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Amount_Per_Acre</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Unit</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mgmt</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ingredient</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Column           dtype  Expected_dtype  Pass\n",
       "0              Year          string          string  True\n",
       "1   Experiment_Code          string          string  True\n",
       "2             Range          string          string  True\n",
       "3              Pass          string          string  True\n",
       "4              Plot          string          string  True\n",
       "5       Application          string          string  True\n",
       "6           Product          string          string  True\n",
       "7     Date_Datetime  datetime64[ns]  datetime64[ns]  True\n",
       "8   Amount_Per_Acre         float64         float64  True\n",
       "9              Unit          string          string  True\n",
       "10             mgmt            bool            bool  True\n",
       "11       Ingredient          string          string  True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_df_dtype_expectations(df = mgmt, dtype_dct = mgmt_col_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109ee8b",
   "metadata": {},
   "source": [
    "# Publish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_out_pkl(obj = sval, path = './data/interim/'+year_string+'sval.pickle')\n",
    "write_out_pkl(obj = wthr, path = './data/interim/'+year_string+'wthr.pickle')\n",
    "write_out_pkl(obj = mgmt, path = './data/interim/'+year_string+'mgmt.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
