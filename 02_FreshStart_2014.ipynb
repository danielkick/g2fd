{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce112a12",
   "metadata": {},
   "source": [
    "# Process Data from 2014 into a consistent format.\n",
    "\n",
    "> This notebook brings the 2014 into alignment with the desired format with respect to field name, type, and grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports ----\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2fd.internal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c88f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2014\n",
    "year_string = '2014'\n",
    "\n",
    "meta_path = './data/raw/G2F_Planting_Season_2014_v4/z._2014_supplemental_info/g2f_2014_field_characteristics.csv' \n",
    "phno_path = './data/raw/G2F_Planting_Season_2014_v4/a._2014_hybrid_phenotypic_data/g2f_2014_hybrid_data_clean.csv' \n",
    "# geno_path = None,  \n",
    "wthr_path = './data/raw/G2F_Planting_Season_2014_v4/b._2014_weather_data/g2f_2014_weather.csv'\n",
    "          # no soil 2014, some info in metadata\n",
    "# soil_path = None, \n",
    "          # no agro 2014, some info in metadata\n",
    "# mgmt_path = None,\n",
    "\n",
    "\n",
    "\n",
    "meta = pd.read_csv(meta_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "phno = pd.read_csv(phno_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "wthr = pd.read_csv(wthr_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "# soil = pd.read_csv(soil_path, encoding = \"ISO-8859-1\", low_memory=False)\n",
    "# mgmt = pd.read_csv(mgmt_path, encoding = \"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d31cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dicts for column renaming\n",
    "meta_name_dict = mk_name_dict(name = 'meta')\n",
    "phno_name_dict = mk_name_dict(name = 'phno')\n",
    "# soil_name_dict = mk_name_dict(name = 'soil')\n",
    "wthr_name_dict = mk_name_dict(name = 'wthr')\n",
    "# mgmt_name_dict = mk_name_dict(name = 'mgmt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ab577",
   "metadata": {},
   "source": [
    "# Rename\n",
    "**Naming rules:**\n",
    "- One dict for each input df\n",
    "- Comment out anything that shouldn't be changed\n",
    "- Upper_Upper_Unit_\\$unit\n",
    "- Upper_$number\n",
    "- No special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b0a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [], [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(find_unrecognized_columns(df = meta, dct = meta_name_dict),\n",
    "find_unrecognized_columns(df = phno, dct = phno_name_dict),\n",
    "# find_unrecognized_columns(df = soil, dct = soil_name_dict),\n",
    "find_unrecognized_columns(df = wthr, dct = wthr_name_dict)#,\n",
    "# find_unrecognized_columns(df = mgmt, dct = mgmt_name_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a50ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location name</th>\n",
       "      <th>Soil test type</th>\n",
       "      <th>Soil texture</th>\n",
       "      <th>Soil pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GA</td>\n",
       "      <td>See file 'Additional G2F metadata-GA1-2014.xlsx'</td>\n",
       "      <td>Loamy Sand</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IA1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Location name                                    Soil test type  \\\n",
       "0            DE                                               NaN   \n",
       "1            GA  See file 'Additional G2F metadata-GA1-2014.xlsx'   \n",
       "2           IA1                                               NaN   \n",
       "\n",
       "  Soil texture Soil pH  \n",
       "0          NaN     NaN  \n",
       "1   Loamy Sand     7.1  \n",
       "2          NaN     NaN  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prlst2dct([])\n",
    "\n",
    "meta.loc[:, ['Location name',\n",
    "  'Soil test type',\n",
    "  'Soil texture',\n",
    "  'Soil pH']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta = meta.rename(columns=meta_name_dict)\n",
    "# phno = phno.rename(columns=phno_name_dict)\n",
    "# soil = soil.rename(columns=soil_name_dict)\n",
    "# wthr = wthr.rename(columns=wthr_name_dict)\n",
    "# mgmt = mgmt.rename(columns=mgmt_name_dict)\n",
    "\n",
    "# # add indicator columns to help with debugging merge\n",
    "# meta['meta'] = True\n",
    "# phno['phno'] = True\n",
    "# soil['soil'] = True\n",
    "# wthr['wthr'] = True\n",
    "# mgmt['mgmt'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b4fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(42, 42), (12675, 38), (207004, 24)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.shape for e in [meta, phno, wthr]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1589ac0",
   "metadata": {},
   "source": [
    "# Sanatize ID columns as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soil = sanitize_Experiment_Codes(\n",
    "#     df = soil, \n",
    "#     simple_renames = {\n",
    "#         'W1H1': 'WIH1', \n",
    "#         'W1H2': 'WIH2', \n",
    "#         'W1H3': 'WIH3'\n",
    "#     }, \n",
    "#     split_renames = {\n",
    "#         'NEH2_NEH3': ['NEH2', 'NEH3']\n",
    "#     })\n",
    "\n",
    "# wthr = sanitize_Experiment_Codes(\n",
    "#     df = wthr, \n",
    "#     simple_renames = {\n",
    "#     }, \n",
    "#     split_renames = {\n",
    "#         'NEH2_NEH3': ['NEH2', 'NEH3'],\n",
    "#         'NYH3_NYS1': ['NYS1', 'NYH3'],\n",
    "#         'TXH1_TXH3': ['TXH1', 'TXH3']\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # confirm everything's okay\n",
    "# print(\n",
    "#   'meta', find_unrecognized_experiments(meta.Experiment_Code, return_all_exps=False), \n",
    "# '\\nphno', find_unrecognized_experiments(phno.Experiment_Code, return_all_exps=False),\n",
    "# '\\nsoil', find_unrecognized_experiments(soil.Experiment_Code, return_all_exps=False),\n",
    "# '\\nwthr', find_unrecognized_experiments(wthr.Experiment_Code, return_all_exps=False),\n",
    "# '\\nmgmt', find_unrecognized_experiments(mgmt.Experiment_Code, return_all_exps=False),\n",
    "# '\\nall ', find_unrecognized_experiments([], return_all_exps=True)\n",
    "# )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c37165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find minimum cols needed to index all rows\n",
    "# df = phno\n",
    "# id_cols = ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot',]\n",
    "# candidate_cols = ['State', 'City',\n",
    "#                  'Experiment', 'Source', 'Pedigree', 'Family', 'Tester', 'Replicate',\n",
    "#                   'Block',  'Plot_ID']\n",
    "# target = df.shape[0]\n",
    "\n",
    "# output = pd.DataFrame(zip(\n",
    "#     candidate_cols,\n",
    "#     [df.loc[:, id_cols+[e]].drop_duplicates().shape[0] for e in candidate_cols]\n",
    "#    ), columns=['Additional_ID', 'Uniq_Vals'])\n",
    "\n",
    "# output.assign(At_Target=lambda x:x.Uniq_Vals == target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94761eb",
   "metadata": {},
   "source": [
    "# Rearrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb762bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # separate static and dynamic values\n",
    "# sval = phno.merge(soil, how = 'outer')\n",
    "# sval = sval.merge(meta, how = 'outer') # This introduces 3 sites that have no data\n",
    "# # sval.shape # used to confirm nrow = #20574 + 3\n",
    "\n",
    "# # these tables are different enought we'll keep them separate\n",
    "# # mgmt\n",
    "# # unfortunately we need multiples because at least one field treats different passes differently\n",
    "# mgmt = phno.loc[:, ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot', 'phno']\n",
    "#                ].drop_duplicates().merge(mgmt, how = 'outer')\n",
    "# # confirm there are no rows in mgmt that are not in phno\n",
    "# temp = mgmt.loc[(~mgmt.phno & mgmt.mgmt), :]\n",
    "# if 0 != temp.shape[0]:\n",
    "#     print(temp)\n",
    "# else:\n",
    "#     mgmt = mgmt.loc[mgmt.mgmt.notna(), :].drop(columns = 'phno')\n",
    "\n",
    "\n",
    "# # wthr\n",
    "# # There's only ever one weather station so we have to worry about imputation but not duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set each id col to a string\n",
    "# for i in ['Year', 'Experiment_Code', 'Range', 'Pass', 'Plot']:\n",
    "#     sval[i] = sval[i].astype('string')\n",
    "#     mgmt[i]  =  mgmt[i].astype('string')\n",
    "    \n",
    "#     if i not in ['Range', 'Pass', 'Plot']:\n",
    "#         wthr[i]  =  wthr[i].astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a94f13",
   "metadata": {},
   "source": [
    "# Sanitize Non-ID columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc8720",
   "metadata": {},
   "source": [
    "## Sanitization functions\n",
    "\n",
    "The pattern to use is:\n",
    " 1. Alter the dataframe\n",
    " 1. Test the dataframe against expectations\n",
    " \n",
    "The main tasks that need to be completed are:\n",
    " 1. Identify values that can't be converted to the expected data type. The \"find_unconvertable_\" family of functions should be used. \n",
    "     1. `find_unconvertable_datetimes`\n",
    "     \n",
    " 1. For simple renaming (e.g. misspellings) or splitting non-tidy data into two rows (\"entry1-entry2\" -> \"entry1\", \"entry2\") use `sanitize_col` \n",
    " 1. Move values that are ambigous but pertain to data imputation to \"Imputation_Notes\" using `relocate_to_Imputation_Notes`\n",
    " 1. If new columns need to be added (e.g. mgmt.Ingredient for parsed components of Product (e.g. elements) ) this should be accomplished with `safe_create_col`.\n",
    " 1. Any one off changes should be accomplised manually. \n",
    " 1. Confirm columns match the expected types with `check_df_dtype_expectations`, and report mismatches. \n",
    "\n",
    "\n",
    "These steps should be completed for each dataframe in turn to minimize the cognitive load of the reader. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cad031",
   "metadata": {},
   "source": [
    "## Sanitization: Column data type expectations\n",
    "Note: to handle missing values some columns that would otherwise be ints are floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sval_col_dtypes = mk_dtype_dict(name = 'sval')\n",
    "# wthr_col_dtypes = mk_dtype_dict(name = 'wthr')\n",
    "# mgmt_col_dtypes = mk_dtype_dict(name = 'mgmt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972cfb9f",
   "metadata": {},
   "source": [
    "# Sanitization: Alter entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccee17b",
   "metadata": {},
   "source": [
    "## Static values (within season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab9954",
   "metadata": {},
   "source": [
    "### Datetime containing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the date cols into datetime. Lean on pd.to_datetime() to infer the format, assume that each site uses the same format.\n",
    "\n",
    "# for e in ['Planted_Unit_Datetime', \n",
    "#     'Harvested_Unit_Datetime', \n",
    "#     'Anthesis_Unit_Datetime', \n",
    "#     'Silking_Unit_Datetime', \n",
    "#     'Recieved_Date_Unit_Datetime', \n",
    "#     'Processed_Date_Unit_Datetime', \n",
    "#     'Weather_Station_Placed_Unit_Datetime', \n",
    "#     'Weather_Station_Removed_Unit_Datetime'\n",
    "#     ]:\n",
    "# # find_unconvertable_datetimes(df_col=sval[e], pattern='%Y-%m-%d %H:%M', index=False)\n",
    "\n",
    "#     sval['Datetime_Temp'] = pd.to_datetime(np.nan)\n",
    "\n",
    "#     for code in list(sval.Experiment_Code.drop_duplicates()):\n",
    "#     # code = list(sval.Experiment_Code.drop_duplicates())[0]\n",
    "#         sval.loc[sval.Experiment_Code == code, 'Datetime_Temp'\n",
    "#                  ] = pd.to_datetime(sval.loc[sval.Experiment_Code == code, e])\n",
    "\n",
    "#     sval.loc[:, e] = sval.loc[:, 'Datetime_Temp'] \n",
    "\n",
    "# sval = sval.drop(columns = 'Datetime_Temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa07a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # -> floats\n",
    "\n",
    "# # # [find_unconvertable_numerics(df_col = sval[e], index = False) for e in [\n",
    "# # #     'Alley_Length_Unit_Inches',\n",
    "# # # 'Row_Spacing_Unit_Inches',\n",
    "# # # 'Pounds_Needed_Soil_Moisture'\n",
    "# # # ]]\n",
    "\n",
    "# sval = sanitize_col(\n",
    "#     df = sval, \n",
    "#     col = 'Pounds_Needed_Soil_Moisture', \n",
    "#     simple_renames= {'3 to 4':'3.5'}, \n",
    "#     split_renames= {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert types\n",
    "# for e in ['Alley_Length_Unit_Inches', 'Row_Spacing_Unit_Inches', 'Pounds_Needed_Soil_Moisture',\n",
    "#          'Anthesis_Unit_Days', 'Silking_Unit_Days', 'Kernels_Per_Plot']:\n",
    "#     err_list = find_unconvertable_numerics(df_col = sval[e], index = False)\n",
    "#     if err_list != []:\n",
    "#         print(e)\n",
    "#         print(err_list)\n",
    "#     else:\n",
    "#         sval[e] = sval[e].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to bool\n",
    "# sval = sanitize_col(\n",
    "#     df = sval, \n",
    "#     col = 'Discarded', \n",
    "#     simple_renames= {\n",
    "#         'Yes':'True',\n",
    "#         'yes':'True'}, \n",
    "#     split_renames= {})\n",
    "\n",
    "# # set missing to false\n",
    "# sval.loc[sval.Discarded.isna(), 'Discarded'] = 'False'\n",
    "# sval.Discarded = sval.Discarded.map({'True': True, 'False': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4df11",
   "metadata": {},
   "source": [
    "### Simple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6927f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to float\n",
    "# sval.Pounds_Needed_Soil_Moisture.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to bool\n",
    "# sval['phno'] = sval['phno'].astype('bool')\n",
    "# sval['soil'] = sval['soil'].astype('bool')\n",
    "# sval['meta'] = sval['meta'].astype('bool')\n",
    "\n",
    "# # to string\n",
    "# sval = cols_astype_string(\n",
    "#     df = sval, \n",
    "#     col_list = [key for key in sval_col_dtypes.keys() if sval_col_dtypes[key] == 'string'])\n",
    "\n",
    "# sval.Year = year_string\n",
    "# sval.Year = sval.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b1261",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec742610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = check_df_dtype_expectations(df = sval, dtype_dct = sval_col_dtypes)\n",
    "\n",
    "# if sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n",
    "#     pass\n",
    "# else:\n",
    "#     print(checkpoint.loc[~checkpoint.Pass, ]) \n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54014602",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31085f",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instead of writing regexes to figure out the mose likely format for each datetime, we assume each experiment will be consistent withing that experiment\n",
    "# # and let pd figure it out.\n",
    "# # wthr['Datetime_Temp'] = pd.to_datetime(np.nan)\n",
    "\n",
    "# # for code in list(wthr.loc[:, 'Experiment_Code'].drop_duplicates()):\n",
    "# #     wthr.loc[wthr.Experiment_Code == code, 'Datetime_Temp'] = pd.to_datetime(wthr.loc[wthr.Experiment_Code == code, 'Datetime'], errors='coerce')\n",
    "\n",
    "\n",
    "# # ... or we use the fields in the df to make a consistent format\n",
    "# wthr = cols_astype_string(\n",
    "#     df = wthr, \n",
    "#     col_list = ['Year', 'Month', 'Day', 'Time'])\n",
    "\n",
    "# wthr = sanitize_col(\n",
    "#     df = wthr,\n",
    "#     col = 'Time', \n",
    "#     simple_renames= {'24:00:00': '00:00:00'}, # this could be day + 24 h instead of a miscoded day + 0 h\n",
    "#     split_renames= {})\n",
    "\n",
    "# wthr['Datetime_Temp'] = wthr['Year']+'-'+wthr['Month']+'-'+wthr['Day']+' '+wthr['Time']\n",
    "\n",
    "# # convert types\n",
    "# err_list = find_unconvertable_datetimes(df_col=wthr['Datetime_Temp'], pattern='%Y-%m-%d %H:%M', index=False)\n",
    "# if err_list != []:\n",
    "#     print(err_list)\n",
    "# else:\n",
    "#     wthr.Datetime_Temp = pd.to_datetime(pd.Series(wthr.Datetime_Temp), errors='coerce')\n",
    "#     wthr.Datetime = wthr.Datetime_Temp\n",
    "#     wthr = wthr.drop(columns= 'Datetime_Temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ccd74b",
   "metadata": {},
   "source": [
    "### Simple Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed990b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to string\n",
    "# wthr = cols_astype_string(\n",
    "#     df = wthr, \n",
    "#     col_list = [key for key in wthr_col_dtypes.keys() if wthr_col_dtypes[key] == 'string'])\n",
    "\n",
    "# wthr.Year = year_string\n",
    "# wthr.Year = wthr.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ad550",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fd6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = check_df_dtype_expectations(df = wthr, dtype_dct = wthr_col_dtypes)\n",
    "\n",
    "# if sum(checkpoint.Pass)/checkpoint.shape[0] == 1:\n",
    "#     pass\n",
    "# else:\n",
    "#     print(checkpoint.loc[~checkpoint.Pass, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958033e",
   "metadata": {},
   "source": [
    "## Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf1a42",
   "metadata": {},
   "source": [
    "### Date_Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt = relocate_to_Imputation_Notes(df = mgmt, col = 'Date_Datetime', val_list= ['Before Planting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt = sanitize_col(\n",
    "#     df = mgmt, \n",
    "#     col = 'Date_Datetime', \n",
    "#     simple_renames= {}, \n",
    "#     split_renames= {'6/24/21 for all but plots in pass 2; 7/5/21 for pass 2' : [\n",
    "#                         '6/24/21 for all but plots in pass 2', '7/5/21 for pass 2']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make corrections too one-off to fix with a funciton. \n",
    "# mask = ((mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2') & (mgmt.Pass != 2.))\n",
    "# mgmt.loc[mask, 'Date_Datetime'] = '6/24/21'\n",
    "# # since we split without specifiying pass we need to remove any rows that still have the search string.\n",
    "# # and overwrite the df\n",
    "# mask = (mgmt.Date_Datetime == '6/24/21 for all but plots in pass 2')\n",
    "# mgmt = mgmt.loc[~mask, :].copy()\n",
    "\n",
    "# mask = ((mgmt.Date_Datetime == '7/5/21 for pass 2') & (mgmt.Pass == 2.))\n",
    "# mgmt.loc[mask, 'Date_Datetime'] = '7/5/21'\n",
    "# mask = (mgmt.Date_Datetime == '7/5/21 for pass 2')\n",
    "# mgmt = mgmt.loc[~mask, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b612643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert types\n",
    "# err_list = find_unconvertable_datetimes(df_col=mgmt.Date_Datetime, pattern='%m/%d/%y', index=False)\n",
    "# if err_list != []:\n",
    "#     print(err_list)\n",
    "# else:\n",
    "#     mgmt.Date_Datetime = pd.to_datetime(pd.Series(mgmt.Date_Datetime), format = '%m/%d/%y', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e0f42",
   "metadata": {},
   "source": [
    "### Amount_Per_Acre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt.loc[find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = True), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt = sanitize_col(\n",
    "#     df = mgmt, \n",
    "#     col = 'Amount_Per_Acre', \n",
    "#     simple_renames= {'170 lb (actual N)': '170 (N)'}, \n",
    "#     split_renames= {'51.75, 40.7, 111.7 (N,P,K)': ['51.75 (N)', '40.7 (P)', '111.7 (K)'],\n",
    "#                     '31-150-138': ['31 (N)', '150 (P)', '138 (K)'],\n",
    "#                     '16 (N), 41 (P)': ['16 (N)', '41 (P)']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ead32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgmt = safe_create_col(mgmt, \"Ingredient\")\n",
    "# mask = mgmt.Ingredient.isna()\n",
    "# mgmt.loc[mask, 'Ingredient'] = mgmt.loc[mask, 'Product']\n",
    "\n",
    "# # assume each string is formated as 'val (key)'. `sanitize_col` should be used to enforce this.\n",
    "# for e in ['150 (P)', '36.6 (N)', '138 (K)', '111.7 (K)', '41 (P)', '16 (N)', '170 (N)', '35.7 (N)', '51.75 (N)', '31 (N)', '40.7 (P)']:\n",
    "#     val = re.findall('^\\d+[.]*\\d*', e)[0]\n",
    "#     key = re.findall('\\(.+\\)',      e)[0].replace('(', '').replace(')', '')\n",
    "    \n",
    "#     mask = (mgmt['Amount_Per_Acre'] == e)\n",
    "#     mgmt.loc[mask, 'Ingredient'] = key\n",
    "#     mgmt.loc[mask, 'Amount_Per_Acre'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc11303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert types\n",
    "# err_list = find_unconvertable_numerics(df_col = mgmt['Amount_Per_Acre'], index = False)\n",
    "# if err_list != []:\n",
    "#     print(err_list)\n",
    "# else:\n",
    "#     mgmt.Amount_Per_Acre = pd.to_numeric(mgmt.Amount_Per_Acre, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57973e80",
   "metadata": {},
   "source": [
    "### Ingredient\n",
    "This is to be the cleaned up version of the \"Product\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66140777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(mgmt.loc[:, 'Ingredient'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c3e99",
   "metadata": {},
   "source": [
    "### Simple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb60d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to bool\n",
    "# mgmt['mgmt'] = mgmt['mgmt'].astype('bool')\n",
    "\n",
    "# # to string\n",
    "# for e in [ee for ee in ['Application', 'Product', 'Ingredient', 'Unit', 'Imputation_Notes'] if ee in mgmt.columns]:\n",
    "#     mgmt[e] = mgmt[e].astype('string')\n",
    "    \n",
    "\n",
    "# mgmt.Year = year_string\n",
    "# mgmt.Year = mgmt.Year.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2160e",
   "metadata": {},
   "source": [
    "### Check Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1578ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_df_dtype_expectations(df = mgmt, dtype_dct = mgmt_col_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109ee8b",
   "metadata": {},
   "source": [
    "# Publish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_out_pkl(obj = sval, path = './data/interim/'+year_string+'sval.pickle')\n",
    "# write_out_pkl(obj = wthr, path = './data/interim/'+year_string+'wthr.pickle')\n",
    "# write_out_pkl(obj = mgmt, path = './data/interim/'+year_string+'mgmt.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
